{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "kingsolarbeam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rH3GAkJs36t"
      },
      "source": [
        "import numpy as np\r\n",
        "from numpy import concatenate\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "import math\r\n",
        "import seaborn as sns\r\n",
        "from scipy.optimize import curve_fit\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, LSTM, Dropout\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import RepeatVector\r\n",
        "from keras.layers import TimeDistributed\r\n",
        "from keras.layers import ConvLSTM2D\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtGQCg2stOeq"
      },
      "source": [
        "solar_data = pd.read_csv('./data/train/train.csv',encoding='utf-8')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "FnTDmbS3vXLr",
        "outputId": "a8a003e4-b813-437a-b7e6-29485c439b60"
      },
      "source": [
        "corrList = []\r\n",
        "corr = lambda p : p['TARGET'].corr(p['GHI'])\r\n",
        "\r\n",
        "def makeCorrList(df):\r\n",
        "    result = corr(df)\r\n",
        "    corrList.append(result)\r\n",
        "\r\n",
        "for i in range(0,90):\r\n",
        "    solar_data['GHI'] = solar_data['DNI']*math.cos(math.pi/180*i)+solar_data['DHI']\r\n",
        "    makeCorrList(solar_data)\r\n",
        "\r\n",
        "x = np.arange(0,90)\r\n",
        "plt.plot(x,corrList)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print(corrList.index(max(corrList)), max(corrList))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "63 0.9451229383187165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALCroCYTvXPB"
      },
      "source": [
        "solar_data['GHI'] = solar_data['DNI']*math.cos(math.pi/180*63)+solar_data['DHI']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VRh_Rn6vXR9"
      },
      "source": [
        "def eval_dewpoint(T, RH):\r\n",
        "    b = 17.62\r\n",
        "    c = 243.12\r\n",
        "    gamma = (b * T / (c + T)) + math.log(RH / 100.0)\r\n",
        "    dewpoint = (c * gamma) / (b - gamma)\r\n",
        "    return dewpoint"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOYpWH4KvzIj"
      },
      "source": [
        "for i in range(len(solar_data)):\r\n",
        "    solar_data.loc[i, [\"DP\"]] = eval_dewpoint(float(solar_data.loc[i, [\"T\"]]), solar_data.loc[i , [\"RH\"]])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zimBa52mvzMf"
      },
      "source": [
        "solar_data = solar_data.reindex(columns=[\"Day\", \"Hour\", \"Minute\", \"GHI\", \"DHI\", \"DNI\", \"WS\", \"RH\", \"T\", \"DP\", \"TARGET\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "eZPXTvXml1YV",
        "outputId": "9c94b477-7886-4538-e77c-daeb72a7c23c"
      },
      "source": [
        "mask = (solar_data.Hour==12)&(solar_data.Minute==00)\r\n",
        "# solar_data_filtered = solar_data.loc[mask,:]\r\n",
        "# solar_data_filtered.drop(['Day', 'Hour','Minute'], axis='columns', inplace=True)\r\n",
        "plt.figure(figsize=(7, 7))\r\n",
        "# sns.heatmap(data=solar_data_filtered.corr(), annot=True,fmt='.2f', linewidths=.5, cmap='Blues')\r\n",
        "sns.heatmap(data=solar_data.corr(), annot=True,fmt='.2f', linewidths=.5, cmap='Blues')\r\n",
        "plt.rcParams['font.family'] = 'NanumGothic'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE66oxfcl1cn"
      },
      "source": [
        "solar_data.drop(['Day', 'Hour','Minute'], axis='columns', inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khBNwoQOtXLG"
      },
      "source": [
        "class CustomHistory(keras.callbacks.Callback):\r\n",
        "    def init(self):\r\n",
        "        self.train_loss = []\r\n",
        "        self.val_loss = []\r\n",
        "        \r\n",
        "    def on_epoch_end(self, batch, logs={}):\r\n",
        "        self.train_loss.append(logs.get('loss'))\r\n",
        "        self.val_loss.append(logs.get('val_loss'))\r\n",
        "\r\n",
        "def create_dataset(solar_data, index):\r\n",
        "    dataX, dataY = [], []\r\n",
        "    for i in range(0,48*7):\r\n",
        "        dataX.append(list(np.array(solar_data.loc[index+i].tolist())))\r\n",
        "    for i in range(48*7,48*7+48*2):\r\n",
        "        dataY.append(solar_data.loc[index+i,'TARGET'])\r\n",
        "    return np.array(dataX), np.array(dataY)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGcvKLehtXgX"
      },
      "source": [
        "# 데이터셋 생성\r\n",
        "input_data, output_data = [], []\r\n",
        "\r\n",
        "last_index = 3*365*48-48*9\r\n",
        "list_index = list(range(0,last_index,48))\r\n",
        "\r\n",
        "# last_index = 3*365*48 - 432\r\n",
        "# list_index = list(0:last_index,1))\r\n",
        "\r\n",
        "for i in list_index:\r\n",
        "    X, Y = create_dataset(solar_data,i)\r\n",
        "    input_data.append(X)\r\n",
        "    output_data.append(Y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KBltPk21Pt5"
      },
      "source": [
        "input_array = np.array(input_data)\r\n",
        "output_array = np.reshape(np.array(output_data),(1086,96,1))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UniartIetYjT"
      },
      "source": [
        "# 데이터셋 분배\r\n",
        "train_x, test_x, train_y, test_y = train_test_split(input_array, output_array, test_size = 78/1086,shuffle = True)\r\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=48*3/1008,shuffle=True)\r\n",
        "\r\n",
        "# x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\r\n",
        "# train_x = train_x.reshape((train_x.shape[0], 7, 1, 48, 6))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr55gSym3wvQ"
      },
      "source": [
        "# 돌려!\r\n",
        "\r\n",
        "import keras.backend as K\r\n",
        "\r\n",
        "submission = pd.read_csv('./data/sample_submission.csv')\r\n",
        "submission.set_index('id',inplace=False)\r\n",
        "test = []\r\n",
        "for i in range(81):\r\n",
        "    data = []\r\n",
        "    tmp = pd.read_csv(f'./data/test/{i}.csv')\r\n",
        "    tmp['GHI'] = tmp['DNI']*math.cos(math.pi/180*63)+tmp['DHI']\r\n",
        "    for i in range(len(tmp)):\r\n",
        "      tmp.loc[i, [\"DP\"]] = eval_dewpoint(float(tmp.loc[i, [\"T\"]]), tmp.loc[i , [\"RH\"]])\r\n",
        "    tmp = tmp.reindex(columns=[\"Day\", \"Hour\", \"Minute\", \"GHI\", \"DHI\", \"DNI\", \"WS\", \"RH\", \"T\", \"DP\", \"TARGET\"])\r\n",
        "    tmp.drop(['Day', 'Hour','Minute'], axis='columns', inplace=True)\r\n",
        "    for i in range(0,48*7):\r\n",
        "        data.append(list(np.array(tmp.loc[i].tolist())))\r\n",
        "    test.append(data)\r\n",
        "test = np.array(test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o_Xm1z7RjZD"
      },
      "source": [
        "def tilted_loss(q,y,f):\r\n",
        "    e = (y-f)\r\n",
        "    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)\r\n",
        "\r\n",
        "def sun_rise(test_data,prediction_data):\r\n",
        "  sun_time=[]\r\n",
        "  test_data = np.reshape(test_data,(336,8))\r\n",
        "  for i in range(48):\r\n",
        "      if (test_data[288 + i, -1])>0:\r\n",
        "          sun_time.append(1)\r\n",
        "      else:\r\n",
        "          sun_time.append(0)\r\n",
        "  sum_time = np.array(sun_time + sun_time)\r\n",
        "  pre_data = np.reshape(prediction_data,(96))\r\n",
        "  ans = sum_time * pre_data\r\n",
        "  return ans"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHlZTat2QyIy",
        "outputId": "b93c6fb7-caa2-4cba-b590-1649479ee6f7"
      },
      "source": [
        "n_batch = 8\r\n",
        "\r\n",
        "for l, q in enumerate(np.arange(0.1, 1, 0.1)):\r\n",
        "  print(q)\r\n",
        "  model = Sequential()\r\n",
        "  for i in range(2):\r\n",
        "      model.add(LSTM(128, batch_input_shape=(8, 336, 8), stateful=True, return_sequences=True))\r\n",
        "      model.add(Dropout(0.3))\r\n",
        "  model.add(LSTM(128, batch_input_shape=(8, 336, 8), stateful=True, return_sequences=True))\r\n",
        "  model.add(Dropout(0.3))\r\n",
        "  model.add(LSTM(64, return_sequences=True))\r\n",
        "  model.add(Dropout(0.3))\r\n",
        "  model.add(LSTM(16))\r\n",
        "  model.add(Dropout(0.3))\r\n",
        "  model.add(Dense(96))\r\n",
        "  model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "  for j in range(500):\r\n",
        "    # model.fit(train_x, train_y, epochs=1, batch_size=1, shuffle=False, callbacks=[custom_hist], validation_data=(val_x, val_y))\r\n",
        "    model.fit(train_x, train_y, epochs=1, batch_size=n_batch, shuffle=False, validation_data=(val_x, val_y),  verbose=2)\r\n",
        "    model.reset_states()\r\n",
        "  weights = model.get_weights()\r\n",
        "  \r\n",
        "  single_item_model = Sequential()\r\n",
        "  for i in range(2):\r\n",
        "      single_item_model.add(LSTM(128, batch_input_shape=(1, 336, 8), stateful=True, return_sequences=True))\r\n",
        "      single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(LSTM(128, batch_input_shape=(1, 336, 8), stateful=True, return_sequences=True))\r\n",
        "  single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(LSTM(64, return_sequences=True))\r\n",
        "  single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(LSTM(16))\r\n",
        "  single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(Dense(96))\r\n",
        "  single_item_model.set_weights(weights)\r\n",
        "  single_item_model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "  \r\n",
        "  predictions = []\r\n",
        "  for k in range(81):\r\n",
        "    prediction = single_item_model.predict(np.array([test[k]]), batch_size=1)\r\n",
        "    predictions.append(sun_rise(np.array([test[k]]),prediction))\r\n",
        "  predictions = np.reshape(np.concatenate(np.array(predictions), axis=0),(81*96))\r\n",
        "  submission.iloc[:,l+1] = predictions\r\n",
        "submission.to_csv(f'submission.csv', index=False)\r\n",
        "print('finish')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1\n",
            "108/108 - 18s - loss: 1.7784 - val_loss: 1.8338\n",
            "108/108 - 7s - loss: 1.7214 - val_loss: 1.7203\n",
            "108/108 - 7s - loss: 1.6398 - val_loss: 1.6696\n",
            "108/108 - 7s - loss: 1.5981 - val_loss: 1.6367\n",
            "108/108 - 7s - loss: 1.5676 - val_loss: 1.6136\n",
            "108/108 - 7s - loss: 1.5483 - val_loss: 1.5981\n",
            "108/108 - 7s - loss: 1.5362 - val_loss: 1.5867\n",
            "108/108 - 7s - loss: 1.5294 - val_loss: 1.5789\n",
            "108/108 - 7s - loss: 1.5219 - val_loss: 1.5728\n",
            "108/108 - 7s - loss: 1.5181 - val_loss: 1.5679\n",
            "108/108 - 7s - loss: 1.5172 - val_loss: 1.5642\n",
            "108/108 - 7s - loss: 1.5141 - val_loss: 1.5619\n",
            "108/108 - 7s - loss: 1.5073 - val_loss: 1.5597\n",
            "108/108 - 7s - loss: 1.5050 - val_loss: 1.5578\n",
            "108/108 - 7s - loss: 1.5053 - val_loss: 1.5563\n",
            "108/108 - 7s - loss: 1.5021 - val_loss: 1.5550\n",
            "108/108 - 7s - loss: 1.4975 - val_loss: 1.5540\n",
            "108/108 - 7s - loss: 1.4991 - val_loss: 1.5532\n",
            "108/108 - 7s - loss: 1.5005 - val_loss: 1.5527\n",
            "108/108 - 7s - loss: 1.4937 - val_loss: 1.5522\n",
            "108/108 - 7s - loss: 1.5026 - val_loss: 1.5518\n",
            "108/108 - 7s - loss: 1.4967 - val_loss: 1.5513\n",
            "108/108 - 7s - loss: 1.4978 - val_loss: 1.5511\n",
            "108/108 - 7s - loss: 1.4944 - val_loss: 1.5508\n",
            "108/108 - 7s - loss: 1.4918 - val_loss: 1.5506\n",
            "108/108 - 7s - loss: 1.4965 - val_loss: 1.5505\n",
            "108/108 - 7s - loss: 1.4969 - val_loss: 1.5505\n",
            "108/108 - 7s - loss: 1.4947 - val_loss: 1.5503\n",
            "108/108 - 7s - loss: 1.5001 - val_loss: 1.5505\n",
            "108/108 - 7s - loss: 1.4983 - val_loss: 1.5502\n",
            "108/108 - 7s - loss: 1.4934 - val_loss: 1.5501\n",
            "108/108 - 7s - loss: 1.4996 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4952 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.4947 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4951 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4961 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4970 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4996 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4986 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.4949 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.4884 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4971 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4912 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4921 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4996 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.5005 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4963 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4965 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4934 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4965 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4973 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4950 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.5001 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.5008 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4932 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4953 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4934 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4956 - val_loss: 1.5501\n",
            "108/108 - 7s - loss: 1.4929 - val_loss: 1.5500\n",
            "108/108 - 7s - loss: 1.4912 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4975 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4912 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4946 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4938 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4970 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4916 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4993 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4975 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4971 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4968 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.4954 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4965 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4967 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4952 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4968 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4962 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4902 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4964 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.5005 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4953 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4984 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.5072 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4961 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4986 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4935 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.5036 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4952 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4949 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.5042 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.5003 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4988 - val_loss: 1.5498\n",
            "108/108 - 7s - loss: 1.4993 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.4964 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.4932 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4943 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4957 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4969 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4975 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4982 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4938 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4979 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4892 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.5030 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4961 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4980 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4962 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4948 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.5011 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4943 - val_loss: 1.5493\n",
            "108/108 - 7s - loss: 1.4931 - val_loss: 1.5493\n",
            "108/108 - 7s - loss: 1.4962 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4975 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4979 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.5033 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4885 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4906 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4927 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.5007 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4955 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4879 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4978 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4916 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4984 - val_loss: 1.5493\n",
            "108/108 - 7s - loss: 1.4921 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4966 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4975 - val_loss: 1.5493\n",
            "108/108 - 7s - loss: 1.4957 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4960 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4993 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4998 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4918 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4969 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4913 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4998 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4910 - val_loss: 1.5491\n",
            "108/108 - 7s - loss: 1.4965 - val_loss: 1.5491\n",
            "108/108 - 7s - loss: 1.4970 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4956 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4948 - val_loss: 1.5491\n",
            "108/108 - 7s - loss: 1.4854 - val_loss: 1.5491\n",
            "108/108 - 7s - loss: 1.4941 - val_loss: 1.5492\n",
            "108/108 - 7s - loss: 1.4994 - val_loss: 1.5493\n",
            "108/108 - 7s - loss: 1.4965 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4924 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4966 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4919 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4938 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4986 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4972 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4963 - val_loss: 1.5493\n",
            "108/108 - 7s - loss: 1.5030 - val_loss: 1.5515\n",
            "108/108 - 7s - loss: 1.4976 - val_loss: 1.5508\n",
            "108/108 - 7s - loss: 1.4941 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.5001 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4959 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4979 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4933 - val_loss: 1.5497\n",
            "108/108 - 7s - loss: 1.4914 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4954 - val_loss: 1.5493\n",
            "108/108 - 7s - loss: 1.5080 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.4985 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.5034 - val_loss: 1.5487\n",
            "108/108 - 7s - loss: 1.4920 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.4931 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4939 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4929 - val_loss: 1.5495\n",
            "108/108 - 7s - loss: 1.4982 - val_loss: 1.5494\n",
            "108/108 - 7s - loss: 1.4935 - val_loss: 1.5491\n",
            "108/108 - 7s - loss: 1.4945 - val_loss: 1.5488\n",
            "108/108 - 7s - loss: 1.4959 - val_loss: 1.5478\n",
            "108/108 - 7s - loss: 1.4941 - val_loss: 1.5485\n",
            "108/108 - 7s - loss: 1.5023 - val_loss: 1.5490\n",
            "108/108 - 7s - loss: 1.4980 - val_loss: 1.5489\n",
            "108/108 - 7s - loss: 1.4915 - val_loss: 1.5527\n",
            "108/108 - 7s - loss: 1.4990 - val_loss: 1.5512\n",
            "108/108 - 7s - loss: 1.4947 - val_loss: 1.5503\n",
            "108/108 - 7s - loss: 1.4951 - val_loss: 1.5478\n",
            "108/108 - 7s - loss: 1.4881 - val_loss: 1.5430\n",
            "108/108 - 7s - loss: 1.4921 - val_loss: 1.5455\n",
            "108/108 - 7s - loss: 1.4963 - val_loss: 1.5452\n",
            "108/108 - 7s - loss: 1.4907 - val_loss: 1.5408\n",
            "108/108 - 7s - loss: 1.4874 - val_loss: 1.5448\n",
            "108/108 - 7s - loss: 1.4915 - val_loss: 1.5390\n",
            "108/108 - 7s - loss: 1.4844 - val_loss: 1.5436\n",
            "108/108 - 7s - loss: 1.4855 - val_loss: 1.5398\n",
            "108/108 - 7s - loss: 1.4822 - val_loss: 1.5420\n",
            "108/108 - 7s - loss: 1.4813 - val_loss: 1.5343\n",
            "108/108 - 7s - loss: 1.4808 - val_loss: 1.5331\n",
            "108/108 - 7s - loss: 1.4811 - val_loss: 1.5348\n",
            "108/108 - 7s - loss: 1.4802 - val_loss: 1.5310\n",
            "108/108 - 7s - loss: 1.4737 - val_loss: 1.5304\n",
            "108/108 - 7s - loss: 1.4767 - val_loss: 1.5312\n",
            "108/108 - 7s - loss: 1.4785 - val_loss: 1.5295\n",
            "108/108 - 7s - loss: 1.4775 - val_loss: 1.5336\n",
            "108/108 - 7s - loss: 1.4706 - val_loss: 1.5291\n",
            "108/108 - 7s - loss: 1.4708 - val_loss: 1.5295\n",
            "108/108 - 7s - loss: 1.4651 - val_loss: 1.5292\n",
            "108/108 - 7s - loss: 1.4696 - val_loss: 1.5308\n",
            "108/108 - 7s - loss: 1.4646 - val_loss: 1.5318\n",
            "108/108 - 7s - loss: 1.4652 - val_loss: 1.5262\n",
            "108/108 - 7s - loss: 1.4697 - val_loss: 1.5299\n",
            "108/108 - 7s - loss: 1.4688 - val_loss: 1.5263\n",
            "108/108 - 7s - loss: 1.4636 - val_loss: 1.5243\n",
            "108/108 - 7s - loss: 1.4667 - val_loss: 1.5274\n",
            "108/108 - 7s - loss: 1.4646 - val_loss: 1.5354\n",
            "108/108 - 7s - loss: 1.4598 - val_loss: 1.5242\n",
            "108/108 - 7s - loss: 1.4580 - val_loss: 1.5236\n",
            "108/108 - 7s - loss: 1.4559 - val_loss: 1.5274\n",
            "108/108 - 7s - loss: 1.4604 - val_loss: 1.5316\n",
            "108/108 - 7s - loss: 1.4591 - val_loss: 1.5216\n",
            "108/108 - 7s - loss: 1.4550 - val_loss: 1.5243\n",
            "108/108 - 7s - loss: 1.4560 - val_loss: 1.5192\n",
            "108/108 - 7s - loss: 1.4554 - val_loss: 1.5199\n",
            "108/108 - 7s - loss: 1.4415 - val_loss: 1.5209\n",
            "108/108 - 7s - loss: 1.4536 - val_loss: 1.5305\n",
            "108/108 - 7s - loss: 1.4477 - val_loss: 1.5275\n",
            "108/108 - 7s - loss: 1.4498 - val_loss: 1.5276\n",
            "108/108 - 7s - loss: 1.4510 - val_loss: 1.5227\n",
            "108/108 - 7s - loss: 1.4537 - val_loss: 1.5205\n",
            "108/108 - 7s - loss: 1.4409 - val_loss: 1.5187\n",
            "108/108 - 7s - loss: 1.4464 - val_loss: 1.5252\n",
            "108/108 - 7s - loss: 1.4557 - val_loss: 1.5068\n",
            "108/108 - 7s - loss: 1.4473 - val_loss: 1.5160\n",
            "108/108 - 7s - loss: 1.4496 - val_loss: 1.5185\n",
            "108/108 - 7s - loss: 1.4475 - val_loss: 1.5144\n",
            "108/108 - 7s - loss: 1.4454 - val_loss: 1.5134\n",
            "108/108 - 7s - loss: 1.4563 - val_loss: 1.5312\n",
            "108/108 - 7s - loss: 1.4461 - val_loss: 1.5179\n",
            "108/108 - 7s - loss: 1.4470 - val_loss: 1.5253\n",
            "108/108 - 7s - loss: 1.4352 - val_loss: 1.5152\n",
            "108/108 - 7s - loss: 1.4430 - val_loss: 1.5116\n",
            "108/108 - 7s - loss: 1.4385 - val_loss: 1.5037\n",
            "108/108 - 7s - loss: 1.4325 - val_loss: 1.5054\n",
            "108/108 - 7s - loss: 1.4326 - val_loss: 1.5152\n",
            "108/108 - 7s - loss: 1.4265 - val_loss: 1.5059\n",
            "108/108 - 7s - loss: 1.4289 - val_loss: 1.5172\n",
            "108/108 - 7s - loss: 1.4255 - val_loss: 1.5055\n",
            "108/108 - 7s - loss: 1.4329 - val_loss: 1.5118\n",
            "108/108 - 7s - loss: 1.4342 - val_loss: 1.5102\n",
            "108/108 - 7s - loss: 1.4181 - val_loss: 1.5135\n",
            "108/108 - 7s - loss: 1.4332 - val_loss: 1.5154\n",
            "108/108 - 7s - loss: 1.4234 - val_loss: 1.5067\n",
            "108/108 - 7s - loss: 1.4239 - val_loss: 1.5002\n",
            "108/108 - 7s - loss: 1.4209 - val_loss: 1.5131\n",
            "108/108 - 7s - loss: 1.4236 - val_loss: 1.5000\n",
            "108/108 - 7s - loss: 1.4190 - val_loss: 1.5039\n",
            "108/108 - 7s - loss: 1.4197 - val_loss: 1.5090\n",
            "108/108 - 7s - loss: 1.4216 - val_loss: 1.5115\n",
            "108/108 - 7s - loss: 1.4153 - val_loss: 1.5100\n",
            "108/108 - 7s - loss: 1.4207 - val_loss: 1.5068\n",
            "108/108 - 7s - loss: 1.4226 - val_loss: 1.4991\n",
            "108/108 - 7s - loss: 1.4126 - val_loss: 1.4992\n",
            "108/108 - 7s - loss: 1.4057 - val_loss: 1.5113\n",
            "108/108 - 7s - loss: 1.4179 - val_loss: 1.4990\n",
            "108/108 - 7s - loss: 1.4217 - val_loss: 1.5077\n",
            "108/108 - 7s - loss: 1.4169 - val_loss: 1.5073\n",
            "108/108 - 7s - loss: 1.4223 - val_loss: 1.5075\n",
            "108/108 - 7s - loss: 1.4122 - val_loss: 1.5053\n",
            "108/108 - 7s - loss: 1.4198 - val_loss: 1.5072\n",
            "108/108 - 7s - loss: 1.4131 - val_loss: 1.4855\n",
            "108/108 - 7s - loss: 1.4069 - val_loss: 1.4922\n",
            "108/108 - 7s - loss: 1.4033 - val_loss: 1.5065\n",
            "108/108 - 7s - loss: 1.4117 - val_loss: 1.5106\n",
            "108/108 - 7s - loss: 1.4048 - val_loss: 1.5040\n",
            "108/108 - 7s - loss: 1.4077 - val_loss: 1.5112\n",
            "108/108 - 7s - loss: 1.4090 - val_loss: 1.5116\n",
            "108/108 - 7s - loss: 1.4104 - val_loss: 1.5100\n",
            "108/108 - 7s - loss: 1.4012 - val_loss: 1.5099\n",
            "108/108 - 7s - loss: 1.4086 - val_loss: 1.4979\n",
            "108/108 - 7s - loss: 1.4032 - val_loss: 1.5087\n",
            "108/108 - 7s - loss: 1.4146 - val_loss: 1.4976\n",
            "108/108 - 7s - loss: 1.4087 - val_loss: 1.5218\n",
            "108/108 - 7s - loss: 1.3988 - val_loss: 1.5072\n",
            "108/108 - 7s - loss: 1.4028 - val_loss: 1.4999\n",
            "108/108 - 7s - loss: 1.4052 - val_loss: 1.5129\n",
            "108/108 - 7s - loss: 1.3892 - val_loss: 1.5140\n",
            "108/108 - 7s - loss: 1.3979 - val_loss: 1.4994\n",
            "108/108 - 7s - loss: 1.4006 - val_loss: 1.4980\n",
            "108/108 - 7s - loss: 1.3995 - val_loss: 1.5069\n",
            "108/108 - 7s - loss: 1.3888 - val_loss: 1.4887\n",
            "108/108 - 7s - loss: 1.4033 - val_loss: 1.4931\n",
            "108/108 - 7s - loss: 1.3885 - val_loss: 1.4949\n",
            "108/108 - 7s - loss: 1.3880 - val_loss: 1.5017\n",
            "108/108 - 7s - loss: 1.3955 - val_loss: 1.5002\n",
            "108/108 - 7s - loss: 1.3917 - val_loss: 1.5097\n",
            "108/108 - 7s - loss: 1.3923 - val_loss: 1.4983\n",
            "108/108 - 7s - loss: 1.3965 - val_loss: 1.4838\n",
            "108/108 - 7s - loss: 1.3942 - val_loss: 1.5061\n",
            "108/108 - 7s - loss: 1.3925 - val_loss: 1.5090\n",
            "108/108 - 7s - loss: 1.3879 - val_loss: 1.4915\n",
            "108/108 - 7s - loss: 1.3868 - val_loss: 1.5022\n",
            "108/108 - 7s - loss: 1.3863 - val_loss: 1.4998\n",
            "108/108 - 7s - loss: 1.3990 - val_loss: 1.5168\n",
            "108/108 - 7s - loss: 1.3881 - val_loss: 1.5100\n",
            "108/108 - 7s - loss: 1.3780 - val_loss: 1.5030\n",
            "108/108 - 7s - loss: 1.3860 - val_loss: 1.4885\n",
            "108/108 - 7s - loss: 1.3811 - val_loss: 1.4883\n",
            "108/108 - 7s - loss: 1.3886 - val_loss: 1.4992\n",
            "108/108 - 7s - loss: 1.3875 - val_loss: 1.4965\n",
            "108/108 - 7s - loss: 1.3758 - val_loss: 1.4881\n",
            "108/108 - 7s - loss: 1.3797 - val_loss: 1.4805\n",
            "108/108 - 7s - loss: 1.3827 - val_loss: 1.4793\n",
            "108/108 - 7s - loss: 1.3739 - val_loss: 1.5089\n",
            "108/108 - 7s - loss: 1.3757 - val_loss: 1.4840\n",
            "108/108 - 7s - loss: 1.3736 - val_loss: 1.4795\n",
            "108/108 - 7s - loss: 1.3673 - val_loss: 1.4761\n",
            "108/108 - 7s - loss: 1.3753 - val_loss: 1.4994\n",
            "108/108 - 7s - loss: 1.3741 - val_loss: 1.4813\n",
            "108/108 - 7s - loss: 1.3709 - val_loss: 1.4841\n",
            "108/108 - 7s - loss: 1.3690 - val_loss: 1.4657\n",
            "108/108 - 7s - loss: 1.3733 - val_loss: 1.4801\n",
            "108/108 - 7s - loss: 1.3594 - val_loss: 1.4654\n",
            "108/108 - 7s - loss: 1.3691 - val_loss: 1.4764\n",
            "108/108 - 7s - loss: 1.3672 - val_loss: 1.4920\n",
            "108/108 - 7s - loss: 1.3618 - val_loss: 1.4850\n",
            "108/108 - 7s - loss: 1.3733 - val_loss: 1.4902\n",
            "108/108 - 7s - loss: 1.3662 - val_loss: 1.5060\n",
            "108/108 - 7s - loss: 1.3800 - val_loss: 1.4826\n",
            "108/108 - 7s - loss: 1.3656 - val_loss: 1.4780\n",
            "108/108 - 7s - loss: 1.3540 - val_loss: 1.4836\n",
            "108/108 - 7s - loss: 1.3614 - val_loss: 1.4839\n",
            "108/108 - 7s - loss: 1.3500 - val_loss: 1.4768\n",
            "108/108 - 7s - loss: 1.3512 - val_loss: 1.4816\n",
            "108/108 - 7s - loss: 1.3578 - val_loss: 1.4782\n",
            "108/108 - 7s - loss: 1.3469 - val_loss: 1.4850\n",
            "108/108 - 7s - loss: 1.3600 - val_loss: 1.4742\n",
            "108/108 - 7s - loss: 1.3513 - val_loss: 1.4713\n",
            "108/108 - 7s - loss: 1.3513 - val_loss: 1.5045\n",
            "108/108 - 7s - loss: 1.3599 - val_loss: 1.4680\n",
            "108/108 - 7s - loss: 1.3580 - val_loss: 1.4954\n",
            "108/108 - 7s - loss: 1.3583 - val_loss: 1.4820\n",
            "108/108 - 7s - loss: 1.3506 - val_loss: 1.5190\n",
            "108/108 - 7s - loss: 1.3540 - val_loss: 1.4899\n",
            "108/108 - 7s - loss: 1.3591 - val_loss: 1.4742\n",
            "108/108 - 7s - loss: 1.3597 - val_loss: 1.4926\n",
            "108/108 - 7s - loss: 1.3447 - val_loss: 1.4869\n",
            "108/108 - 7s - loss: 1.3353 - val_loss: 1.4820\n",
            "108/108 - 7s - loss: 1.3421 - val_loss: 1.4900\n",
            "108/108 - 7s - loss: 1.3448 - val_loss: 1.5074\n",
            "108/108 - 7s - loss: 1.3508 - val_loss: 1.5115\n",
            "108/108 - 7s - loss: 1.3407 - val_loss: 1.5155\n",
            "108/108 - 7s - loss: 1.3491 - val_loss: 1.5038\n",
            "108/108 - 7s - loss: 1.3488 - val_loss: 1.4940\n",
            "108/108 - 7s - loss: 1.3388 - val_loss: 1.4986\n",
            "108/108 - 7s - loss: 1.3349 - val_loss: 1.5042\n",
            "108/108 - 7s - loss: 1.3333 - val_loss: 1.4982\n",
            "108/108 - 7s - loss: 1.3360 - val_loss: 1.4889\n",
            "108/108 - 7s - loss: 1.3445 - val_loss: 1.5143\n",
            "108/108 - 7s - loss: 1.3446 - val_loss: 1.4835\n",
            "108/108 - 7s - loss: 1.3543 - val_loss: 1.5113\n",
            "108/108 - 7s - loss: 1.3784 - val_loss: 1.4817\n",
            "108/108 - 7s - loss: 1.3531 - val_loss: 1.5130\n",
            "108/108 - 7s - loss: 1.3467 - val_loss: 1.4621\n",
            "108/108 - 7s - loss: 1.3329 - val_loss: 1.4910\n",
            "108/108 - 7s - loss: 1.3422 - val_loss: 1.4888\n",
            "108/108 - 7s - loss: 1.3382 - val_loss: 1.4769\n",
            "108/108 - 7s - loss: 1.3290 - val_loss: 1.4987\n",
            "108/108 - 7s - loss: 1.3327 - val_loss: 1.4838\n",
            "108/108 - 7s - loss: 1.3450 - val_loss: 1.4859\n",
            "108/108 - 7s - loss: 1.3250 - val_loss: 1.4871\n",
            "108/108 - 7s - loss: 1.3369 - val_loss: 1.5161\n",
            "108/108 - 7s - loss: 1.3352 - val_loss: 1.5046\n",
            "108/108 - 7s - loss: 1.3288 - val_loss: 1.5156\n",
            "108/108 - 7s - loss: 1.3147 - val_loss: 1.5274\n",
            "108/108 - 7s - loss: 1.3228 - val_loss: 1.5270\n",
            "108/108 - 7s - loss: 1.3274 - val_loss: 1.5409\n",
            "108/108 - 7s - loss: 1.3204 - val_loss: 1.5321\n",
            "108/108 - 7s - loss: 1.3241 - val_loss: 1.5066\n",
            "108/108 - 7s - loss: 1.3373 - val_loss: 1.4986\n",
            "108/108 - 7s - loss: 1.3486 - val_loss: 1.5260\n",
            "108/108 - 7s - loss: 1.3236 - val_loss: 1.5178\n",
            "108/108 - 7s - loss: 1.3489 - val_loss: 1.5213\n",
            "108/108 - 7s - loss: 1.3296 - val_loss: 1.5173\n",
            "108/108 - 7s - loss: 1.3285 - val_loss: 1.5108\n",
            "108/108 - 7s - loss: 1.3220 - val_loss: 1.5123\n",
            "108/108 - 7s - loss: 1.3266 - val_loss: 1.4835\n",
            "108/108 - 7s - loss: 1.3161 - val_loss: 1.5194\n",
            "108/108 - 7s - loss: 1.3311 - val_loss: 1.5044\n",
            "108/108 - 7s - loss: 1.3097 - val_loss: 1.5349\n",
            "108/108 - 7s - loss: 1.3091 - val_loss: 1.5160\n",
            "108/108 - 7s - loss: 1.3150 - val_loss: 1.5069\n",
            "108/108 - 7s - loss: 1.3252 - val_loss: 1.5227\n",
            "108/108 - 7s - loss: 1.3201 - val_loss: 1.4962\n",
            "108/108 - 7s - loss: 1.3122 - val_loss: 1.5036\n",
            "108/108 - 7s - loss: 1.3199 - val_loss: 1.5019\n",
            "108/108 - 7s - loss: 1.3288 - val_loss: 1.4872\n",
            "108/108 - 7s - loss: 1.3209 - val_loss: 1.5107\n",
            "108/108 - 7s - loss: 1.3240 - val_loss: 1.4841\n",
            "108/108 - 7s - loss: 1.3158 - val_loss: 1.4767\n",
            "108/108 - 7s - loss: 1.3057 - val_loss: 1.4775\n",
            "108/108 - 7s - loss: 1.3042 - val_loss: 1.5018\n",
            "108/108 - 7s - loss: 1.3032 - val_loss: 1.4863\n",
            "108/108 - 7s - loss: 1.2973 - val_loss: 1.4971\n",
            "108/108 - 7s - loss: 1.3033 - val_loss: 1.4805\n",
            "108/108 - 7s - loss: 1.3178 - val_loss: 1.4822\n",
            "108/108 - 7s - loss: 1.3168 - val_loss: 1.4683\n",
            "108/108 - 7s - loss: 1.3182 - val_loss: 1.4721\n",
            "108/108 - 7s - loss: 1.2989 - val_loss: 1.4650\n",
            "108/108 - 7s - loss: 1.3077 - val_loss: 1.4722\n",
            "108/108 - 7s - loss: 1.3146 - val_loss: 1.4976\n",
            "108/108 - 7s - loss: 1.3039 - val_loss: 1.4935\n",
            "108/108 - 7s - loss: 1.3023 - val_loss: 1.4909\n",
            "108/108 - 7s - loss: 1.2859 - val_loss: 1.4830\n",
            "108/108 - 7s - loss: 1.2782 - val_loss: 1.5151\n",
            "108/108 - 7s - loss: 1.2982 - val_loss: 1.4993\n",
            "108/108 - 7s - loss: 1.2966 - val_loss: 1.4976\n",
            "108/108 - 7s - loss: 1.2960 - val_loss: 1.4803\n",
            "108/108 - 7s - loss: 1.2793 - val_loss: 1.5169\n",
            "108/108 - 7s - loss: 1.2959 - val_loss: 1.5581\n",
            "108/108 - 7s - loss: 1.3191 - val_loss: 1.5164\n",
            "108/108 - 7s - loss: 1.3062 - val_loss: 1.5419\n",
            "108/108 - 7s - loss: 1.3089 - val_loss: 1.4919\n",
            "108/108 - 7s - loss: 1.2995 - val_loss: 1.4908\n",
            "108/108 - 7s - loss: 1.2963 - val_loss: 1.4461\n",
            "108/108 - 7s - loss: 1.2932 - val_loss: 1.5009\n",
            "108/108 - 7s - loss: 1.2834 - val_loss: 1.4861\n",
            "108/108 - 7s - loss: 1.2898 - val_loss: 1.4570\n",
            "108/108 - 7s - loss: 1.2731 - val_loss: 1.4668\n",
            "108/108 - 7s - loss: 1.2794 - val_loss: 1.4675\n",
            "108/108 - 7s - loss: 1.2745 - val_loss: 1.4622\n",
            "108/108 - 7s - loss: 1.2893 - val_loss: 1.4642\n",
            "108/108 - 7s - loss: 1.2924 - val_loss: 1.4650\n",
            "108/108 - 7s - loss: 1.3013 - val_loss: 1.4571\n",
            "108/108 - 7s - loss: 1.2914 - val_loss: 1.4711\n",
            "108/108 - 7s - loss: 1.2981 - val_loss: 1.4474\n",
            "108/108 - 7s - loss: 1.2974 - val_loss: 1.4698\n",
            "108/108 - 7s - loss: 1.3105 - val_loss: 1.4838\n",
            "108/108 - 7s - loss: 1.3173 - val_loss: 1.5211\n",
            "108/108 - 7s - loss: 1.3234 - val_loss: 1.4865\n",
            "108/108 - 7s - loss: 1.2925 - val_loss: 1.4504\n",
            "108/108 - 7s - loss: 1.2850 - val_loss: 1.4437\n",
            "108/108 - 7s - loss: 1.2806 - val_loss: 1.4720\n",
            "108/108 - 7s - loss: 1.2778 - val_loss: 1.4594\n",
            "108/108 - 7s - loss: 1.2825 - val_loss: 1.4945\n",
            "108/108 - 7s - loss: 1.2880 - val_loss: 1.4626\n",
            "108/108 - 7s - loss: 1.2884 - val_loss: 1.4750\n",
            "108/108 - 7s - loss: 1.2794 - val_loss: 1.4701\n",
            "108/108 - 7s - loss: 1.2794 - val_loss: 1.4424\n",
            "108/108 - 7s - loss: 1.2777 - val_loss: 1.4724\n",
            "108/108 - 7s - loss: 1.2766 - val_loss: 1.4422\n",
            "108/108 - 7s - loss: 1.2763 - val_loss: 1.4411\n",
            "108/108 - 7s - loss: 1.2841 - val_loss: 1.4798\n",
            "108/108 - 7s - loss: 1.2676 - val_loss: 1.5098\n",
            "108/108 - 7s - loss: 1.3082 - val_loss: 1.4790\n",
            "108/108 - 7s - loss: 1.3019 - val_loss: 1.5391\n",
            "108/108 - 7s - loss: 1.2893 - val_loss: 1.5082\n",
            "108/108 - 7s - loss: 1.2679 - val_loss: 1.4878\n",
            "108/108 - 7s - loss: 1.2537 - val_loss: 1.4517\n",
            "108/108 - 7s - loss: 1.2604 - val_loss: 1.4646\n",
            "108/108 - 7s - loss: 1.2633 - val_loss: 1.4744\n",
            "108/108 - 7s - loss: 1.2606 - val_loss: 1.4430\n",
            "108/108 - 7s - loss: 1.2817 - val_loss: 1.4416\n",
            "108/108 - 7s - loss: 1.2645 - val_loss: 1.4797\n",
            "108/108 - 7s - loss: 1.2605 - val_loss: 1.5007\n",
            "108/108 - 7s - loss: 1.2829 - val_loss: 1.4996\n",
            "108/108 - 7s - loss: 1.2762 - val_loss: 1.4885\n",
            "108/108 - 7s - loss: 1.2653 - val_loss: 1.4639\n",
            "108/108 - 7s - loss: 1.2764 - val_loss: 1.4442\n",
            "108/108 - 7s - loss: 1.2719 - val_loss: 1.4553\n",
            "108/108 - 7s - loss: 1.2553 - val_loss: 1.4496\n",
            "108/108 - 7s - loss: 1.2646 - val_loss: 1.4848\n",
            "108/108 - 7s - loss: 1.2574 - val_loss: 1.5066\n",
            "108/108 - 7s - loss: 1.2417 - val_loss: 1.4722\n",
            "108/108 - 7s - loss: 1.2530 - val_loss: 1.4984\n",
            "108/108 - 7s - loss: 1.2550 - val_loss: 1.4596\n",
            "108/108 - 7s - loss: 1.2562 - val_loss: 1.4750\n",
            "108/108 - 7s - loss: 1.2426 - val_loss: 1.4577\n",
            "108/108 - 7s - loss: 1.2467 - val_loss: 1.5241\n",
            "108/108 - 7s - loss: 1.2414 - val_loss: 1.4646\n",
            "108/108 - 7s - loss: 1.2550 - val_loss: 1.5125\n",
            "108/108 - 7s - loss: 1.2514 - val_loss: 1.4725\n",
            "108/108 - 7s - loss: 1.2467 - val_loss: 1.4773\n",
            "108/108 - 7s - loss: 1.2499 - val_loss: 1.5059\n",
            "108/108 - 7s - loss: 1.2424 - val_loss: 1.4676\n",
            "108/108 - 7s - loss: 1.2364 - val_loss: 1.4926\n",
            "108/108 - 7s - loss: 1.2378 - val_loss: 1.5040\n",
            "108/108 - 7s - loss: 1.2384 - val_loss: 1.4905\n",
            "108/108 - 7s - loss: 1.2473 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.3344 - val_loss: 1.5068\n",
            "108/108 - 7s - loss: 1.2959 - val_loss: 1.5441\n",
            "108/108 - 7s - loss: 1.2693 - val_loss: 1.5202\n",
            "108/108 - 7s - loss: 1.2718 - val_loss: 1.4817\n",
            "108/108 - 7s - loss: 1.2743 - val_loss: 1.4795\n",
            "108/108 - 7s - loss: 1.2664 - val_loss: 1.5277\n",
            "108/108 - 7s - loss: 1.2459 - val_loss: 1.4981\n",
            "108/108 - 7s - loss: 1.2386 - val_loss: 1.4657\n",
            "108/108 - 7s - loss: 1.2394 - val_loss: 1.4673\n",
            "108/108 - 7s - loss: 1.2391 - val_loss: 1.4843\n",
            "108/108 - 7s - loss: 1.2317 - val_loss: 1.5097\n",
            "108/108 - 7s - loss: 1.2376 - val_loss: 1.4839\n",
            "108/108 - 7s - loss: 1.2314 - val_loss: 1.5372\n",
            "108/108 - 7s - loss: 1.2200 - val_loss: 1.5431\n",
            "108/108 - 7s - loss: 1.2235 - val_loss: 1.4876\n",
            "108/108 - 7s - loss: 1.2623 - val_loss: 1.4527\n",
            "108/108 - 7s - loss: 1.2396 - val_loss: 1.4631\n",
            "108/108 - 7s - loss: 1.2265 - val_loss: 1.4595\n",
            "108/108 - 7s - loss: 1.2261 - val_loss: 1.4577\n",
            "108/108 - 7s - loss: 1.2212 - val_loss: 1.4650\n",
            "108/108 - 7s - loss: 1.2103 - val_loss: 1.4970\n",
            "108/108 - 7s - loss: 1.2095 - val_loss: 1.5187\n",
            "0.2\n",
            "108/108 - 14s - loss: 3.5450 - val_loss: 3.5703\n",
            "108/108 - 7s - loss: 3.3583 - val_loss: 3.3788\n",
            "108/108 - 7s - loss: 3.2110 - val_loss: 3.2636\n",
            "108/108 - 7s - loss: 3.1061 - val_loss: 3.1737\n",
            "108/108 - 7s - loss: 3.0231 - val_loss: 3.1002\n",
            "108/108 - 7s - loss: 2.9601 - val_loss: 3.0393\n",
            "108/108 - 7s - loss: 2.9092 - val_loss: 2.9872\n",
            "108/108 - 7s - loss: 2.8573 - val_loss: 2.9419\n",
            "108/108 - 7s - loss: 2.8158 - val_loss: 2.9029\n",
            "108/108 - 7s - loss: 2.7868 - val_loss: 2.8689\n",
            "108/108 - 7s - loss: 2.7518 - val_loss: 2.8393\n",
            "108/108 - 7s - loss: 2.7286 - val_loss: 2.8132\n",
            "108/108 - 7s - loss: 2.7102 - val_loss: 2.7918\n",
            "108/108 - 7s - loss: 2.6942 - val_loss: 2.7728\n",
            "108/108 - 7s - loss: 2.6764 - val_loss: 2.7570\n",
            "108/108 - 7s - loss: 2.6601 - val_loss: 2.7431\n",
            "108/108 - 7s - loss: 2.6538 - val_loss: 2.7311\n",
            "108/108 - 7s - loss: 2.6428 - val_loss: 2.7208\n",
            "108/108 - 7s - loss: 2.6525 - val_loss: 2.7125\n",
            "108/108 - 7s - loss: 2.6316 - val_loss: 2.7047\n",
            "108/108 - 7s - loss: 2.6220 - val_loss: 2.6971\n",
            "108/108 - 7s - loss: 2.6287 - val_loss: 2.6914\n",
            "108/108 - 7s - loss: 2.6190 - val_loss: 2.6864\n",
            "108/108 - 7s - loss: 2.6009 - val_loss: 2.6816\n",
            "108/108 - 7s - loss: 2.6155 - val_loss: 2.6777\n",
            "108/108 - 7s - loss: 2.6156 - val_loss: 2.6746\n",
            "108/108 - 7s - loss: 2.6160 - val_loss: 2.6721\n",
            "108/108 - 7s - loss: 2.6262 - val_loss: 2.6696\n",
            "108/108 - 7s - loss: 2.6059 - val_loss: 2.6670\n",
            "108/108 - 7s - loss: 2.6011 - val_loss: 2.6649\n",
            "108/108 - 7s - loss: 2.5942 - val_loss: 2.6633\n",
            "108/108 - 7s - loss: 2.5862 - val_loss: 2.6611\n",
            "108/108 - 7s - loss: 2.6126 - val_loss: 2.6598\n",
            "108/108 - 7s - loss: 2.5919 - val_loss: 2.6588\n",
            "108/108 - 7s - loss: 2.6119 - val_loss: 2.6580\n",
            "108/108 - 7s - loss: 2.5926 - val_loss: 2.6568\n",
            "108/108 - 7s - loss: 2.6106 - val_loss: 2.6562\n",
            "108/108 - 7s - loss: 2.5989 - val_loss: 2.6553\n",
            "108/108 - 7s - loss: 2.6084 - val_loss: 2.6553\n",
            "108/108 - 7s - loss: 2.5988 - val_loss: 2.6551\n",
            "108/108 - 7s - loss: 2.6084 - val_loss: 2.6547\n",
            "108/108 - 7s - loss: 2.5966 - val_loss: 2.6542\n",
            "108/108 - 7s - loss: 2.5993 - val_loss: 2.6534\n",
            "108/108 - 7s - loss: 2.6005 - val_loss: 2.6529\n",
            "108/108 - 7s - loss: 2.6016 - val_loss: 2.6525\n",
            "108/108 - 7s - loss: 2.5792 - val_loss: 2.6522\n",
            "108/108 - 7s - loss: 2.5899 - val_loss: 2.6519\n",
            "108/108 - 7s - loss: 2.5855 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.5929 - val_loss: 2.6507\n",
            "108/108 - 7s - loss: 2.6009 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.6067 - val_loss: 2.6498\n",
            "108/108 - 7s - loss: 2.6111 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.6184 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.5995 - val_loss: 2.6910\n",
            "108/108 - 7s - loss: 2.6228 - val_loss: 2.6858\n",
            "108/108 - 7s - loss: 2.6159 - val_loss: 2.6814\n",
            "108/108 - 7s - loss: 2.6038 - val_loss: 2.6634\n",
            "108/108 - 7s - loss: 2.6041 - val_loss: 2.6576\n",
            "108/108 - 7s - loss: 2.5949 - val_loss: 2.6562\n",
            "108/108 - 7s - loss: 2.5791 - val_loss: 2.6553\n",
            "108/108 - 7s - loss: 2.6001 - val_loss: 2.6549\n",
            "108/108 - 7s - loss: 2.5879 - val_loss: 2.6537\n",
            "108/108 - 7s - loss: 2.5921 - val_loss: 2.6533\n",
            "108/108 - 7s - loss: 2.5909 - val_loss: 2.6527\n",
            "108/108 - 7s - loss: 2.6171 - val_loss: 2.6527\n",
            "108/108 - 7s - loss: 2.6048 - val_loss: 2.6527\n",
            "108/108 - 7s - loss: 2.6083 - val_loss: 2.6527\n",
            "108/108 - 7s - loss: 2.6053 - val_loss: 2.6525\n",
            "108/108 - 7s - loss: 2.6122 - val_loss: 2.6528\n",
            "108/108 - 7s - loss: 2.6029 - val_loss: 2.6526\n",
            "108/108 - 7s - loss: 2.6031 - val_loss: 2.6524\n",
            "108/108 - 7s - loss: 2.5907 - val_loss: 2.6523\n",
            "108/108 - 7s - loss: 2.6066 - val_loss: 2.6525\n",
            "108/108 - 7s - loss: 2.5899 - val_loss: 2.6518\n",
            "108/108 - 7s - loss: 2.5962 - val_loss: 2.6514\n",
            "108/108 - 7s - loss: 2.5900 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.6043 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.5895 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.6080 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.5992 - val_loss: 2.6512\n",
            "108/108 - 7s - loss: 2.6192 - val_loss: 2.6517\n",
            "108/108 - 7s - loss: 2.6017 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.6127 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.6108 - val_loss: 2.6516\n",
            "108/108 - 7s - loss: 2.6007 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.6214 - val_loss: 2.6514\n",
            "108/108 - 7s - loss: 2.6164 - val_loss: 2.6516\n",
            "108/108 - 7s - loss: 2.6092 - val_loss: 2.6520\n",
            "108/108 - 7s - loss: 2.5995 - val_loss: 2.6523\n",
            "108/108 - 7s - loss: 2.5855 - val_loss: 2.6518\n",
            "108/108 - 7s - loss: 2.5995 - val_loss: 2.6512\n",
            "108/108 - 7s - loss: 2.5980 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.6043 - val_loss: 2.6517\n",
            "108/108 - 7s - loss: 2.6046 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.5976 - val_loss: 2.6509\n",
            "108/108 - 7s - loss: 2.6070 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.5915 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.5891 - val_loss: 2.6500\n",
            "108/108 - 7s - loss: 2.6110 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.6165 - val_loss: 2.6507\n",
            "108/108 - 7s - loss: 2.5920 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.6040 - val_loss: 2.6501\n",
            "108/108 - 7s - loss: 2.6088 - val_loss: 2.6501\n",
            "108/108 - 7s - loss: 2.6012 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6032 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6120 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.5893 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.5945 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6053 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.5843 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.6044 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.5921 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6071 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6042 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.6023 - val_loss: 2.6507\n",
            "108/108 - 7s - loss: 2.6030 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6086 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.6107 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.6178 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.5987 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6098 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.6025 - val_loss: 2.6509\n",
            "108/108 - 7s - loss: 2.6017 - val_loss: 2.6509\n",
            "108/108 - 7s - loss: 2.6055 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.5961 - val_loss: 2.6509\n",
            "108/108 - 7s - loss: 2.5906 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6024 - val_loss: 2.6507\n",
            "108/108 - 7s - loss: 2.5916 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6016 - val_loss: 2.6507\n",
            "108/108 - 7s - loss: 2.6019 - val_loss: 2.6507\n",
            "108/108 - 7s - loss: 2.6015 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.6113 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.5887 - val_loss: 2.6512\n",
            "108/108 - 7s - loss: 2.5839 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.5998 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.6030 - val_loss: 2.6509\n",
            "108/108 - 7s - loss: 2.6089 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.5975 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.5990 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.6037 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.5901 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.5921 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.6055 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6015 - val_loss: 2.6509\n",
            "108/108 - 7s - loss: 2.5943 - val_loss: 2.6509\n",
            "108/108 - 7s - loss: 2.6015 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6025 - val_loss: 2.6507\n",
            "108/108 - 7s - loss: 2.5976 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.5993 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.5961 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6145 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.5949 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.5854 - val_loss: 2.6501\n",
            "108/108 - 7s - loss: 2.6031 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.6061 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6022 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.6132 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.6089 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.6031 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.5982 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.5978 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.6108 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.6055 - val_loss: 2.6515\n",
            "108/108 - 7s - loss: 2.6053 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.5986 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.5899 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.6011 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.6068 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6088 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.5968 - val_loss: 2.6501\n",
            "108/108 - 7s - loss: 2.5989 - val_loss: 2.6500\n",
            "108/108 - 7s - loss: 2.5987 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.5993 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.6016 - val_loss: 2.6501\n",
            "108/108 - 7s - loss: 2.5942 - val_loss: 2.6495\n",
            "108/108 - 7s - loss: 2.6059 - val_loss: 2.6494\n",
            "108/108 - 7s - loss: 2.6138 - val_loss: 2.6495\n",
            "108/108 - 7s - loss: 2.6011 - val_loss: 2.6492\n",
            "108/108 - 7s - loss: 2.6062 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.6050 - val_loss: 2.6498\n",
            "108/108 - 7s - loss: 2.6084 - val_loss: 2.6498\n",
            "108/108 - 7s - loss: 2.6007 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.5944 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.6030 - val_loss: 2.6498\n",
            "108/108 - 7s - loss: 2.5984 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.6135 - val_loss: 2.6500\n",
            "108/108 - 7s - loss: 2.5820 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.5915 - val_loss: 2.6494\n",
            "108/108 - 7s - loss: 2.5863 - val_loss: 2.6492\n",
            "108/108 - 7s - loss: 2.6027 - val_loss: 2.6492\n",
            "108/108 - 7s - loss: 2.5834 - val_loss: 2.6484\n",
            "108/108 - 7s - loss: 2.6007 - val_loss: 2.6483\n",
            "108/108 - 7s - loss: 2.6054 - val_loss: 2.6482\n",
            "108/108 - 7s - loss: 2.6037 - val_loss: 2.6485\n",
            "108/108 - 7s - loss: 2.5871 - val_loss: 2.6484\n",
            "108/108 - 7s - loss: 2.5998 - val_loss: 2.6489\n",
            "108/108 - 7s - loss: 2.6087 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.5941 - val_loss: 2.6490\n",
            "108/108 - 7s - loss: 2.5962 - val_loss: 2.6491\n",
            "108/108 - 7s - loss: 2.5951 - val_loss: 2.6489\n",
            "108/108 - 7s - loss: 2.5839 - val_loss: 2.6485\n",
            "108/108 - 7s - loss: 2.6016 - val_loss: 2.6486\n",
            "108/108 - 7s - loss: 2.6063 - val_loss: 2.6486\n",
            "108/108 - 7s - loss: 2.6105 - val_loss: 2.6491\n",
            "108/108 - 7s - loss: 2.6143 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.5953 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.5810 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.5975 - val_loss: 2.6494\n",
            "108/108 - 7s - loss: 2.5917 - val_loss: 2.6491\n",
            "108/108 - 7s - loss: 2.6065 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.6099 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.6031 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.5983 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.6081 - val_loss: 2.6500\n",
            "108/108 - 7s - loss: 2.6006 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.6030 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.6043 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.6013 - val_loss: 2.6501\n",
            "108/108 - 7s - loss: 2.5985 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.5866 - val_loss: 2.6498\n",
            "108/108 - 7s - loss: 2.5993 - val_loss: 2.6498\n",
            "108/108 - 7s - loss: 2.6008 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.6093 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.6000 - val_loss: 2.6495\n",
            "108/108 - 7s - loss: 2.5895 - val_loss: 2.6492\n",
            "108/108 - 7s - loss: 2.5974 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.5906 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.6026 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.5991 - val_loss: 2.6492\n",
            "108/108 - 7s - loss: 2.5915 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.5977 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.6058 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.5902 - val_loss: 2.6492\n",
            "108/108 - 7s - loss: 2.6085 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.5793 - val_loss: 2.6489\n",
            "108/108 - 7s - loss: 2.6225 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.6019 - val_loss: 2.6499\n",
            "108/108 - 7s - loss: 2.6188 - val_loss: 2.6503\n",
            "108/108 - 7s - loss: 2.6126 - val_loss: 2.6503\n",
            "108/108 - 7s - loss: 2.5860 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6037 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6089 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6003 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.5989 - val_loss: 2.6501\n",
            "108/108 - 7s - loss: 2.5956 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.5839 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.6096 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.5865 - val_loss: 2.6498\n",
            "108/108 - 7s - loss: 2.5949 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.5932 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.5933 - val_loss: 2.6493\n",
            "108/108 - 7s - loss: 2.6131 - val_loss: 2.6497\n",
            "108/108 - 7s - loss: 2.5987 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.5909 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.5988 - val_loss: 2.6501\n",
            "108/108 - 7s - loss: 2.6015 - val_loss: 2.6503\n",
            "108/108 - 7s - loss: 2.5976 - val_loss: 2.6500\n",
            "108/108 - 7s - loss: 2.6115 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6014 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.5895 - val_loss: 2.6500\n",
            "108/108 - 7s - loss: 2.6052 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.5757 - val_loss: 2.6502\n",
            "108/108 - 7s - loss: 2.5945 - val_loss: 2.6500\n",
            "108/108 - 7s - loss: 2.6120 - val_loss: 2.6504\n",
            "108/108 - 7s - loss: 2.6081 - val_loss: 2.6507\n",
            "108/108 - 7s - loss: 2.5881 - val_loss: 2.6509\n",
            "108/108 - 7s - loss: 2.6100 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.6024 - val_loss: 2.6514\n",
            "108/108 - 7s - loss: 2.6000 - val_loss: 2.6514\n",
            "108/108 - 7s - loss: 2.6092 - val_loss: 2.6514\n",
            "108/108 - 7s - loss: 2.5953 - val_loss: 2.6512\n",
            "108/108 - 7s - loss: 2.6011 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.5991 - val_loss: 2.6510\n",
            "108/108 - 7s - loss: 2.5904 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.5931 - val_loss: 2.6500\n",
            "108/108 - 7s - loss: 2.6068 - val_loss: 2.6505\n",
            "108/108 - 7s - loss: 2.5920 - val_loss: 2.6511\n",
            "108/108 - 7s - loss: 2.6125 - val_loss: 2.6520\n",
            "108/108 - 7s - loss: 2.5960 - val_loss: 2.6508\n",
            "108/108 - 7s - loss: 2.5986 - val_loss: 2.6573\n",
            "108/108 - 7s - loss: 2.5827 - val_loss: 2.6483\n",
            "108/108 - 7s - loss: 2.6015 - val_loss: 2.6490\n",
            "108/108 - 7s - loss: 2.5913 - val_loss: 2.6486\n",
            "108/108 - 7s - loss: 2.5972 - val_loss: 2.6486\n",
            "108/108 - 7s - loss: 2.5927 - val_loss: 2.6476\n",
            "108/108 - 7s - loss: 2.5857 - val_loss: 2.6475\n",
            "108/108 - 7s - loss: 2.5928 - val_loss: 2.6478\n",
            "108/108 - 7s - loss: 2.5894 - val_loss: 2.6482\n",
            "108/108 - 7s - loss: 2.6171 - val_loss: 2.6489\n",
            "108/108 - 7s - loss: 2.5829 - val_loss: 2.6487\n",
            "108/108 - 7s - loss: 2.5861 - val_loss: 2.6487\n",
            "108/108 - 7s - loss: 2.6106 - val_loss: 2.6489\n",
            "108/108 - 7s - loss: 2.6040 - val_loss: 2.6491\n",
            "108/108 - 7s - loss: 2.5842 - val_loss: 2.6492\n",
            "108/108 - 7s - loss: 2.5918 - val_loss: 2.6488\n",
            "108/108 - 7s - loss: 2.6077 - val_loss: 2.6486\n",
            "108/108 - 7s - loss: 2.6164 - val_loss: 2.6494\n",
            "108/108 - 7s - loss: 2.5803 - val_loss: 2.6492\n",
            "108/108 - 7s - loss: 2.6071 - val_loss: 2.6491\n",
            "108/108 - 7s - loss: 2.5916 - val_loss: 2.6494\n",
            "108/108 - 7s - loss: 2.5778 - val_loss: 2.6489\n",
            "108/108 - 7s - loss: 2.5945 - val_loss: 2.6477\n",
            "108/108 - 7s - loss: 2.5911 - val_loss: 2.6481\n",
            "108/108 - 7s - loss: 2.5819 - val_loss: 2.6506\n",
            "108/108 - 7s - loss: 2.5748 - val_loss: 2.6417\n",
            "108/108 - 7s - loss: 2.5775 - val_loss: 2.6278\n",
            "108/108 - 7s - loss: 2.5752 - val_loss: 2.6891\n",
            "108/108 - 7s - loss: 2.5983 - val_loss: 2.6579\n",
            "108/108 - 7s - loss: 2.6105 - val_loss: 2.6560\n",
            "108/108 - 7s - loss: 2.5893 - val_loss: 2.6552\n",
            "108/108 - 7s - loss: 2.5969 - val_loss: 2.6539\n",
            "108/108 - 7s - loss: 2.5936 - val_loss: 2.6513\n",
            "108/108 - 7s - loss: 2.5988 - val_loss: 2.6485\n",
            "108/108 - 7s - loss: 2.6028 - val_loss: 2.6396\n",
            "108/108 - 7s - loss: 2.6031 - val_loss: 2.6412\n",
            "108/108 - 7s - loss: 2.6117 - val_loss: 2.6447\n",
            "108/108 - 7s - loss: 2.5986 - val_loss: 2.6428\n",
            "108/108 - 7s - loss: 2.5680 - val_loss: 2.6152\n",
            "108/108 - 7s - loss: 2.5590 - val_loss: 2.6238\n",
            "108/108 - 7s - loss: 2.6065 - val_loss: 2.6359\n",
            "108/108 - 7s - loss: 2.5868 - val_loss: 2.6301\n",
            "108/108 - 7s - loss: 2.5921 - val_loss: 2.6462\n",
            "108/108 - 7s - loss: 2.5871 - val_loss: 2.6381\n",
            "108/108 - 7s - loss: 2.5640 - val_loss: 2.6396\n",
            "108/108 - 7s - loss: 2.5693 - val_loss: 2.6284\n",
            "108/108 - 7s - loss: 2.5567 - val_loss: 2.6236\n",
            "108/108 - 7s - loss: 2.5500 - val_loss: 2.6155\n",
            "108/108 - 7s - loss: 2.5455 - val_loss: 2.6118\n",
            "108/108 - 7s - loss: 2.5288 - val_loss: 2.6179\n",
            "108/108 - 7s - loss: 2.5363 - val_loss: 2.5895\n",
            "108/108 - 7s - loss: 2.5330 - val_loss: 2.5927\n",
            "108/108 - 7s - loss: 2.5395 - val_loss: 2.5928\n",
            "108/108 - 7s - loss: 2.5202 - val_loss: 2.5903\n",
            "108/108 - 7s - loss: 2.5188 - val_loss: 2.5927\n",
            "108/108 - 7s - loss: 2.4951 - val_loss: 2.5848\n",
            "108/108 - 7s - loss: 2.5054 - val_loss: 2.5743\n",
            "108/108 - 7s - loss: 2.5369 - val_loss: 2.5881\n",
            "108/108 - 7s - loss: 2.5189 - val_loss: 2.5848\n",
            "108/108 - 7s - loss: 2.5336 - val_loss: 2.5927\n",
            "108/108 - 7s - loss: 2.5693 - val_loss: 2.6246\n",
            "108/108 - 7s - loss: 2.5540 - val_loss: 2.6226\n",
            "108/108 - 7s - loss: 2.5616 - val_loss: 2.6410\n",
            "108/108 - 7s - loss: 2.5473 - val_loss: 2.6007\n",
            "108/108 - 7s - loss: 2.5555 - val_loss: 2.6059\n",
            "108/108 - 7s - loss: 2.5213 - val_loss: 2.5860\n",
            "108/108 - 7s - loss: 2.5117 - val_loss: 2.5845\n",
            "108/108 - 7s - loss: 2.4987 - val_loss: 2.5764\n",
            "108/108 - 7s - loss: 2.5018 - val_loss: 2.5681\n",
            "108/108 - 7s - loss: 2.4988 - val_loss: 2.5749\n",
            "108/108 - 7s - loss: 2.4904 - val_loss: 2.5837\n",
            "108/108 - 7s - loss: 2.4985 - val_loss: 2.5602\n",
            "108/108 - 7s - loss: 2.4970 - val_loss: 2.5536\n",
            "108/108 - 7s - loss: 2.4958 - val_loss: 2.5569\n",
            "108/108 - 7s - loss: 2.4785 - val_loss: 2.5714\n",
            "108/108 - 7s - loss: 2.5062 - val_loss: 2.5660\n",
            "108/108 - 7s - loss: 2.4886 - val_loss: 2.5752\n",
            "108/108 - 7s - loss: 2.5191 - val_loss: 2.5868\n",
            "108/108 - 7s - loss: 2.4800 - val_loss: 2.5473\n",
            "108/108 - 7s - loss: 2.5176 - val_loss: 2.5566\n",
            "108/108 - 7s - loss: 2.4992 - val_loss: 2.5689\n",
            "108/108 - 7s - loss: 2.5005 - val_loss: 2.5552\n",
            "108/108 - 7s - loss: 2.4853 - val_loss: 2.5530\n",
            "108/108 - 7s - loss: 2.4920 - val_loss: 2.5405\n",
            "108/108 - 7s - loss: 2.4792 - val_loss: 2.5531\n",
            "108/108 - 7s - loss: 2.4685 - val_loss: 2.5488\n",
            "108/108 - 7s - loss: 2.4840 - val_loss: 2.5506\n",
            "108/108 - 7s - loss: 2.4701 - val_loss: 2.5389\n",
            "108/108 - 7s - loss: 2.4566 - val_loss: 2.5518\n",
            "108/108 - 7s - loss: 2.4685 - val_loss: 2.5313\n",
            "108/108 - 7s - loss: 2.4729 - val_loss: 2.5131\n",
            "108/108 - 7s - loss: 2.4669 - val_loss: 2.5143\n",
            "108/108 - 7s - loss: 2.4618 - val_loss: 2.5173\n",
            "108/108 - 7s - loss: 2.4635 - val_loss: 2.5083\n",
            "108/108 - 7s - loss: 2.4557 - val_loss: 2.5032\n",
            "108/108 - 7s - loss: 2.4411 - val_loss: 2.5110\n",
            "108/108 - 7s - loss: 2.4259 - val_loss: 2.5031\n",
            "108/108 - 7s - loss: 2.4393 - val_loss: 2.5133\n",
            "108/108 - 7s - loss: 2.4254 - val_loss: 2.5009\n",
            "108/108 - 7s - loss: 2.4233 - val_loss: 2.5056\n",
            "108/108 - 7s - loss: 2.4224 - val_loss: 2.4913\n",
            "108/108 - 7s - loss: 2.4509 - val_loss: 2.5013\n",
            "108/108 - 7s - loss: 2.4204 - val_loss: 2.4915\n",
            "108/108 - 7s - loss: 2.4123 - val_loss: 2.4840\n",
            "108/108 - 7s - loss: 2.4196 - val_loss: 2.4859\n",
            "108/108 - 7s - loss: 2.4206 - val_loss: 2.5344\n",
            "108/108 - 7s - loss: 2.4364 - val_loss: 2.5150\n",
            "108/108 - 7s - loss: 2.4171 - val_loss: 2.4878\n",
            "108/108 - 7s - loss: 2.4147 - val_loss: 2.4823\n",
            "108/108 - 7s - loss: 2.4456 - val_loss: 2.5221\n",
            "108/108 - 7s - loss: 2.4170 - val_loss: 2.4813\n",
            "108/108 - 7s - loss: 2.4111 - val_loss: 2.4805\n",
            "108/108 - 7s - loss: 2.4219 - val_loss: 2.4664\n",
            "108/108 - 7s - loss: 2.3951 - val_loss: 2.4992\n",
            "108/108 - 7s - loss: 2.4085 - val_loss: 2.4805\n",
            "108/108 - 7s - loss: 2.3944 - val_loss: 2.4902\n",
            "108/108 - 7s - loss: 2.3816 - val_loss: 2.4879\n",
            "108/108 - 7s - loss: 2.3796 - val_loss: 2.4976\n",
            "108/108 - 7s - loss: 2.3862 - val_loss: 2.4572\n",
            "108/108 - 7s - loss: 2.3832 - val_loss: 2.4627\n",
            "108/108 - 7s - loss: 2.3769 - val_loss: 2.4709\n",
            "108/108 - 7s - loss: 2.3790 - val_loss: 2.4668\n",
            "108/108 - 7s - loss: 2.3590 - val_loss: 2.4673\n",
            "108/108 - 7s - loss: 2.3659 - val_loss: 2.4589\n",
            "108/108 - 7s - loss: 2.3509 - val_loss: 2.4600\n",
            "108/108 - 7s - loss: 2.3608 - val_loss: 2.4773\n",
            "108/108 - 7s - loss: 2.3387 - val_loss: 2.4581\n",
            "108/108 - 7s - loss: 2.3815 - val_loss: 2.4745\n",
            "108/108 - 7s - loss: 2.3813 - val_loss: 2.4794\n",
            "108/108 - 7s - loss: 2.3480 - val_loss: 2.4461\n",
            "108/108 - 7s - loss: 2.3601 - val_loss: 2.4780\n",
            "108/108 - 7s - loss: 2.3425 - val_loss: 2.4456\n",
            "108/108 - 7s - loss: 2.3599 - val_loss: 2.4619\n",
            "108/108 - 7s - loss: 2.3549 - val_loss: 2.4595\n",
            "108/108 - 7s - loss: 2.3396 - val_loss: 2.4571\n",
            "108/108 - 7s - loss: 2.3601 - val_loss: 2.4712\n",
            "108/108 - 7s - loss: 2.3229 - val_loss: 2.4698\n",
            "108/108 - 7s - loss: 2.3240 - val_loss: 2.4507\n",
            "108/108 - 7s - loss: 2.3153 - val_loss: 2.4700\n",
            "108/108 - 7s - loss: 2.3165 - val_loss: 2.4528\n",
            "108/108 - 7s - loss: 2.3487 - val_loss: 2.4572\n",
            "108/108 - 7s - loss: 2.3152 - val_loss: 2.4641\n",
            "108/108 - 7s - loss: 2.3134 - val_loss: 2.4636\n",
            "108/108 - 7s - loss: 2.3515 - val_loss: 2.4523\n",
            "108/108 - 7s - loss: 2.3397 - val_loss: 2.4618\n",
            "108/108 - 7s - loss: 2.3156 - val_loss: 2.4625\n",
            "108/108 - 7s - loss: 2.3202 - val_loss: 2.4481\n",
            "108/108 - 7s - loss: 2.3047 - val_loss: 2.4538\n",
            "108/108 - 7s - loss: 2.2911 - val_loss: 2.4395\n",
            "108/108 - 7s - loss: 2.3157 - val_loss: 2.4415\n",
            "108/108 - 7s - loss: 2.3147 - val_loss: 2.4366\n",
            "108/108 - 7s - loss: 2.3100 - val_loss: 2.4721\n",
            "108/108 - 7s - loss: 2.3116 - val_loss: 2.4458\n",
            "108/108 - 7s - loss: 2.2982 - val_loss: 2.4293\n",
            "108/108 - 7s - loss: 2.3229 - val_loss: 2.4586\n",
            "108/108 - 7s - loss: 2.2985 - val_loss: 2.4463\n",
            "108/108 - 7s - loss: 2.2817 - val_loss: 2.4570\n",
            "108/108 - 7s - loss: 2.3049 - val_loss: 2.4175\n",
            "108/108 - 7s - loss: 2.3229 - val_loss: 2.4497\n",
            "108/108 - 7s - loss: 2.3237 - val_loss: 2.4518\n",
            "108/108 - 7s - loss: 2.3205 - val_loss: 2.4307\n",
            "108/108 - 7s - loss: 2.3024 - val_loss: 2.4435\n",
            "108/108 - 7s - loss: 2.2981 - val_loss: 2.4420\n",
            "108/108 - 7s - loss: 2.3066 - val_loss: 2.4440\n",
            "108/108 - 7s - loss: 2.3373 - val_loss: 2.4373\n",
            "108/108 - 7s - loss: 2.2984 - val_loss: 2.4453\n",
            "108/108 - 7s - loss: 2.2874 - val_loss: 2.4584\n",
            "108/108 - 7s - loss: 2.2975 - val_loss: 2.4743\n",
            "108/108 - 7s - loss: 2.2991 - val_loss: 2.4475\n",
            "108/108 - 7s - loss: 2.2821 - val_loss: 2.4459\n",
            "108/108 - 7s - loss: 2.2908 - val_loss: 2.4543\n",
            "108/108 - 7s - loss: 2.2910 - val_loss: 2.4633\n",
            "108/108 - 7s - loss: 2.3031 - val_loss: 2.4646\n",
            "108/108 - 7s - loss: 2.2661 - val_loss: 2.4735\n",
            "108/108 - 7s - loss: 2.2473 - val_loss: 2.4713\n",
            "108/108 - 7s - loss: 2.2746 - val_loss: 2.4781\n",
            "108/108 - 7s - loss: 2.2823 - val_loss: 2.4599\n",
            "108/108 - 7s - loss: 2.2741 - val_loss: 2.4428\n",
            "108/108 - 7s - loss: 2.2795 - val_loss: 2.4881\n",
            "108/108 - 7s - loss: 2.2531 - val_loss: 2.4948\n",
            "108/108 - 7s - loss: 2.2522 - val_loss: 2.4798\n",
            "108/108 - 7s - loss: 2.2601 - val_loss: 2.4520\n",
            "108/108 - 7s - loss: 2.2476 - val_loss: 2.4968\n",
            "108/108 - 7s - loss: 2.2480 - val_loss: 2.4349\n",
            "108/108 - 7s - loss: 2.2596 - val_loss: 2.4471\n",
            "108/108 - 7s - loss: 2.2657 - val_loss: 2.4492\n",
            "108/108 - 7s - loss: 2.2763 - val_loss: 2.4594\n",
            "108/108 - 7s - loss: 2.2443 - val_loss: 2.4744\n",
            "108/108 - 7s - loss: 2.2466 - val_loss: 2.4203\n",
            "108/108 - 7s - loss: 2.2677 - val_loss: 2.4505\n",
            "108/108 - 7s - loss: 2.2383 - val_loss: 2.4667\n",
            "108/108 - 7s - loss: 2.2409 - val_loss: 2.4645\n",
            "108/108 - 7s - loss: 2.2377 - val_loss: 2.4734\n",
            "108/108 - 7s - loss: 2.2459 - val_loss: 2.4785\n",
            "108/108 - 7s - loss: 2.2375 - val_loss: 2.4346\n",
            "108/108 - 7s - loss: 2.2156 - val_loss: 2.4797\n",
            "108/108 - 7s - loss: 2.2370 - val_loss: 2.4569\n",
            "108/108 - 7s - loss: 2.2131 - val_loss: 2.4560\n",
            "108/108 - 7s - loss: 2.2094 - val_loss: 2.4483\n",
            "108/108 - 7s - loss: 2.2319 - val_loss: 2.4300\n",
            "108/108 - 7s - loss: 2.2297 - val_loss: 2.4519\n",
            "108/108 - 7s - loss: 2.2233 - val_loss: 2.4911\n",
            "108/108 - 7s - loss: 2.2338 - val_loss: 2.4617\n",
            "108/108 - 7s - loss: 2.2029 - val_loss: 2.4484\n",
            "108/108 - 7s - loss: 2.2076 - val_loss: 2.4461\n",
            "108/108 - 7s - loss: 2.2014 - val_loss: 2.4437\n",
            "108/108 - 7s - loss: 2.2036 - val_loss: 2.4352\n",
            "108/108 - 7s - loss: 2.2204 - val_loss: 2.4398\n",
            "108/108 - 7s - loss: 2.2014 - val_loss: 2.4417\n",
            "108/108 - 7s - loss: 2.2121 - val_loss: 2.4492\n",
            "108/108 - 7s - loss: 2.2285 - val_loss: 2.4543\n",
            "108/108 - 7s - loss: 2.1919 - val_loss: 2.4644\n",
            "108/108 - 7s - loss: 2.2086 - val_loss: 2.4477\n",
            "108/108 - 7s - loss: 2.1983 - val_loss: 2.4352\n",
            "108/108 - 7s - loss: 2.1870 - val_loss: 2.4735\n",
            "108/108 - 7s - loss: 2.1829 - val_loss: 2.4363\n",
            "108/108 - 7s - loss: 2.2123 - val_loss: 2.4695\n",
            "108/108 - 7s - loss: 2.2318 - val_loss: 2.4152\n",
            "108/108 - 7s - loss: 2.2001 - val_loss: 2.4364\n",
            "108/108 - 7s - loss: 2.1990 - val_loss: 2.4474\n",
            "108/108 - 7s - loss: 2.1753 - val_loss: 2.4568\n",
            "0.30000000000000004\n",
            "108/108 - 14s - loss: 5.2876 - val_loss: 5.2972\n",
            "108/108 - 7s - loss: 4.9993 - val_loss: 5.0371\n",
            "108/108 - 7s - loss: 4.7804 - val_loss: 4.8518\n",
            "108/108 - 7s - loss: 4.6074 - val_loss: 4.6966\n",
            "108/108 - 7s - loss: 4.4648 - val_loss: 4.5627\n",
            "108/108 - 7s - loss: 4.3280 - val_loss: 4.4435\n",
            "108/108 - 7s - loss: 4.2245 - val_loss: 4.3371\n",
            "108/108 - 7s - loss: 4.1363 - val_loss: 4.2423\n",
            "108/108 - 7s - loss: 4.0364 - val_loss: 4.1548\n",
            "108/108 - 7s - loss: 3.9659 - val_loss: 4.0754\n",
            "108/108 - 7s - loss: 3.8996 - val_loss: 4.0036\n",
            "108/108 - 7s - loss: 3.8288 - val_loss: 3.9399\n",
            "108/108 - 7s - loss: 3.7749 - val_loss: 3.8832\n",
            "108/108 - 7s - loss: 3.7309 - val_loss: 3.8323\n",
            "108/108 - 7s - loss: 3.6949 - val_loss: 3.7870\n",
            "108/108 - 7s - loss: 3.6422 - val_loss: 3.7445\n",
            "108/108 - 7s - loss: 3.6002 - val_loss: 3.7056\n",
            "108/108 - 7s - loss: 3.5867 - val_loss: 3.6720\n",
            "108/108 - 7s - loss: 3.5602 - val_loss: 3.6425\n",
            "108/108 - 7s - loss: 3.5319 - val_loss: 3.6149\n",
            "108/108 - 7s - loss: 3.5126 - val_loss: 3.5914\n",
            "108/108 - 7s - loss: 3.4995 - val_loss: 3.5696\n",
            "108/108 - 7s - loss: 3.4820 - val_loss: 3.5503\n",
            "108/108 - 7s - loss: 3.4665 - val_loss: 3.5331\n",
            "108/108 - 7s - loss: 3.4516 - val_loss: 3.5178\n",
            "108/108 - 7s - loss: 3.4412 - val_loss: 3.5040\n",
            "108/108 - 7s - loss: 3.4229 - val_loss: 3.4912\n",
            "108/108 - 7s - loss: 3.4246 - val_loss: 3.4811\n",
            "108/108 - 7s - loss: 3.4184 - val_loss: 3.4724\n",
            "108/108 - 7s - loss: 3.4356 - val_loss: 3.4639\n",
            "108/108 - 7s - loss: 3.4201 - val_loss: 3.4572\n",
            "108/108 - 7s - loss: 3.4178 - val_loss: 3.4507\n",
            "108/108 - 7s - loss: 3.4108 - val_loss: 3.4456\n",
            "108/108 - 7s - loss: 3.4070 - val_loss: 3.4412\n",
            "108/108 - 7s - loss: 3.3980 - val_loss: 3.4368\n",
            "108/108 - 7s - loss: 3.4126 - val_loss: 3.4336\n",
            "108/108 - 7s - loss: 3.4346 - val_loss: 3.4320\n",
            "108/108 - 7s - loss: 3.4181 - val_loss: 3.4297\n",
            "108/108 - 7s - loss: 3.3867 - val_loss: 3.4265\n",
            "108/108 - 7s - loss: 3.4326 - val_loss: 3.4260\n",
            "108/108 - 7s - loss: 3.4313 - val_loss: 3.4251\n",
            "108/108 - 7s - loss: 3.3763 - val_loss: 3.4234\n",
            "108/108 - 7s - loss: 3.4282 - val_loss: 3.4226\n",
            "108/108 - 7s - loss: 3.3943 - val_loss: 3.4211\n",
            "108/108 - 7s - loss: 3.4195 - val_loss: 3.4211\n",
            "108/108 - 7s - loss: 3.4058 - val_loss: 3.4205\n",
            "108/108 - 7s - loss: 3.4101 - val_loss: 3.4198\n",
            "108/108 - 7s - loss: 3.3888 - val_loss: 3.4189\n",
            "108/108 - 7s - loss: 3.3981 - val_loss: 3.4178\n",
            "108/108 - 7s - loss: 3.4050 - val_loss: 3.4171\n",
            "108/108 - 7s - loss: 3.3999 - val_loss: 3.4172\n",
            "108/108 - 7s - loss: 3.4079 - val_loss: 3.4164\n",
            "108/108 - 7s - loss: 3.3992 - val_loss: 3.4166\n",
            "108/108 - 7s - loss: 3.4151 - val_loss: 3.4167\n",
            "108/108 - 7s - loss: 3.3771 - val_loss: 3.4146\n",
            "108/108 - 7s - loss: 3.4142 - val_loss: 3.4148\n",
            "108/108 - 7s - loss: 3.4133 - val_loss: 3.4148\n",
            "108/108 - 7s - loss: 3.3997 - val_loss: 3.4143\n",
            "108/108 - 7s - loss: 3.3936 - val_loss: 3.4134\n",
            "108/108 - 7s - loss: 3.3840 - val_loss: 3.4131\n",
            "108/108 - 7s - loss: 3.4023 - val_loss: 3.4133\n",
            "108/108 - 7s - loss: 3.3707 - val_loss: 3.4131\n",
            "108/108 - 7s - loss: 3.3978 - val_loss: 3.4139\n",
            "108/108 - 7s - loss: 3.3850 - val_loss: 3.4136\n",
            "108/108 - 7s - loss: 3.4258 - val_loss: 3.4138\n",
            "108/108 - 7s - loss: 3.3910 - val_loss: 3.4134\n",
            "108/108 - 7s - loss: 3.4255 - val_loss: 3.4141\n",
            "108/108 - 7s - loss: 3.4400 - val_loss: 3.4145\n",
            "108/108 - 7s - loss: 3.4126 - val_loss: 3.4141\n",
            "108/108 - 7s - loss: 3.4115 - val_loss: 3.4144\n",
            "108/108 - 7s - loss: 3.4212 - val_loss: 3.4139\n",
            "108/108 - 7s - loss: 3.4098 - val_loss: 3.4137\n",
            "108/108 - 7s - loss: 3.3936 - val_loss: 3.4131\n",
            "108/108 - 7s - loss: 3.4020 - val_loss: 3.4129\n",
            "108/108 - 7s - loss: 3.3954 - val_loss: 3.4138\n",
            "108/108 - 7s - loss: 3.4175 - val_loss: 3.4321\n",
            "108/108 - 7s - loss: 3.4077 - val_loss: 3.4178\n",
            "108/108 - 7s - loss: 3.3985 - val_loss: 3.4153\n",
            "108/108 - 7s - loss: 3.4365 - val_loss: 3.4159\n",
            "108/108 - 7s - loss: 3.4069 - val_loss: 3.4126\n",
            "108/108 - 7s - loss: 3.4057 - val_loss: 3.4114\n",
            "108/108 - 7s - loss: 3.4098 - val_loss: 3.4113\n",
            "108/108 - 7s - loss: 3.4025 - val_loss: 3.4112\n",
            "108/108 - 7s - loss: 3.3954 - val_loss: 3.4112\n",
            "108/108 - 7s - loss: 3.4018 - val_loss: 3.4115\n",
            "108/108 - 7s - loss: 3.3961 - val_loss: 3.4112\n",
            "108/108 - 7s - loss: 3.3976 - val_loss: 3.4114\n",
            "108/108 - 7s - loss: 3.4032 - val_loss: 3.4120\n",
            "108/108 - 7s - loss: 3.3927 - val_loss: 3.4114\n",
            "108/108 - 7s - loss: 3.3988 - val_loss: 3.4117\n",
            "108/108 - 7s - loss: 3.4019 - val_loss: 3.4124\n",
            "108/108 - 7s - loss: 3.4168 - val_loss: 3.4125\n",
            "108/108 - 7s - loss: 3.4080 - val_loss: 3.4129\n",
            "108/108 - 7s - loss: 3.4138 - val_loss: 3.4122\n",
            "108/108 - 7s - loss: 3.4013 - val_loss: 3.4120\n",
            "108/108 - 7s - loss: 3.4105 - val_loss: 3.4118\n",
            "108/108 - 7s - loss: 3.4237 - val_loss: 3.4123\n",
            "108/108 - 7s - loss: 3.4175 - val_loss: 3.4118\n",
            "108/108 - 7s - loss: 4.1141 - val_loss: 3.7425\n",
            "108/108 - 7s - loss: 3.3891 - val_loss: 3.4092\n",
            "108/108 - 7s - loss: 3.4063 - val_loss: 3.4118\n",
            "108/108 - 7s - loss: 3.4354 - val_loss: 3.4832\n",
            "108/108 - 7s - loss: 3.4084 - val_loss: 3.4282\n",
            "108/108 - 7s - loss: 3.4043 - val_loss: 3.4328\n",
            "108/108 - 7s - loss: 3.3758 - val_loss: 3.4034\n",
            "108/108 - 7s - loss: 3.3621 - val_loss: 3.3991\n",
            "108/108 - 7s - loss: 3.3412 - val_loss: 3.3874\n",
            "108/108 - 7s - loss: 3.3147 - val_loss: 3.3424\n",
            "108/108 - 7s - loss: 3.4092 - val_loss: 3.3558\n",
            "108/108 - 7s - loss: 3.3549 - val_loss: 3.3203\n",
            "108/108 - 7s - loss: 3.2810 - val_loss: 3.3066\n",
            "108/108 - 7s - loss: 3.2560 - val_loss: 3.3231\n",
            "108/108 - 7s - loss: 3.2336 - val_loss: 3.2715\n",
            "108/108 - 7s - loss: 3.2254 - val_loss: 3.2561\n",
            "108/108 - 7s - loss: 3.2053 - val_loss: 3.2512\n",
            "108/108 - 7s - loss: 3.1960 - val_loss: 3.2265\n",
            "108/108 - 7s - loss: 3.1772 - val_loss: 3.2323\n",
            "108/108 - 7s - loss: 3.1890 - val_loss: 3.2158\n",
            "108/108 - 7s - loss: 3.1314 - val_loss: 3.1691\n",
            "108/108 - 7s - loss: 3.1607 - val_loss: 3.2280\n",
            "108/108 - 7s - loss: 3.1266 - val_loss: 3.1991\n",
            "108/108 - 7s - loss: 3.0868 - val_loss: 3.1532\n",
            "108/108 - 7s - loss: 3.1157 - val_loss: 3.1779\n",
            "108/108 - 7s - loss: 3.1148 - val_loss: 3.1759\n",
            "108/108 - 7s - loss: 3.0818 - val_loss: 3.1455\n",
            "108/108 - 7s - loss: 3.0676 - val_loss: 3.1303\n",
            "108/108 - 7s - loss: 3.0653 - val_loss: 3.1428\n",
            "108/108 - 7s - loss: 3.0645 - val_loss: 3.1162\n",
            "108/108 - 7s - loss: 3.0791 - val_loss: 3.1034\n",
            "108/108 - 7s - loss: 3.0549 - val_loss: 3.1067\n",
            "108/108 - 7s - loss: 3.0433 - val_loss: 3.0680\n",
            "108/108 - 7s - loss: 3.0794 - val_loss: 3.0706\n",
            "108/108 - 7s - loss: 3.0429 - val_loss: 3.0909\n",
            "108/108 - 7s - loss: 3.0424 - val_loss: 3.0850\n",
            "108/108 - 7s - loss: 3.0238 - val_loss: 3.1080\n",
            "108/108 - 7s - loss: 3.0244 - val_loss: 3.0685\n",
            "108/108 - 7s - loss: 3.0111 - val_loss: 3.0581\n",
            "108/108 - 7s - loss: 3.0519 - val_loss: 3.0518\n",
            "108/108 - 7s - loss: 3.0095 - val_loss: 3.0733\n",
            "108/108 - 7s - loss: 3.0078 - val_loss: 3.0532\n",
            "108/108 - 7s - loss: 2.9992 - val_loss: 3.0402\n",
            "108/108 - 7s - loss: 2.9794 - val_loss: 3.0194\n",
            "108/108 - 7s - loss: 2.9693 - val_loss: 3.0252\n",
            "108/108 - 7s - loss: 2.9738 - val_loss: 3.0339\n",
            "108/108 - 7s - loss: 2.9889 - val_loss: 3.0295\n",
            "108/108 - 7s - loss: 2.9655 - val_loss: 2.9942\n",
            "108/108 - 7s - loss: 2.9559 - val_loss: 3.0061\n",
            "108/108 - 7s - loss: 2.9089 - val_loss: 2.9871\n",
            "108/108 - 7s - loss: 2.9562 - val_loss: 2.9945\n",
            "108/108 - 7s - loss: 2.9641 - val_loss: 2.9948\n",
            "108/108 - 7s - loss: 2.9686 - val_loss: 3.0029\n",
            "108/108 - 7s - loss: 2.9393 - val_loss: 2.9975\n",
            "108/108 - 7s - loss: 2.9485 - val_loss: 2.9865\n",
            "108/108 - 7s - loss: 2.9130 - val_loss: 3.0067\n",
            "108/108 - 7s - loss: 2.9564 - val_loss: 2.9687\n",
            "108/108 - 7s - loss: 2.9243 - val_loss: 2.9743\n",
            "108/108 - 7s - loss: 2.9112 - val_loss: 2.9963\n",
            "108/108 - 7s - loss: 2.9432 - val_loss: 2.9819\n",
            "108/108 - 7s - loss: 2.9313 - val_loss: 2.9687\n",
            "108/108 - 7s - loss: 2.9089 - val_loss: 2.9598\n",
            "108/108 - 7s - loss: 2.8999 - val_loss: 2.9407\n",
            "108/108 - 7s - loss: 2.9389 - val_loss: 2.9477\n",
            "108/108 - 7s - loss: 2.9134 - val_loss: 2.9846\n",
            "108/108 - 7s - loss: 2.8698 - val_loss: 2.9464\n",
            "108/108 - 7s - loss: 2.8955 - val_loss: 2.9519\n",
            "108/108 - 7s - loss: 2.9116 - val_loss: 2.9577\n",
            "108/108 - 7s - loss: 2.8771 - val_loss: 2.9523\n",
            "108/108 - 7s - loss: 2.8756 - val_loss: 2.9276\n",
            "108/108 - 7s - loss: 2.8802 - val_loss: 2.9424\n",
            "108/108 - 7s - loss: 2.8622 - val_loss: 2.9302\n",
            "108/108 - 7s - loss: 2.9105 - val_loss: 2.9865\n",
            "108/108 - 7s - loss: 2.8688 - val_loss: 2.9597\n",
            "108/108 - 7s - loss: 2.8855 - val_loss: 2.9670\n",
            "108/108 - 7s - loss: 2.8965 - val_loss: 2.9337\n",
            "108/108 - 7s - loss: 2.8568 - val_loss: 2.9273\n",
            "108/108 - 7s - loss: 2.8968 - val_loss: 2.9167\n",
            "108/108 - 7s - loss: 2.8485 - val_loss: 2.9415\n",
            "108/108 - 7s - loss: 2.8884 - val_loss: 2.9264\n",
            "108/108 - 7s - loss: 2.8428 - val_loss: 2.9272\n",
            "108/108 - 7s - loss: 2.8159 - val_loss: 2.9205\n",
            "108/108 - 7s - loss: 2.8265 - val_loss: 2.9044\n",
            "108/108 - 7s - loss: 2.8430 - val_loss: 2.9307\n",
            "108/108 - 7s - loss: 2.8668 - val_loss: 2.9580\n",
            "108/108 - 7s - loss: 2.8411 - val_loss: 2.8982\n",
            "108/108 - 7s - loss: 2.8222 - val_loss: 2.9086\n",
            "108/108 - 7s - loss: 2.8615 - val_loss: 2.9176\n",
            "108/108 - 7s - loss: 2.8507 - val_loss: 2.9506\n",
            "108/108 - 7s - loss: 2.8286 - val_loss: 2.9169\n",
            "108/108 - 7s - loss: 2.8371 - val_loss: 2.8980\n",
            "108/108 - 7s - loss: 2.8279 - val_loss: 2.9277\n",
            "108/108 - 7s - loss: 2.8606 - val_loss: 2.9312\n",
            "108/108 - 7s - loss: 2.7986 - val_loss: 2.9001\n",
            "108/108 - 7s - loss: 2.8576 - val_loss: 2.9281\n",
            "108/108 - 7s - loss: 2.8337 - val_loss: 2.9622\n",
            "108/108 - 7s - loss: 2.8223 - val_loss: 2.9189\n",
            "108/108 - 7s - loss: 2.8014 - val_loss: 2.9464\n",
            "108/108 - 7s - loss: 2.8304 - val_loss: 2.9291\n",
            "108/108 - 7s - loss: 2.8292 - val_loss: 2.9215\n",
            "108/108 - 7s - loss: 2.8383 - val_loss: 2.9351\n",
            "108/108 - 7s - loss: 2.8049 - val_loss: 2.9411\n",
            "108/108 - 7s - loss: 2.8251 - val_loss: 2.9560\n",
            "108/108 - 7s - loss: 2.7973 - val_loss: 2.9240\n",
            "108/108 - 7s - loss: 2.8397 - val_loss: 2.9393\n",
            "108/108 - 7s - loss: 2.8039 - val_loss: 2.9548\n",
            "108/108 - 7s - loss: 2.8040 - val_loss: 2.9881\n",
            "108/108 - 7s - loss: 2.7876 - val_loss: 2.9214\n",
            "108/108 - 7s - loss: 2.8074 - val_loss: 2.9174\n",
            "108/108 - 7s - loss: 2.7873 - val_loss: 2.9278\n",
            "108/108 - 7s - loss: 2.7695 - val_loss: 2.9687\n",
            "108/108 - 7s - loss: 2.8148 - val_loss: 2.9259\n",
            "108/108 - 7s - loss: 2.7795 - val_loss: 2.8869\n",
            "108/108 - 7s - loss: 2.7727 - val_loss: 2.9258\n",
            "108/108 - 7s - loss: 2.7917 - val_loss: 2.9446\n",
            "108/108 - 7s - loss: 2.7659 - val_loss: 2.8956\n",
            "108/108 - 7s - loss: 2.7828 - val_loss: 2.9082\n",
            "108/108 - 7s - loss: 2.7706 - val_loss: 2.8885\n",
            "108/108 - 7s - loss: 2.7729 - val_loss: 2.9364\n",
            "108/108 - 7s - loss: 2.7459 - val_loss: 2.9503\n",
            "108/108 - 7s - loss: 2.7723 - val_loss: 3.0174\n",
            "108/108 - 7s - loss: 2.7633 - val_loss: 2.9560\n",
            "108/108 - 7s - loss: 2.7829 - val_loss: 2.9551\n",
            "108/108 - 7s - loss: 2.7746 - val_loss: 2.9507\n",
            "108/108 - 7s - loss: 2.7702 - val_loss: 2.9480\n",
            "108/108 - 7s - loss: 2.7589 - val_loss: 2.9930\n",
            "108/108 - 7s - loss: 2.7396 - val_loss: 2.9289\n",
            "108/108 - 7s - loss: 2.7927 - val_loss: 2.9304\n",
            "108/108 - 7s - loss: 2.7714 - val_loss: 2.9655\n",
            "108/108 - 7s - loss: 2.7922 - val_loss: 2.9176\n",
            "108/108 - 7s - loss: 2.7887 - val_loss: 2.9471\n",
            "108/108 - 7s - loss: 2.7468 - val_loss: 2.9312\n",
            "108/108 - 7s - loss: 2.7761 - val_loss: 2.9776\n",
            "108/108 - 7s - loss: 2.7651 - val_loss: 2.9846\n",
            "108/108 - 7s - loss: 2.7969 - val_loss: 2.9875\n",
            "108/108 - 7s - loss: 2.7631 - val_loss: 2.9339\n",
            "108/108 - 7s - loss: 2.7746 - val_loss: 2.9514\n",
            "108/108 - 7s - loss: 2.7462 - val_loss: 2.9676\n",
            "108/108 - 7s - loss: 2.7408 - val_loss: 2.9521\n",
            "108/108 - 7s - loss: 2.7894 - val_loss: 2.9622\n",
            "108/108 - 7s - loss: 2.7401 - val_loss: 2.9405\n",
            "108/108 - 7s - loss: 2.7632 - val_loss: 2.9485\n",
            "108/108 - 7s - loss: 2.7402 - val_loss: 2.9476\n",
            "108/108 - 7s - loss: 2.7369 - val_loss: 2.9492\n",
            "108/108 - 7s - loss: 2.6978 - val_loss: 3.0115\n",
            "108/108 - 7s - loss: 2.7341 - val_loss: 2.9477\n",
            "108/108 - 7s - loss: 2.7497 - val_loss: 2.9207\n",
            "108/108 - 7s - loss: 2.7540 - val_loss: 2.9537\n",
            "108/108 - 7s - loss: 2.7296 - val_loss: 2.9121\n",
            "108/108 - 7s - loss: 2.6907 - val_loss: 2.9183\n",
            "108/108 - 7s - loss: 2.7077 - val_loss: 2.8910\n",
            "108/108 - 7s - loss: 2.7192 - val_loss: 2.8501\n",
            "108/108 - 7s - loss: 2.7340 - val_loss: 2.9971\n",
            "108/108 - 7s - loss: 2.7490 - val_loss: 2.9440\n",
            "108/108 - 7s - loss: 2.7144 - val_loss: 2.9947\n",
            "108/108 - 7s - loss: 2.7282 - val_loss: 2.9223\n",
            "108/108 - 7s - loss: 2.7182 - val_loss: 2.9295\n",
            "108/108 - 7s - loss: 2.7340 - val_loss: 2.9704\n",
            "108/108 - 7s - loss: 2.6901 - val_loss: 2.9269\n",
            "108/108 - 7s - loss: 2.6895 - val_loss: 3.0049\n",
            "108/108 - 7s - loss: 2.6972 - val_loss: 2.9859\n",
            "108/108 - 7s - loss: 2.6905 - val_loss: 2.9794\n",
            "108/108 - 7s - loss: 2.7109 - val_loss: 2.9468\n",
            "108/108 - 7s - loss: 2.7133 - val_loss: 2.9720\n",
            "108/108 - 7s - loss: 2.7155 - val_loss: 2.9861\n",
            "108/108 - 7s - loss: 2.6769 - val_loss: 2.8794\n",
            "108/108 - 7s - loss: 2.7052 - val_loss: 2.9527\n",
            "108/108 - 7s - loss: 2.7026 - val_loss: 2.9477\n",
            "108/108 - 7s - loss: 2.7308 - val_loss: 2.8592\n",
            "108/108 - 7s - loss: 2.7254 - val_loss: 2.9229\n",
            "108/108 - 7s - loss: 2.7063 - val_loss: 2.9129\n",
            "108/108 - 7s - loss: 2.6583 - val_loss: 2.9203\n",
            "108/108 - 7s - loss: 2.6946 - val_loss: 2.8841\n",
            "108/108 - 7s - loss: 2.7027 - val_loss: 2.9213\n",
            "108/108 - 7s - loss: 2.6860 - val_loss: 2.9298\n",
            "108/108 - 7s - loss: 2.6531 - val_loss: 2.9359\n",
            "108/108 - 7s - loss: 2.6893 - val_loss: 2.8830\n",
            "108/108 - 7s - loss: 2.6522 - val_loss: 2.9325\n",
            "108/108 - 7s - loss: 2.6501 - val_loss: 2.9233\n",
            "108/108 - 7s - loss: 2.6653 - val_loss: 2.9528\n",
            "108/108 - 7s - loss: 2.6611 - val_loss: 2.8408\n",
            "108/108 - 7s - loss: 2.6665 - val_loss: 2.9009\n",
            "108/108 - 7s - loss: 2.6416 - val_loss: 2.9235\n",
            "108/108 - 7s - loss: 2.6402 - val_loss: 2.9265\n",
            "108/108 - 7s - loss: 2.6352 - val_loss: 2.9487\n",
            "108/108 - 7s - loss: 2.6808 - val_loss: 2.9622\n",
            "108/108 - 7s - loss: 2.6537 - val_loss: 2.9383\n",
            "108/108 - 7s - loss: 2.6208 - val_loss: 2.9428\n",
            "108/108 - 7s - loss: 2.6327 - val_loss: 2.9697\n",
            "108/108 - 7s - loss: 2.6427 - val_loss: 2.8986\n",
            "108/108 - 7s - loss: 2.6185 - val_loss: 2.9101\n",
            "108/108 - 7s - loss: 2.6535 - val_loss: 2.9182\n",
            "108/108 - 7s - loss: 2.5977 - val_loss: 2.9110\n",
            "108/108 - 7s - loss: 2.6245 - val_loss: 2.9351\n",
            "108/108 - 7s - loss: 2.6497 - val_loss: 2.9254\n",
            "108/108 - 7s - loss: 2.6429 - val_loss: 2.9788\n",
            "108/108 - 7s - loss: 2.6275 - val_loss: 2.9272\n",
            "108/108 - 7s - loss: 2.6742 - val_loss: 2.9096\n",
            "108/108 - 7s - loss: 2.6654 - val_loss: 2.9213\n",
            "108/108 - 7s - loss: 2.6193 - val_loss: 2.9545\n",
            "108/108 - 7s - loss: 2.6451 - val_loss: 2.9318\n",
            "108/108 - 7s - loss: 2.6287 - val_loss: 2.9057\n",
            "108/108 - 7s - loss: 2.7152 - val_loss: 2.9068\n",
            "108/108 - 7s - loss: 2.6527 - val_loss: 2.9375\n",
            "108/108 - 7s - loss: 2.6408 - val_loss: 2.9131\n",
            "108/108 - 7s - loss: 2.5932 - val_loss: 2.9223\n",
            "108/108 - 7s - loss: 2.6222 - val_loss: 2.8577\n",
            "108/108 - 7s - loss: 2.6128 - val_loss: 2.9358\n",
            "108/108 - 7s - loss: 2.6152 - val_loss: 2.9471\n",
            "108/108 - 7s - loss: 2.6141 - val_loss: 2.8940\n",
            "108/108 - 7s - loss: 2.5900 - val_loss: 2.8982\n",
            "108/108 - 7s - loss: 2.5747 - val_loss: 2.9117\n",
            "108/108 - 7s - loss: 2.6089 - val_loss: 2.9474\n",
            "108/108 - 7s - loss: 2.6135 - val_loss: 2.9527\n",
            "108/108 - 7s - loss: 2.5902 - val_loss: 2.9867\n",
            "108/108 - 7s - loss: 2.6035 - val_loss: 2.8856\n",
            "108/108 - 7s - loss: 2.5993 - val_loss: 2.8745\n",
            "108/108 - 7s - loss: 2.5810 - val_loss: 2.9194\n",
            "108/108 - 7s - loss: 2.6224 - val_loss: 2.9168\n",
            "108/108 - 7s - loss: 2.6228 - val_loss: 2.9432\n",
            "108/108 - 7s - loss: 2.6026 - val_loss: 2.9493\n",
            "108/108 - 7s - loss: 2.5977 - val_loss: 2.9936\n",
            "108/108 - 7s - loss: 2.6077 - val_loss: 2.9088\n",
            "108/108 - 7s - loss: 2.5861 - val_loss: 2.9696\n",
            "108/108 - 7s - loss: 2.5814 - val_loss: 2.9468\n",
            "108/108 - 7s - loss: 2.6028 - val_loss: 2.9576\n",
            "108/108 - 7s - loss: 2.5553 - val_loss: 2.9254\n",
            "108/108 - 7s - loss: 2.6157 - val_loss: 2.8826\n",
            "108/108 - 7s - loss: 2.5910 - val_loss: 2.9467\n",
            "108/108 - 7s - loss: 2.5747 - val_loss: 2.8935\n",
            "108/108 - 7s - loss: 2.5732 - val_loss: 2.9189\n",
            "108/108 - 7s - loss: 2.5949 - val_loss: 2.9125\n",
            "108/108 - 7s - loss: 2.5761 - val_loss: 2.8954\n",
            "108/108 - 7s - loss: 2.5429 - val_loss: 2.9397\n",
            "108/108 - 7s - loss: 2.5675 - val_loss: 2.9110\n",
            "108/108 - 7s - loss: 2.5912 - val_loss: 2.9311\n",
            "108/108 - 7s - loss: 2.5687 - val_loss: 2.9347\n",
            "108/108 - 7s - loss: 2.5631 - val_loss: 2.9080\n",
            "108/108 - 7s - loss: 2.5887 - val_loss: 2.8355\n",
            "108/108 - 7s - loss: 2.5366 - val_loss: 2.8686\n",
            "108/108 - 7s - loss: 2.5554 - val_loss: 2.9049\n",
            "108/108 - 7s - loss: 2.5669 - val_loss: 2.9429\n",
            "108/108 - 7s - loss: 2.5634 - val_loss: 2.9320\n",
            "108/108 - 7s - loss: 2.5622 - val_loss: 2.8865\n",
            "108/108 - 7s - loss: 2.5659 - val_loss: 2.9296\n",
            "108/108 - 7s - loss: 2.5473 - val_loss: 2.9575\n",
            "108/108 - 7s - loss: 2.5546 - val_loss: 2.8759\n",
            "108/108 - 7s - loss: 2.5436 - val_loss: 2.9070\n",
            "108/108 - 7s - loss: 2.5636 - val_loss: 2.9146\n",
            "108/108 - 7s - loss: 2.5393 - val_loss: 2.9428\n",
            "108/108 - 7s - loss: 2.5557 - val_loss: 2.9202\n",
            "108/108 - 7s - loss: 2.5709 - val_loss: 2.9117\n",
            "108/108 - 7s - loss: 2.5370 - val_loss: 2.9554\n",
            "108/108 - 7s - loss: 2.5745 - val_loss: 2.9419\n",
            "108/108 - 7s - loss: 2.5280 - val_loss: 2.8758\n",
            "108/108 - 7s - loss: 2.5319 - val_loss: 2.9480\n",
            "108/108 - 7s - loss: 2.5374 - val_loss: 2.8754\n",
            "108/108 - 7s - loss: 2.5195 - val_loss: 2.9186\n",
            "108/108 - 7s - loss: 2.5482 - val_loss: 2.8965\n",
            "108/108 - 7s - loss: 2.5598 - val_loss: 2.8382\n",
            "108/108 - 7s - loss: 2.5447 - val_loss: 2.8355\n",
            "108/108 - 7s - loss: 2.5521 - val_loss: 2.8771\n",
            "108/108 - 7s - loss: 2.5100 - val_loss: 2.8944\n",
            "108/108 - 7s - loss: 2.5421 - val_loss: 2.9115\n",
            "108/108 - 7s - loss: 2.5405 - val_loss: 2.9162\n",
            "108/108 - 7s - loss: 2.5303 - val_loss: 2.9652\n",
            "108/108 - 7s - loss: 2.5436 - val_loss: 2.9112\n",
            "108/108 - 7s - loss: 2.5105 - val_loss: 2.9127\n",
            "108/108 - 7s - loss: 2.5317 - val_loss: 2.9126\n",
            "108/108 - 7s - loss: 2.5490 - val_loss: 2.9880\n",
            "108/108 - 7s - loss: 2.5290 - val_loss: 2.8625\n",
            "108/108 - 7s - loss: 2.5076 - val_loss: 2.9626\n",
            "108/108 - 7s - loss: 2.5466 - val_loss: 2.9799\n",
            "108/108 - 7s - loss: 2.5558 - val_loss: 2.8988\n",
            "108/108 - 7s - loss: 2.5427 - val_loss: 2.9670\n",
            "108/108 - 7s - loss: 2.5225 - val_loss: 2.8843\n",
            "108/108 - 7s - loss: 2.5017 - val_loss: 2.9631\n",
            "108/108 - 7s - loss: 2.4819 - val_loss: 2.9444\n",
            "108/108 - 7s - loss: 2.5288 - val_loss: 2.9506\n",
            "108/108 - 7s - loss: 2.5304 - val_loss: 2.9324\n",
            "108/108 - 7s - loss: 2.5135 - val_loss: 2.9702\n",
            "108/108 - 7s - loss: 2.4951 - val_loss: 2.9080\n",
            "108/108 - 7s - loss: 2.5162 - val_loss: 2.9414\n",
            "108/108 - 7s - loss: 2.5379 - val_loss: 2.8330\n",
            "108/108 - 7s - loss: 2.5024 - val_loss: 2.9394\n",
            "108/108 - 7s - loss: 2.5451 - val_loss: 2.9066\n",
            "108/108 - 7s - loss: 2.4956 - val_loss: 2.8539\n",
            "108/108 - 7s - loss: 2.5383 - val_loss: 2.9417\n",
            "108/108 - 7s - loss: 2.4794 - val_loss: 2.9197\n",
            "108/108 - 7s - loss: 2.5070 - val_loss: 2.9130\n",
            "108/108 - 7s - loss: 2.5281 - val_loss: 2.9624\n",
            "108/108 - 7s - loss: 2.5282 - val_loss: 2.9170\n",
            "108/108 - 7s - loss: 2.5164 - val_loss: 2.9302\n",
            "108/108 - 7s - loss: 2.4993 - val_loss: 2.9158\n",
            "108/108 - 7s - loss: 2.4804 - val_loss: 2.9744\n",
            "108/108 - 7s - loss: 2.4927 - val_loss: 2.9768\n",
            "108/108 - 7s - loss: 2.4942 - val_loss: 2.9630\n",
            "108/108 - 7s - loss: 2.5073 - val_loss: 2.9595\n",
            "108/108 - 7s - loss: 2.5272 - val_loss: 2.9013\n",
            "108/108 - 7s - loss: 2.4948 - val_loss: 2.9255\n",
            "108/108 - 7s - loss: 2.4906 - val_loss: 2.9007\n",
            "108/108 - 7s - loss: 2.4716 - val_loss: 2.9025\n",
            "108/108 - 7s - loss: 2.4624 - val_loss: 2.8773\n",
            "108/108 - 7s - loss: 2.5067 - val_loss: 2.9075\n",
            "108/108 - 7s - loss: 2.4584 - val_loss: 2.9229\n",
            "108/108 - 7s - loss: 2.4814 - val_loss: 2.9064\n",
            "108/108 - 7s - loss: 2.4878 - val_loss: 2.9310\n",
            "108/108 - 7s - loss: 2.4446 - val_loss: 2.8990\n",
            "108/108 - 7s - loss: 2.4833 - val_loss: 2.9601\n",
            "108/108 - 7s - loss: 2.4505 - val_loss: 2.9437\n",
            "108/108 - 7s - loss: 2.4753 - val_loss: 2.9251\n",
            "108/108 - 7s - loss: 2.4853 - val_loss: 2.9541\n",
            "108/108 - 7s - loss: 2.4826 - val_loss: 2.9180\n",
            "108/108 - 7s - loss: 2.4958 - val_loss: 2.9625\n",
            "108/108 - 7s - loss: 2.4854 - val_loss: 2.9084\n",
            "108/108 - 7s - loss: 2.4820 - val_loss: 2.9197\n",
            "108/108 - 7s - loss: 2.4486 - val_loss: 2.9066\n",
            "108/108 - 7s - loss: 2.4836 - val_loss: 2.9539\n",
            "108/108 - 7s - loss: 2.4509 - val_loss: 2.9173\n",
            "108/108 - 7s - loss: 2.4621 - val_loss: 2.8839\n",
            "108/108 - 7s - loss: 2.4797 - val_loss: 2.9146\n",
            "108/108 - 7s - loss: 2.4773 - val_loss: 2.9295\n",
            "108/108 - 7s - loss: 2.4727 - val_loss: 2.9255\n",
            "108/108 - 7s - loss: 2.4709 - val_loss: 2.9296\n",
            "108/108 - 7s - loss: 2.4661 - val_loss: 2.9778\n",
            "108/108 - 7s - loss: 2.4758 - val_loss: 2.9006\n",
            "108/108 - 7s - loss: 2.4738 - val_loss: 2.8422\n",
            "108/108 - 7s - loss: 2.4627 - val_loss: 2.8009\n",
            "108/108 - 7s - loss: 2.4706 - val_loss: 2.8140\n",
            "108/108 - 7s - loss: 2.4765 - val_loss: 2.9615\n",
            "108/108 - 7s - loss: 2.4761 - val_loss: 2.8600\n",
            "108/108 - 7s - loss: 2.4793 - val_loss: 2.8729\n",
            "108/108 - 7s - loss: 2.4447 - val_loss: 2.8699\n",
            "108/108 - 7s - loss: 2.4279 - val_loss: 2.8677\n",
            "108/108 - 7s - loss: 2.4457 - val_loss: 2.9213\n",
            "108/108 - 7s - loss: 2.4399 - val_loss: 2.9794\n",
            "108/108 - 7s - loss: 2.4426 - val_loss: 2.9302\n",
            "108/108 - 7s - loss: 2.4658 - val_loss: 2.9186\n",
            "108/108 - 7s - loss: 2.4440 - val_loss: 2.8600\n",
            "108/108 - 7s - loss: 2.4464 - val_loss: 2.8200\n",
            "108/108 - 7s - loss: 2.4606 - val_loss: 2.8352\n",
            "108/108 - 7s - loss: 2.4558 - val_loss: 2.8330\n",
            "108/108 - 7s - loss: 2.4477 - val_loss: 2.8661\n",
            "108/108 - 7s - loss: 2.4504 - val_loss: 2.9152\n",
            "108/108 - 7s - loss: 2.4787 - val_loss: 2.9108\n",
            "108/108 - 7s - loss: 2.4306 - val_loss: 2.8964\n",
            "108/108 - 7s - loss: 2.4175 - val_loss: 2.8850\n",
            "108/108 - 7s - loss: 2.4152 - val_loss: 2.8928\n",
            "108/108 - 7s - loss: 2.4364 - val_loss: 2.9066\n",
            "108/108 - 7s - loss: 2.4443 - val_loss: 2.8708\n",
            "108/108 - 7s - loss: 2.4314 - val_loss: 2.9365\n",
            "108/108 - 7s - loss: 2.4442 - val_loss: 2.8824\n",
            "108/108 - 7s - loss: 2.4184 - val_loss: 2.9207\n",
            "108/108 - 7s - loss: 2.4480 - val_loss: 2.9453\n",
            "108/108 - 7s - loss: 2.4266 - val_loss: 2.9416\n",
            "108/108 - 7s - loss: 2.4293 - val_loss: 2.9278\n",
            "108/108 - 7s - loss: 2.4593 - val_loss: 2.9016\n",
            "108/108 - 7s - loss: 2.4462 - val_loss: 2.8319\n",
            "108/108 - 7s - loss: 2.4507 - val_loss: 2.9266\n",
            "108/108 - 7s - loss: 2.4442 - val_loss: 2.8807\n",
            "108/108 - 7s - loss: 2.4251 - val_loss: 2.9174\n",
            "108/108 - 7s - loss: 2.4320 - val_loss: 2.9276\n",
            "108/108 - 7s - loss: 2.4189 - val_loss: 2.8719\n",
            "108/108 - 7s - loss: 2.4200 - val_loss: 2.9656\n",
            "108/108 - 7s - loss: 2.4010 - val_loss: 2.8648\n",
            "108/108 - 7s - loss: 2.4409 - val_loss: 2.8982\n",
            "108/108 - 7s - loss: 2.4727 - val_loss: 2.8319\n",
            "108/108 - 7s - loss: 2.3966 - val_loss: 2.8555\n",
            "108/108 - 7s - loss: 2.4460 - val_loss: 2.8772\n",
            "108/108 - 7s - loss: 2.4373 - val_loss: 2.8897\n",
            "108/108 - 7s - loss: 2.4400 - val_loss: 2.8116\n",
            "108/108 - 7s - loss: 2.3965 - val_loss: 2.8203\n",
            "108/108 - 7s - loss: 2.4203 - val_loss: 2.8036\n",
            "108/108 - 7s - loss: 2.4302 - val_loss: 2.8975\n",
            "108/108 - 7s - loss: 2.4259 - val_loss: 2.8259\n",
            "108/108 - 7s - loss: 2.4323 - val_loss: 2.8786\n",
            "108/108 - 7s - loss: 2.3876 - val_loss: 2.7743\n",
            "108/108 - 7s - loss: 2.3748 - val_loss: 2.8296\n",
            "108/108 - 7s - loss: 2.4364 - val_loss: 2.8162\n",
            "108/108 - 7s - loss: 2.4326 - val_loss: 2.9132\n",
            "108/108 - 7s - loss: 2.4048 - val_loss: 2.8557\n",
            "108/108 - 7s - loss: 2.4560 - val_loss: 2.8712\n",
            "108/108 - 7s - loss: 2.4097 - val_loss: 2.8703\n",
            "108/108 - 7s - loss: 2.4089 - val_loss: 2.8817\n",
            "108/108 - 7s - loss: 2.4025 - val_loss: 2.8790\n",
            "108/108 - 7s - loss: 2.3748 - val_loss: 2.8894\n",
            "108/108 - 7s - loss: 2.3688 - val_loss: 2.8939\n",
            "108/108 - 7s - loss: 2.4057 - val_loss: 2.9342\n",
            "108/108 - 7s - loss: 2.4049 - val_loss: 2.8781\n",
            "108/108 - 7s - loss: 2.4047 - val_loss: 2.9077\n",
            "108/108 - 7s - loss: 2.3840 - val_loss: 2.8937\n",
            "108/108 - 7s - loss: 2.3895 - val_loss: 2.8552\n",
            "108/108 - 7s - loss: 2.3916 - val_loss: 2.9481\n",
            "108/108 - 7s - loss: 2.3731 - val_loss: 2.9661\n",
            "108/108 - 7s - loss: 2.3597 - val_loss: 2.9034\n",
            "108/108 - 7s - loss: 2.3867 - val_loss: 2.8357\n",
            "108/108 - 7s - loss: 2.3981 - val_loss: 2.9179\n",
            "108/108 - 7s - loss: 2.3881 - val_loss: 2.9465\n",
            "108/108 - 7s - loss: 2.4071 - val_loss: 2.8941\n",
            "108/108 - 7s - loss: 2.3979 - val_loss: 2.9066\n",
            "108/108 - 7s - loss: 2.3769 - val_loss: 2.8575\n",
            "108/108 - 7s - loss: 2.3633 - val_loss: 2.8517\n",
            "0.4\n",
            "108/108 - 14s - loss: 7.0192 - val_loss: 6.9967\n",
            "108/108 - 7s - loss: 6.6243 - val_loss: 6.6776\n",
            "108/108 - 7s - loss: 6.3295 - val_loss: 6.4226\n",
            "108/108 - 7s - loss: 6.0839 - val_loss: 6.1997\n",
            "108/108 - 7s - loss: 5.8685 - val_loss: 5.9990\n",
            "108/108 - 7s - loss: 5.6852 - val_loss: 5.8179\n",
            "108/108 - 7s - loss: 5.4993 - val_loss: 5.6500\n",
            "108/108 - 7s - loss: 5.3518 - val_loss: 5.4964\n",
            "108/108 - 7s - loss: 5.2062 - val_loss: 5.3543\n",
            "108/108 - 7s - loss: 5.0935 - val_loss: 5.2248\n",
            "108/108 - 7s - loss: 4.9702 - val_loss: 5.1044\n",
            "108/108 - 7s - loss: 4.8460 - val_loss: 4.9934\n",
            "108/108 - 7s - loss: 4.7277 - val_loss: 4.8890\n",
            "108/108 - 7s - loss: 4.6738 - val_loss: 4.7948\n",
            "108/108 - 7s - loss: 4.5623 - val_loss: 4.7075\n",
            "108/108 - 7s - loss: 4.4905 - val_loss: 4.6270\n",
            "108/108 - 7s - loss: 4.4606 - val_loss: 4.5559\n",
            "108/108 - 7s - loss: 4.3462 - val_loss: 4.4874\n",
            "108/108 - 7s - loss: 4.2968 - val_loss: 4.4243\n",
            "108/108 - 7s - loss: 4.2664 - val_loss: 4.3675\n",
            "108/108 - 7s - loss: 4.2110 - val_loss: 4.3159\n",
            "108/108 - 7s - loss: 4.1712 - val_loss: 4.2681\n",
            "108/108 - 7s - loss: 4.1861 - val_loss: 4.2276\n",
            "108/108 - 7s - loss: 4.1229 - val_loss: 4.1918\n",
            "108/108 - 7s - loss: 4.0990 - val_loss: 4.1574\n",
            "108/108 - 7s - loss: 4.0847 - val_loss: 4.1270\n",
            "108/108 - 7s - loss: 4.0464 - val_loss: 4.1004\n",
            "108/108 - 7s - loss: 4.0214 - val_loss: 4.0754\n",
            "108/108 - 7s - loss: 4.0249 - val_loss: 4.0542\n",
            "108/108 - 7s - loss: 4.0121 - val_loss: 4.0381\n",
            "108/108 - 7s - loss: 3.9826 - val_loss: 4.0229\n",
            "108/108 - 7s - loss: 3.9989 - val_loss: 4.0098\n",
            "108/108 - 7s - loss: 3.9903 - val_loss: 3.9976\n",
            "108/108 - 7s - loss: 3.9882 - val_loss: 3.9885\n",
            "108/108 - 7s - loss: 4.0110 - val_loss: 3.9804\n",
            "108/108 - 7s - loss: 3.9933 - val_loss: 3.9747\n",
            "108/108 - 7s - loss: 3.9842 - val_loss: 3.9681\n",
            "108/108 - 7s - loss: 3.9951 - val_loss: 3.9631\n",
            "108/108 - 7s - loss: 4.0020 - val_loss: 3.9596\n",
            "108/108 - 7s - loss: 3.9735 - val_loss: 3.9562\n",
            "108/108 - 7s - loss: 3.9547 - val_loss: 3.9524\n",
            "108/108 - 7s - loss: 3.9894 - val_loss: 3.9507\n",
            "108/108 - 7s - loss: 3.9763 - val_loss: 3.9488\n",
            "108/108 - 7s - loss: 3.9547 - val_loss: 3.9470\n",
            "108/108 - 7s - loss: 3.9789 - val_loss: 3.9443\n",
            "108/108 - 7s - loss: 3.9839 - val_loss: 3.9422\n",
            "108/108 - 7s - loss: 3.9855 - val_loss: 3.9411\n",
            "108/108 - 7s - loss: 3.9777 - val_loss: 3.9404\n",
            "108/108 - 7s - loss: 3.9935 - val_loss: 3.9393\n",
            "108/108 - 7s - loss: 3.9619 - val_loss: 3.9385\n",
            "108/108 - 7s - loss: 3.9841 - val_loss: 3.9387\n",
            "108/108 - 7s - loss: 3.9699 - val_loss: 3.9374\n",
            "108/108 - 7s - loss: 3.9267 - val_loss: 3.9358\n",
            "108/108 - 7s - loss: 4.0032 - val_loss: 3.9356\n",
            "108/108 - 7s - loss: 3.9647 - val_loss: 3.9349\n",
            "108/108 - 7s - loss: 3.9682 - val_loss: 3.9347\n",
            "108/108 - 7s - loss: 3.9403 - val_loss: 3.9345\n",
            "108/108 - 7s - loss: 3.9870 - val_loss: 3.9349\n",
            "108/108 - 7s - loss: 3.9929 - val_loss: 3.9346\n",
            "108/108 - 7s - loss: 3.9568 - val_loss: 3.9342\n",
            "108/108 - 7s - loss: 3.9550 - val_loss: 3.9335\n",
            "108/108 - 7s - loss: 3.9612 - val_loss: 3.9334\n",
            "108/108 - 7s - loss: 3.9848 - val_loss: 3.9335\n",
            "108/108 - 7s - loss: 3.9413 - val_loss: 3.9323\n",
            "108/108 - 7s - loss: 4.0083 - val_loss: 3.9332\n",
            "108/108 - 7s - loss: 3.9411 - val_loss: 3.9327\n",
            "108/108 - 7s - loss: 3.9452 - val_loss: 3.9327\n",
            "108/108 - 7s - loss: 3.9469 - val_loss: 3.9323\n",
            "108/108 - 7s - loss: 3.9543 - val_loss: 3.9314\n",
            "108/108 - 7s - loss: 3.9893 - val_loss: 3.9325\n",
            "108/108 - 7s - loss: 3.9505 - val_loss: 3.9317\n",
            "108/108 - 7s - loss: 3.9690 - val_loss: 3.9323\n",
            "108/108 - 7s - loss: 3.9570 - val_loss: 3.9316\n",
            "108/108 - 7s - loss: 3.9568 - val_loss: 3.9309\n",
            "108/108 - 7s - loss: 3.9801 - val_loss: 3.9315\n",
            "108/108 - 7s - loss: 3.9756 - val_loss: 3.9321\n",
            "108/108 - 7s - loss: 3.9599 - val_loss: 3.9312\n",
            "108/108 - 7s - loss: 3.9360 - val_loss: 3.9305\n",
            "108/108 - 7s - loss: 3.9811 - val_loss: 3.9307\n",
            "108/108 - 7s - loss: 3.9581 - val_loss: 3.9301\n",
            "108/108 - 7s - loss: 3.9309 - val_loss: 3.9298\n",
            "108/108 - 7s - loss: 3.9906 - val_loss: 3.9308\n",
            "108/108 - 7s - loss: 3.9810 - val_loss: 3.9319\n",
            "108/108 - 7s - loss: 3.9661 - val_loss: 3.9318\n",
            "108/108 - 7s - loss: 3.9615 - val_loss: 3.9318\n",
            "108/108 - 7s - loss: 3.9163 - val_loss: 3.9304\n",
            "108/108 - 7s - loss: 3.9617 - val_loss: 3.9303\n",
            "108/108 - 7s - loss: 3.9626 - val_loss: 3.9296\n",
            "108/108 - 7s - loss: 3.9611 - val_loss: 3.9290\n",
            "108/108 - 7s - loss: 3.9387 - val_loss: 3.9295\n",
            "108/108 - 7s - loss: 3.9677 - val_loss: 3.9294\n",
            "108/108 - 7s - loss: 3.9509 - val_loss: 3.9297\n",
            "108/108 - 7s - loss: 3.9569 - val_loss: 3.9287\n",
            "108/108 - 7s - loss: 3.9777 - val_loss: 3.9285\n",
            "108/108 - 7s - loss: 3.9517 - val_loss: 3.9289\n",
            "108/108 - 7s - loss: 3.9385 - val_loss: 3.9285\n",
            "108/108 - 7s - loss: 4.0038 - val_loss: 3.9295\n",
            "108/108 - 7s - loss: 3.9733 - val_loss: 3.9303\n",
            "108/108 - 7s - loss: 3.9541 - val_loss: 3.9310\n",
            "108/108 - 7s - loss: 4.0237 - val_loss: 4.0419\n",
            "108/108 - 7s - loss: 3.9709 - val_loss: 3.9643\n",
            "108/108 - 7s - loss: 3.9515 - val_loss: 3.9592\n",
            "108/108 - 7s - loss: 3.9523 - val_loss: 3.9548\n",
            "108/108 - 7s - loss: 3.9220 - val_loss: 3.9496\n",
            "108/108 - 7s - loss: 3.9399 - val_loss: 3.9461\n",
            "108/108 - 7s - loss: 3.9890 - val_loss: 3.9436\n",
            "108/108 - 7s - loss: 3.9944 - val_loss: 3.9421\n",
            "108/108 - 7s - loss: 3.9642 - val_loss: 3.9416\n",
            "108/108 - 7s - loss: 3.9927 - val_loss: 3.9417\n",
            "108/108 - 7s - loss: 3.9333 - val_loss: 3.9394\n",
            "108/108 - 7s - loss: 3.9791 - val_loss: 3.9379\n",
            "108/108 - 7s - loss: 3.9413 - val_loss: 3.9372\n",
            "108/108 - 7s - loss: 3.9380 - val_loss: 3.9363\n",
            "108/108 - 7s - loss: 4.0263 - val_loss: 3.9366\n",
            "108/108 - 7s - loss: 3.9248 - val_loss: 3.9354\n",
            "108/108 - 7s - loss: 3.9739 - val_loss: 3.9347\n",
            "108/108 - 7s - loss: 3.9736 - val_loss: 3.9347\n",
            "108/108 - 7s - loss: 3.9780 - val_loss: 3.9354\n",
            "108/108 - 7s - loss: 3.9606 - val_loss: 3.9347\n",
            "108/108 - 7s - loss: 3.9810 - val_loss: 3.9350\n",
            "108/108 - 7s - loss: 3.9559 - val_loss: 3.9342\n",
            "108/108 - 7s - loss: 3.9426 - val_loss: 3.9331\n",
            "108/108 - 7s - loss: 3.9364 - val_loss: 3.9317\n",
            "108/108 - 7s - loss: 3.9288 - val_loss: 3.9312\n",
            "108/108 - 7s - loss: 3.9819 - val_loss: 3.9318\n",
            "108/108 - 7s - loss: 3.9285 - val_loss: 3.9310\n",
            "108/108 - 7s - loss: 3.9257 - val_loss: 3.9308\n",
            "108/108 - 7s - loss: 3.9508 - val_loss: 3.9301\n",
            "108/108 - 7s - loss: 3.9635 - val_loss: 3.9307\n",
            "108/108 - 7s - loss: 3.9204 - val_loss: 3.9304\n",
            "108/108 - 7s - loss: 3.9662 - val_loss: 3.9301\n",
            "108/108 - 7s - loss: 3.9881 - val_loss: 3.9297\n",
            "108/108 - 7s - loss: 3.9663 - val_loss: 3.9294\n",
            "108/108 - 7s - loss: 3.9388 - val_loss: 3.9296\n",
            "108/108 - 7s - loss: 3.9400 - val_loss: 3.9296\n",
            "108/108 - 7s - loss: 3.9450 - val_loss: 3.9296\n",
            "108/108 - 7s - loss: 3.9347 - val_loss: 3.9295\n",
            "108/108 - 7s - loss: 3.9700 - val_loss: 3.9297\n",
            "108/108 - 7s - loss: 3.9515 - val_loss: 3.9297\n",
            "108/108 - 7s - loss: 3.9523 - val_loss: 3.9300\n",
            "108/108 - 7s - loss: 3.9351 - val_loss: 3.9294\n",
            "108/108 - 7s - loss: 3.9722 - val_loss: 3.9295\n",
            "108/108 - 7s - loss: 3.9819 - val_loss: 3.9299\n",
            "108/108 - 7s - loss: 3.9521 - val_loss: 3.9298\n",
            "108/108 - 7s - loss: 3.9449 - val_loss: 3.9304\n",
            "108/108 - 7s - loss: 3.9700 - val_loss: 3.9309\n",
            "108/108 - 7s - loss: 4.0012 - val_loss: 3.9313\n",
            "108/108 - 7s - loss: 3.9432 - val_loss: 3.9312\n",
            "108/108 - 7s - loss: 3.9718 - val_loss: 3.9314\n",
            "108/108 - 7s - loss: 3.9990 - val_loss: 3.9325\n",
            "108/108 - 7s - loss: 3.9645 - val_loss: 3.9314\n",
            "108/108 - 7s - loss: 3.9422 - val_loss: 3.9315\n",
            "108/108 - 7s - loss: 3.9632 - val_loss: 3.9312\n",
            "108/108 - 7s - loss: 3.9612 - val_loss: 3.9307\n",
            "108/108 - 7s - loss: 3.9637 - val_loss: 3.9309\n",
            "108/108 - 7s - loss: 3.9348 - val_loss: 3.9309\n",
            "108/108 - 7s - loss: 3.9743 - val_loss: 3.9314\n",
            "108/108 - 7s - loss: 3.9637 - val_loss: 3.9318\n",
            "108/108 - 7s - loss: 3.9487 - val_loss: 3.9319\n",
            "108/108 - 7s - loss: 3.9427 - val_loss: 3.9318\n",
            "108/108 - 7s - loss: 3.9753 - val_loss: 3.9316\n",
            "108/108 - 7s - loss: 3.9588 - val_loss: 3.9311\n",
            "108/108 - 7s - loss: 3.9709 - val_loss: 3.9313\n",
            "108/108 - 7s - loss: 3.9588 - val_loss: 3.9308\n",
            "108/108 - 7s - loss: 3.9589 - val_loss: 3.9307\n",
            "108/108 - 7s - loss: 3.9774 - val_loss: 3.9309\n",
            "108/108 - 7s - loss: 3.9538 - val_loss: 3.9309\n",
            "108/108 - 7s - loss: 3.9443 - val_loss: 3.9308\n",
            "108/108 - 7s - loss: 3.9843 - val_loss: 3.9315\n",
            "108/108 - 7s - loss: 3.9607 - val_loss: 3.9308\n",
            "108/108 - 7s - loss: 3.9463 - val_loss: 3.9181\n",
            "108/108 - 7s - loss: 3.9804 - val_loss: 3.9191\n",
            "108/108 - 7s - loss: 3.9490 - val_loss: 3.9200\n",
            "108/108 - 7s - loss: 3.9513 - val_loss: 3.9207\n",
            "108/108 - 7s - loss: 3.9963 - val_loss: 3.9233\n",
            "108/108 - 7s - loss: 3.9525 - val_loss: 3.9261\n",
            "108/108 - 7s - loss: 3.9554 - val_loss: 3.9272\n",
            "108/108 - 7s - loss: 3.9686 - val_loss: 3.9311\n",
            "108/108 - 7s - loss: 3.9539 - val_loss: 3.9259\n",
            "108/108 - 7s - loss: 3.9321 - val_loss: 3.9245\n",
            "108/108 - 7s - loss: 3.9344 - val_loss: 3.9251\n",
            "108/108 - 7s - loss: 3.9396 - val_loss: 3.9243\n",
            "108/108 - 7s - loss: 3.9815 - val_loss: 3.9233\n",
            "108/108 - 7s - loss: 3.8911 - val_loss: 3.9234\n",
            "108/108 - 7s - loss: 3.9590 - val_loss: 3.8815\n",
            "108/108 - 7s - loss: 3.8708 - val_loss: 3.9130\n",
            "108/108 - 7s - loss: 3.9059 - val_loss: 3.9098\n",
            "108/108 - 7s - loss: 3.9443 - val_loss: 3.9111\n",
            "108/108 - 7s - loss: 3.9343 - val_loss: 3.9063\n",
            "108/108 - 7s - loss: 3.9435 - val_loss: 3.9108\n",
            "108/108 - 7s - loss: 3.8988 - val_loss: 3.9106\n",
            "108/108 - 7s - loss: 3.9157 - val_loss: 3.9004\n",
            "108/108 - 7s - loss: 3.9070 - val_loss: 3.9028\n",
            "108/108 - 7s - loss: 3.8991 - val_loss: 3.9167\n",
            "108/108 - 7s - loss: 3.8971 - val_loss: 3.8995\n",
            "108/108 - 7s - loss: 3.9190 - val_loss: 3.9055\n",
            "108/108 - 7s - loss: 3.9226 - val_loss: 3.8943\n",
            "108/108 - 7s - loss: 3.8406 - val_loss: 3.8098\n",
            "108/108 - 7s - loss: 3.8178 - val_loss: 3.7937\n",
            "108/108 - 7s - loss: 3.7816 - val_loss: 3.7918\n",
            "108/108 - 7s - loss: 3.7501 - val_loss: 3.7532\n",
            "108/108 - 7s - loss: 3.7622 - val_loss: 3.7702\n",
            "108/108 - 7s - loss: 3.7261 - val_loss: 3.7586\n",
            "108/108 - 7s - loss: 3.7118 - val_loss: 3.7156\n",
            "108/108 - 7s - loss: 3.7658 - val_loss: 3.6819\n",
            "108/108 - 7s - loss: 3.6794 - val_loss: 3.6981\n",
            "108/108 - 7s - loss: 3.7236 - val_loss: 3.6794\n",
            "108/108 - 7s - loss: 3.6852 - val_loss: 3.6709\n",
            "108/108 - 7s - loss: 3.6204 - val_loss: 3.6144\n",
            "108/108 - 7s - loss: 3.6227 - val_loss: 3.6187\n",
            "108/108 - 7s - loss: 3.6279 - val_loss: 3.5454\n",
            "108/108 - 7s - loss: 3.6263 - val_loss: 3.5592\n",
            "108/108 - 7s - loss: 3.6160 - val_loss: 3.5741\n",
            "108/108 - 7s - loss: 3.5948 - val_loss: 3.5980\n",
            "108/108 - 7s - loss: 3.5458 - val_loss: 3.5340\n",
            "108/108 - 7s - loss: 3.5528 - val_loss: 3.5252\n",
            "108/108 - 7s - loss: 3.5438 - val_loss: 3.4800\n",
            "108/108 - 7s - loss: 3.4676 - val_loss: 3.4874\n",
            "108/108 - 7s - loss: 3.4752 - val_loss: 3.3927\n",
            "108/108 - 7s - loss: 3.5039 - val_loss: 3.3983\n",
            "108/108 - 7s - loss: 3.4297 - val_loss: 3.4016\n",
            "108/108 - 7s - loss: 3.4381 - val_loss: 3.3516\n",
            "108/108 - 7s - loss: 3.3951 - val_loss: 3.3471\n",
            "108/108 - 7s - loss: 3.3666 - val_loss: 3.3297\n",
            "108/108 - 7s - loss: 3.3975 - val_loss: 3.3270\n",
            "108/108 - 7s - loss: 3.4092 - val_loss: 3.2887\n",
            "108/108 - 7s - loss: 3.3411 - val_loss: 3.3107\n",
            "108/108 - 7s - loss: 3.3558 - val_loss: 3.3202\n",
            "108/108 - 7s - loss: 3.3874 - val_loss: 3.3268\n",
            "108/108 - 7s - loss: 3.3314 - val_loss: 3.3366\n",
            "108/108 - 7s - loss: 3.3354 - val_loss: 3.3021\n",
            "108/108 - 7s - loss: 3.3495 - val_loss: 3.2647\n",
            "108/108 - 7s - loss: 3.3239 - val_loss: 3.2558\n",
            "108/108 - 7s - loss: 3.3079 - val_loss: 3.2228\n",
            "108/108 - 7s - loss: 3.3307 - val_loss: 3.2678\n",
            "108/108 - 7s - loss: 3.3412 - val_loss: 3.2300\n",
            "108/108 - 7s - loss: 3.3535 - val_loss: 3.2133\n",
            "108/108 - 7s - loss: 3.3111 - val_loss: 3.2096\n",
            "108/108 - 7s - loss: 3.2976 - val_loss: 3.2265\n",
            "108/108 - 7s - loss: 3.2868 - val_loss: 3.2091\n",
            "108/108 - 7s - loss: 3.2823 - val_loss: 3.1548\n",
            "108/108 - 7s - loss: 3.3183 - val_loss: 3.1676\n",
            "108/108 - 7s - loss: 3.2944 - val_loss: 3.1865\n",
            "108/108 - 7s - loss: 3.2737 - val_loss: 3.1556\n",
            "108/108 - 7s - loss: 3.2406 - val_loss: 3.1466\n",
            "108/108 - 7s - loss: 3.2759 - val_loss: 3.1284\n",
            "108/108 - 7s - loss: 3.2806 - val_loss: 3.1307\n",
            "108/108 - 7s - loss: 3.2755 - val_loss: 3.1132\n",
            "108/108 - 7s - loss: 3.2556 - val_loss: 3.1426\n",
            "108/108 - 7s - loss: 3.2789 - val_loss: 3.1011\n",
            "108/108 - 7s - loss: 3.2318 - val_loss: 3.1144\n",
            "108/108 - 7s - loss: 3.1954 - val_loss: 3.0927\n",
            "108/108 - 7s - loss: 3.1743 - val_loss: 3.1182\n",
            "108/108 - 7s - loss: 3.2451 - val_loss: 3.1489\n",
            "108/108 - 7s - loss: 3.2099 - val_loss: 3.1013\n",
            "108/108 - 7s - loss: 3.2281 - val_loss: 3.1860\n",
            "108/108 - 7s - loss: 3.1784 - val_loss: 3.1060\n",
            "108/108 - 7s - loss: 3.1673 - val_loss: 3.1185\n",
            "108/108 - 7s - loss: 3.2088 - val_loss: 3.0981\n",
            "108/108 - 7s - loss: 3.1981 - val_loss: 3.1237\n",
            "108/108 - 7s - loss: 3.2202 - val_loss: 3.1128\n",
            "108/108 - 7s - loss: 3.2137 - val_loss: 3.0982\n",
            "108/108 - 7s - loss: 3.1834 - val_loss: 3.1198\n",
            "108/108 - 7s - loss: 3.2159 - val_loss: 3.1498\n",
            "108/108 - 7s - loss: 3.1920 - val_loss: 3.1311\n",
            "108/108 - 7s - loss: 3.1595 - val_loss: 3.0950\n",
            "108/108 - 7s - loss: 3.1356 - val_loss: 3.0925\n",
            "108/108 - 7s - loss: 3.1374 - val_loss: 3.1075\n",
            "108/108 - 7s - loss: 3.1830 - val_loss: 3.1023\n",
            "108/108 - 7s - loss: 3.1598 - val_loss: 3.1118\n",
            "108/108 - 7s - loss: 3.1615 - val_loss: 3.1031\n",
            "108/108 - 7s - loss: 3.1634 - val_loss: 3.0886\n",
            "108/108 - 7s - loss: 3.1857 - val_loss: 3.0984\n",
            "108/108 - 7s - loss: 3.1555 - val_loss: 3.0628\n",
            "108/108 - 7s - loss: 3.0984 - val_loss: 3.1035\n",
            "108/108 - 7s - loss: 3.1256 - val_loss: 3.0977\n",
            "108/108 - 7s - loss: 3.1779 - val_loss: 3.0851\n",
            "108/108 - 7s - loss: 3.1484 - val_loss: 3.0611\n",
            "108/108 - 7s - loss: 3.1676 - val_loss: 3.0691\n",
            "108/108 - 7s - loss: 3.1284 - val_loss: 3.0920\n",
            "108/108 - 7s - loss: 3.1760 - val_loss: 3.0794\n",
            "108/108 - 7s - loss: 3.1326 - val_loss: 3.0666\n",
            "108/108 - 7s - loss: 3.1150 - val_loss: 3.0847\n",
            "108/108 - 7s - loss: 3.1299 - val_loss: 3.0880\n",
            "108/108 - 7s - loss: 3.1517 - val_loss: 3.0638\n",
            "108/108 - 7s - loss: 3.1338 - val_loss: 3.1017\n",
            "108/108 - 7s - loss: 3.1404 - val_loss: 3.1143\n",
            "108/108 - 7s - loss: 3.1264 - val_loss: 3.0796\n",
            "108/108 - 7s - loss: 3.1421 - val_loss: 3.0828\n",
            "108/108 - 7s - loss: 3.1080 - val_loss: 3.1089\n",
            "108/108 - 7s - loss: 3.1390 - val_loss: 3.1018\n",
            "108/108 - 7s - loss: 3.0981 - val_loss: 3.0899\n",
            "108/108 - 7s - loss: 3.1312 - val_loss: 3.0675\n",
            "108/108 - 7s - loss: 3.1286 - val_loss: 3.0788\n",
            "108/108 - 7s - loss: 3.1719 - val_loss: 3.1070\n",
            "108/108 - 7s - loss: 3.1576 - val_loss: 3.0901\n",
            "108/108 - 7s - loss: 3.1213 - val_loss: 3.0626\n",
            "108/108 - 7s - loss: 3.0632 - val_loss: 3.0765\n",
            "108/108 - 7s - loss: 3.1212 - val_loss: 3.0834\n",
            "108/108 - 7s - loss: 3.1287 - val_loss: 3.0765\n",
            "108/108 - 7s - loss: 3.1455 - val_loss: 3.1019\n",
            "108/108 - 7s - loss: 3.1074 - val_loss: 3.0752\n",
            "108/108 - 7s - loss: 3.0994 - val_loss: 3.0956\n",
            "108/108 - 7s - loss: 3.1402 - val_loss: 3.0832\n",
            "108/108 - 7s - loss: 3.0991 - val_loss: 3.0933\n",
            "108/108 - 7s - loss: 3.0934 - val_loss: 3.0857\n",
            "108/108 - 7s - loss: 3.0824 - val_loss: 3.1013\n",
            "108/108 - 7s - loss: 3.1343 - val_loss: 3.0954\n",
            "108/108 - 7s - loss: 3.1254 - val_loss: 3.0665\n",
            "108/108 - 7s - loss: 3.1224 - val_loss: 3.0594\n",
            "108/108 - 7s - loss: 3.0458 - val_loss: 3.0861\n",
            "108/108 - 7s - loss: 3.0643 - val_loss: 3.0751\n",
            "108/108 - 7s - loss: 3.0892 - val_loss: 3.1026\n",
            "108/108 - 7s - loss: 3.0883 - val_loss: 3.0651\n",
            "108/108 - 7s - loss: 3.0958 - val_loss: 3.0671\n",
            "108/108 - 7s - loss: 3.1323 - val_loss: 3.0784\n",
            "108/108 - 7s - loss: 3.1191 - val_loss: 3.0822\n",
            "108/108 - 7s - loss: 3.1237 - val_loss: 3.0455\n",
            "108/108 - 7s - loss: 3.0629 - val_loss: 3.1024\n",
            "108/108 - 7s - loss: 3.0797 - val_loss: 3.1087\n",
            "108/108 - 7s - loss: 3.0333 - val_loss: 3.0643\n",
            "108/108 - 7s - loss: 3.0847 - val_loss: 3.0791\n",
            "108/108 - 7s - loss: 3.1062 - val_loss: 3.0998\n",
            "108/108 - 7s - loss: 3.0916 - val_loss: 3.0361\n",
            "108/108 - 7s - loss: 3.0745 - val_loss: 3.0519\n",
            "108/108 - 7s - loss: 3.0766 - val_loss: 3.1258\n",
            "108/108 - 7s - loss: 3.0304 - val_loss: 3.0904\n",
            "108/108 - 7s - loss: 3.0270 - val_loss: 3.0445\n",
            "108/108 - 7s - loss: 3.0409 - val_loss: 3.0525\n",
            "108/108 - 7s - loss: 3.0634 - val_loss: 3.0687\n",
            "108/108 - 7s - loss: 3.0304 - val_loss: 3.0731\n",
            "108/108 - 7s - loss: 3.0668 - val_loss: 3.0504\n",
            "108/108 - 7s - loss: 3.0504 - val_loss: 3.0607\n",
            "108/108 - 7s - loss: 3.0484 - val_loss: 3.0686\n",
            "108/108 - 7s - loss: 3.0594 - val_loss: 3.0745\n",
            "108/108 - 7s - loss: 3.0465 - val_loss: 3.0763\n",
            "108/108 - 7s - loss: 3.0452 - val_loss: 3.0499\n",
            "108/108 - 7s - loss: 3.0152 - val_loss: 3.0640\n",
            "108/108 - 7s - loss: 3.0829 - val_loss: 3.0777\n",
            "108/108 - 7s - loss: 3.0235 - val_loss: 3.0573\n",
            "108/108 - 7s - loss: 3.0210 - val_loss: 3.0261\n",
            "108/108 - 7s - loss: 3.0739 - val_loss: 3.0506\n",
            "108/108 - 7s - loss: 3.0438 - val_loss: 3.0973\n",
            "108/108 - 7s - loss: 3.0699 - val_loss: 3.0440\n",
            "108/108 - 7s - loss: 3.0791 - val_loss: 3.0438\n",
            "108/108 - 7s - loss: 3.0485 - val_loss: 3.0451\n",
            "108/108 - 7s - loss: 3.0457 - val_loss: 3.0759\n",
            "108/108 - 7s - loss: 3.0508 - val_loss: 3.0549\n",
            "108/108 - 7s - loss: 2.9798 - val_loss: 3.0773\n",
            "108/108 - 7s - loss: 3.0310 - val_loss: 3.0532\n",
            "108/108 - 7s - loss: 3.0405 - val_loss: 3.0469\n",
            "108/108 - 7s - loss: 3.0212 - val_loss: 3.0291\n",
            "108/108 - 7s - loss: 3.0315 - val_loss: 3.0361\n",
            "108/108 - 7s - loss: 3.0410 - val_loss: 3.0522\n",
            "108/108 - 7s - loss: 3.0188 - val_loss: 3.0602\n",
            "108/108 - 7s - loss: 3.0084 - val_loss: 3.0595\n",
            "108/108 - 7s - loss: 3.0475 - val_loss: 3.0223\n",
            "108/108 - 7s - loss: 2.9861 - val_loss: 3.0691\n",
            "108/108 - 7s - loss: 3.0149 - val_loss: 3.0283\n",
            "108/108 - 7s - loss: 3.0489 - val_loss: 3.0449\n",
            "108/108 - 7s - loss: 3.0073 - val_loss: 3.0369\n",
            "108/108 - 7s - loss: 2.9709 - val_loss: 3.0435\n",
            "108/108 - 7s - loss: 3.0323 - val_loss: 3.0379\n",
            "108/108 - 7s - loss: 2.9821 - val_loss: 3.0385\n",
            "108/108 - 7s - loss: 3.0302 - val_loss: 3.0335\n",
            "108/108 - 7s - loss: 3.0145 - val_loss: 3.0448\n",
            "108/108 - 7s - loss: 3.0280 - val_loss: 3.0091\n",
            "108/108 - 7s - loss: 3.0529 - val_loss: 3.0487\n",
            "108/108 - 7s - loss: 2.9691 - val_loss: 3.0382\n",
            "108/108 - 7s - loss: 3.0201 - val_loss: 3.0252\n",
            "108/108 - 7s - loss: 3.0317 - val_loss: 3.0143\n",
            "108/108 - 7s - loss: 2.9651 - val_loss: 3.0090\n",
            "108/108 - 7s - loss: 3.0767 - val_loss: 3.0044\n",
            "108/108 - 7s - loss: 3.0167 - val_loss: 3.0229\n",
            "108/108 - 7s - loss: 3.0102 - val_loss: 3.0519\n",
            "108/108 - 7s - loss: 3.0253 - val_loss: 3.0114\n",
            "108/108 - 7s - loss: 2.9491 - val_loss: 2.9928\n",
            "108/108 - 7s - loss: 2.9593 - val_loss: 3.0205\n",
            "108/108 - 7s - loss: 2.9799 - val_loss: 3.0115\n",
            "108/108 - 7s - loss: 2.9966 - val_loss: 3.0291\n",
            "108/108 - 7s - loss: 3.0066 - val_loss: 3.0361\n",
            "108/108 - 7s - loss: 3.0598 - val_loss: 3.0471\n",
            "108/108 - 7s - loss: 2.9700 - val_loss: 3.0614\n",
            "108/108 - 7s - loss: 2.9941 - val_loss: 3.0259\n",
            "108/108 - 7s - loss: 3.0310 - val_loss: 3.0664\n",
            "108/108 - 7s - loss: 2.9814 - val_loss: 3.0062\n",
            "108/108 - 7s - loss: 3.0290 - val_loss: 3.0548\n",
            "108/108 - 7s - loss: 3.0099 - val_loss: 3.0147\n",
            "108/108 - 7s - loss: 2.9897 - val_loss: 3.0258\n",
            "108/108 - 7s - loss: 2.9679 - val_loss: 3.0289\n",
            "108/108 - 7s - loss: 2.9835 - val_loss: 3.0367\n",
            "108/108 - 7s - loss: 2.9724 - val_loss: 3.0259\n",
            "108/108 - 7s - loss: 2.9805 - val_loss: 3.0620\n",
            "108/108 - 7s - loss: 2.9875 - val_loss: 3.0415\n",
            "108/108 - 7s - loss: 2.9633 - val_loss: 3.0540\n",
            "108/108 - 7s - loss: 2.9789 - val_loss: 3.0626\n",
            "108/108 - 7s - loss: 2.9923 - val_loss: 2.9973\n",
            "108/108 - 7s - loss: 2.9597 - val_loss: 3.0677\n",
            "108/108 - 7s - loss: 2.9897 - val_loss: 3.0441\n",
            "108/108 - 7s - loss: 2.9593 - val_loss: 3.0303\n",
            "108/108 - 7s - loss: 2.9386 - val_loss: 3.0404\n",
            "108/108 - 7s - loss: 2.9489 - val_loss: 3.0170\n",
            "108/108 - 7s - loss: 2.9578 - val_loss: 3.0824\n",
            "108/108 - 7s - loss: 2.9424 - val_loss: 3.0718\n",
            "108/108 - 7s - loss: 3.0121 - val_loss: 3.0469\n",
            "108/108 - 7s - loss: 2.9734 - val_loss: 3.0107\n",
            "108/108 - 7s - loss: 2.9523 - val_loss: 3.0517\n",
            "108/108 - 7s - loss: 2.9994 - val_loss: 3.0787\n",
            "108/108 - 7s - loss: 2.9102 - val_loss: 3.0234\n",
            "108/108 - 7s - loss: 2.9531 - val_loss: 3.0912\n",
            "108/108 - 7s - loss: 2.9787 - val_loss: 3.0596\n",
            "108/108 - 7s - loss: 2.9442 - val_loss: 3.0725\n",
            "108/108 - 7s - loss: 2.9611 - val_loss: 3.0833\n",
            "108/108 - 7s - loss: 2.9425 - val_loss: 3.0842\n",
            "108/108 - 7s - loss: 2.9829 - val_loss: 3.0262\n",
            "108/108 - 7s - loss: 2.9336 - val_loss: 3.0319\n",
            "108/108 - 7s - loss: 2.8925 - val_loss: 3.0567\n",
            "108/108 - 7s - loss: 2.9121 - val_loss: 3.0434\n",
            "108/108 - 7s - loss: 2.9504 - val_loss: 3.0300\n",
            "108/108 - 7s - loss: 2.9320 - val_loss: 3.0477\n",
            "108/108 - 7s - loss: 2.9148 - val_loss: 3.0629\n",
            "108/108 - 7s - loss: 2.9282 - val_loss: 3.0611\n",
            "108/108 - 7s - loss: 2.9133 - val_loss: 3.0947\n",
            "108/108 - 7s - loss: 2.9537 - val_loss: 3.0533\n",
            "108/108 - 7s - loss: 2.8836 - val_loss: 3.0347\n",
            "108/108 - 7s - loss: 2.9017 - val_loss: 3.0218\n",
            "108/108 - 7s - loss: 2.8973 - val_loss: 3.0250\n",
            "108/108 - 7s - loss: 2.9207 - val_loss: 3.0714\n",
            "108/108 - 7s - loss: 2.9107 - val_loss: 3.0501\n",
            "108/108 - 7s - loss: 2.9454 - val_loss: 3.0358\n",
            "108/108 - 7s - loss: 2.8497 - val_loss: 3.0547\n",
            "108/108 - 7s - loss: 2.9400 - val_loss: 3.0294\n",
            "108/108 - 7s - loss: 2.8984 - val_loss: 3.0212\n",
            "108/108 - 7s - loss: 2.9281 - val_loss: 3.0595\n",
            "108/108 - 7s - loss: 2.8845 - val_loss: 3.0241\n",
            "108/108 - 7s - loss: 2.9171 - val_loss: 3.0291\n",
            "108/108 - 7s - loss: 2.8471 - val_loss: 3.0449\n",
            "108/108 - 7s - loss: 2.8639 - val_loss: 3.0820\n",
            "108/108 - 7s - loss: 2.8890 - val_loss: 3.0664\n",
            "108/108 - 7s - loss: 2.8222 - val_loss: 2.9892\n",
            "108/108 - 7s - loss: 2.8731 - val_loss: 3.0162\n",
            "108/108 - 7s - loss: 2.9243 - val_loss: 3.0258\n",
            "108/108 - 7s - loss: 2.8811 - val_loss: 3.0474\n",
            "108/108 - 7s - loss: 2.9024 - val_loss: 3.0408\n",
            "108/108 - 7s - loss: 2.8892 - val_loss: 3.0247\n",
            "108/108 - 7s - loss: 2.8921 - val_loss: 3.0218\n",
            "108/108 - 7s - loss: 2.9084 - val_loss: 2.9909\n",
            "108/108 - 7s - loss: 2.8990 - val_loss: 3.0645\n",
            "108/108 - 7s - loss: 2.8529 - val_loss: 3.0335\n",
            "108/108 - 7s - loss: 2.8224 - val_loss: 3.0056\n",
            "108/108 - 7s - loss: 2.8754 - val_loss: 3.0094\n",
            "108/108 - 7s - loss: 2.8949 - val_loss: 2.9868\n",
            "108/108 - 7s - loss: 2.8894 - val_loss: 3.0641\n",
            "108/108 - 7s - loss: 2.8869 - val_loss: 3.1074\n",
            "108/108 - 7s - loss: 2.9135 - val_loss: 3.0331\n",
            "108/108 - 7s - loss: 2.9051 - val_loss: 3.0194\n",
            "108/108 - 7s - loss: 2.9040 - val_loss: 3.0522\n",
            "108/108 - 7s - loss: 2.8985 - val_loss: 3.0037\n",
            "108/108 - 7s - loss: 2.8613 - val_loss: 3.0499\n",
            "108/108 - 7s - loss: 2.8667 - val_loss: 2.9778\n",
            "108/108 - 7s - loss: 2.8980 - val_loss: 3.0057\n",
            "108/108 - 7s - loss: 2.8745 - val_loss: 3.0020\n",
            "108/108 - 7s - loss: 2.8710 - val_loss: 3.0135\n",
            "108/108 - 7s - loss: 2.8532 - val_loss: 2.9946\n",
            "108/108 - 7s - loss: 2.8837 - val_loss: 3.0226\n",
            "108/108 - 7s - loss: 2.8451 - val_loss: 3.0371\n",
            "108/108 - 7s - loss: 2.8596 - val_loss: 3.0237\n",
            "108/108 - 7s - loss: 2.8694 - val_loss: 3.0674\n",
            "108/108 - 7s - loss: 2.9099 - val_loss: 2.9999\n",
            "108/108 - 7s - loss: 2.8708 - val_loss: 3.0263\n",
            "108/108 - 7s - loss: 2.8825 - val_loss: 3.0478\n",
            "108/108 - 7s - loss: 2.8803 - val_loss: 3.0244\n",
            "108/108 - 7s - loss: 2.8948 - val_loss: 3.0178\n",
            "108/108 - 7s - loss: 2.8866 - val_loss: 3.0550\n",
            "108/108 - 7s - loss: 2.8796 - val_loss: 3.0433\n",
            "108/108 - 7s - loss: 2.8619 - val_loss: 3.0252\n",
            "108/108 - 7s - loss: 2.8667 - val_loss: 2.9721\n",
            "108/108 - 7s - loss: 2.8520 - val_loss: 3.0866\n",
            "108/108 - 7s - loss: 2.8357 - val_loss: 3.0266\n",
            "108/108 - 7s - loss: 2.8624 - val_loss: 3.0411\n",
            "108/108 - 7s - loss: 2.8528 - val_loss: 3.0891\n",
            "108/108 - 7s - loss: 2.8614 - val_loss: 3.0092\n",
            "108/108 - 7s - loss: 2.8612 - val_loss: 2.9866\n",
            "108/108 - 7s - loss: 2.8523 - val_loss: 3.0080\n",
            "108/108 - 7s - loss: 2.8299 - val_loss: 3.0452\n",
            "108/108 - 7s - loss: 2.8378 - val_loss: 3.1459\n",
            "108/108 - 7s - loss: 2.8682 - val_loss: 3.1137\n",
            "108/108 - 7s - loss: 2.8577 - val_loss: 2.9979\n",
            "108/108 - 7s - loss: 2.8429 - val_loss: 3.0927\n",
            "108/108 - 7s - loss: 2.8013 - val_loss: 3.0225\n",
            "108/108 - 7s - loss: 2.8303 - val_loss: 3.0426\n",
            "108/108 - 7s - loss: 2.8421 - val_loss: 3.0450\n",
            "108/108 - 7s - loss: 2.8130 - val_loss: 3.0274\n",
            "108/108 - 7s - loss: 2.8359 - val_loss: 3.0750\n",
            "108/108 - 7s - loss: 2.8265 - val_loss: 3.0335\n",
            "108/108 - 7s - loss: 2.8255 - val_loss: 3.0234\n",
            "108/108 - 7s - loss: 2.8233 - val_loss: 3.0188\n",
            "108/108 - 7s - loss: 2.8259 - val_loss: 3.0551\n",
            "108/108 - 7s - loss: 2.8238 - val_loss: 3.0969\n",
            "108/108 - 7s - loss: 2.8075 - val_loss: 3.1009\n",
            "0.5\n",
            "108/108 - 14s - loss: 8.7469 - val_loss: 8.7280\n",
            "108/108 - 7s - loss: 8.2548 - val_loss: 8.3283\n",
            "108/108 - 7s - loss: 7.8785 - val_loss: 7.9967\n",
            "108/108 - 7s - loss: 7.5556 - val_loss: 7.6995\n",
            "108/108 - 7s - loss: 7.2673 - val_loss: 7.4273\n",
            "108/108 - 7s - loss: 7.0174 - val_loss: 7.1773\n",
            "108/108 - 7s - loss: 6.7740 - val_loss: 6.9441\n",
            "108/108 - 7s - loss: 6.5823 - val_loss: 6.7275\n",
            "108/108 - 7s - loss: 6.3639 - val_loss: 6.5264\n",
            "108/108 - 7s - loss: 6.1880 - val_loss: 6.3380\n",
            "108/108 - 7s - loss: 5.9713 - val_loss: 6.1601\n",
            "108/108 - 7s - loss: 5.8157 - val_loss: 5.9928\n",
            "108/108 - 7s - loss: 5.6899 - val_loss: 5.8361\n",
            "108/108 - 7s - loss: 5.5173 - val_loss: 5.6894\n",
            "108/108 - 7s - loss: 5.4328 - val_loss: 5.5546\n",
            "108/108 - 7s - loss: 5.2656 - val_loss: 5.4290\n",
            "108/108 - 7s - loss: 5.1833 - val_loss: 5.3121\n",
            "108/108 - 7s - loss: 5.0867 - val_loss: 5.2016\n",
            "108/108 - 7s - loss: 4.9518 - val_loss: 5.0973\n",
            "108/108 - 7s - loss: 4.8703 - val_loss: 5.0030\n",
            "108/108 - 7s - loss: 4.8312 - val_loss: 4.9170\n",
            "108/108 - 7s - loss: 4.7489 - val_loss: 4.8356\n",
            "108/108 - 7s - loss: 4.6655 - val_loss: 4.7596\n",
            "108/108 - 7s - loss: 4.6109 - val_loss: 4.6918\n",
            "108/108 - 7s - loss: 4.5696 - val_loss: 4.6315\n",
            "108/108 - 7s - loss: 4.5032 - val_loss: 4.5765\n",
            "108/108 - 7s - loss: 4.4856 - val_loss: 4.5283\n",
            "108/108 - 7s - loss: 4.4640 - val_loss: 4.4867\n",
            "108/108 - 7s - loss: 4.4176 - val_loss: 4.4489\n",
            "108/108 - 7s - loss: 4.4047 - val_loss: 4.4142\n",
            "108/108 - 7s - loss: 4.3479 - val_loss: 4.3846\n",
            "108/108 - 7s - loss: 4.3454 - val_loss: 4.3597\n",
            "108/108 - 7s - loss: 4.2609 - val_loss: 4.3356\n",
            "108/108 - 7s - loss: 4.2327 - val_loss: 4.3142\n",
            "108/108 - 7s - loss: 4.2270 - val_loss: 4.2969\n",
            "108/108 - 7s - loss: 4.2903 - val_loss: 4.2814\n",
            "108/108 - 7s - loss: 4.2971 - val_loss: 4.2706\n",
            "108/108 - 7s - loss: 4.2190 - val_loss: 4.2595\n",
            "108/108 - 7s - loss: 4.2736 - val_loss: 4.2502\n",
            "108/108 - 7s - loss: 4.2244 - val_loss: 4.2427\n",
            "108/108 - 7s - loss: 4.3095 - val_loss: 4.2363\n",
            "108/108 - 7s - loss: 4.2762 - val_loss: 4.2309\n",
            "108/108 - 7s - loss: 4.2261 - val_loss: 4.2253\n",
            "108/108 - 7s - loss: 4.2166 - val_loss: 4.2191\n",
            "108/108 - 7s - loss: 4.2248 - val_loss: 4.2155\n",
            "108/108 - 7s - loss: 4.2700 - val_loss: 4.2119\n",
            "108/108 - 7s - loss: 4.2447 - val_loss: 4.2089\n",
            "108/108 - 7s - loss: 4.2639 - val_loss: 4.2066\n",
            "108/108 - 7s - loss: 4.2308 - val_loss: 4.2032\n",
            "108/108 - 7s - loss: 4.2304 - val_loss: 4.2012\n",
            "108/108 - 7s - loss: 4.2549 - val_loss: 4.2006\n",
            "108/108 - 7s - loss: 4.2252 - val_loss: 4.1992\n",
            "108/108 - 7s - loss: 4.2067 - val_loss: 4.1970\n",
            "108/108 - 7s - loss: 4.2350 - val_loss: 4.1956\n",
            "108/108 - 7s - loss: 4.1926 - val_loss: 4.1940\n",
            "108/108 - 7s - loss: 4.2501 - val_loss: 4.1934\n",
            "108/108 - 7s - loss: 4.2647 - val_loss: 4.1936\n",
            "108/108 - 7s - loss: 4.1704 - val_loss: 4.1932\n",
            "108/108 - 7s - loss: 4.2660 - val_loss: 4.1932\n",
            "108/108 - 7s - loss: 4.2702 - val_loss: 4.1933\n",
            "108/108 - 7s - loss: 4.2661 - val_loss: 4.1936\n",
            "108/108 - 7s - loss: 4.3022 - val_loss: 4.1939\n",
            "108/108 - 7s - loss: 4.2339 - val_loss: 4.1938\n",
            "108/108 - 7s - loss: 4.2471 - val_loss: 4.1936\n",
            "108/108 - 7s - loss: 4.2577 - val_loss: 4.1935\n",
            "108/108 - 7s - loss: 4.2644 - val_loss: 4.1936\n",
            "108/108 - 7s - loss: 4.2165 - val_loss: 4.1935\n",
            "108/108 - 7s - loss: 4.2401 - val_loss: 4.1929\n",
            "108/108 - 7s - loss: 4.2272 - val_loss: 4.1928\n",
            "108/108 - 7s - loss: 4.2600 - val_loss: 4.1919\n",
            "108/108 - 7s - loss: 4.2476 - val_loss: 4.1913\n",
            "108/108 - 7s - loss: 4.2500 - val_loss: 4.1912\n",
            "108/108 - 7s - loss: 4.2421 - val_loss: 4.1912\n",
            "108/108 - 7s - loss: 4.2365 - val_loss: 4.1907\n",
            "108/108 - 7s - loss: 4.2305 - val_loss: 4.1903\n",
            "108/108 - 7s - loss: 4.2661 - val_loss: 4.1906\n",
            "108/108 - 7s - loss: 4.1676 - val_loss: 4.1905\n",
            "108/108 - 7s - loss: 4.2588 - val_loss: 4.1913\n",
            "108/108 - 7s - loss: 4.2253 - val_loss: 4.1922\n",
            "108/108 - 7s - loss: 4.3040 - val_loss: 4.1922\n",
            "108/108 - 7s - loss: 4.2378 - val_loss: 4.1917\n",
            "108/108 - 7s - loss: 4.2272 - val_loss: 4.1913\n",
            "108/108 - 7s - loss: 4.2668 - val_loss: 4.1913\n",
            "108/108 - 7s - loss: 4.2183 - val_loss: 4.1906\n",
            "108/108 - 7s - loss: 4.2408 - val_loss: 4.1908\n",
            "108/108 - 7s - loss: 4.2041 - val_loss: 4.1903\n",
            "108/108 - 7s - loss: 4.2741 - val_loss: 4.1909\n",
            "108/108 - 7s - loss: 4.2557 - val_loss: 4.1904\n",
            "108/108 - 7s - loss: 4.2278 - val_loss: 4.1903\n",
            "108/108 - 7s - loss: 4.2365 - val_loss: 4.1896\n",
            "108/108 - 7s - loss: 4.2130 - val_loss: 4.1890\n",
            "108/108 - 7s - loss: 4.2468 - val_loss: 4.1895\n",
            "108/108 - 7s - loss: 4.2101 - val_loss: 4.1893\n",
            "108/108 - 7s - loss: 4.2289 - val_loss: 4.1891\n",
            "108/108 - 7s - loss: 4.2835 - val_loss: 4.1900\n",
            "108/108 - 7s - loss: 4.2386 - val_loss: 4.1897\n",
            "108/108 - 7s - loss: 4.2438 - val_loss: 4.1898\n",
            "108/108 - 7s - loss: 4.2173 - val_loss: 4.1896\n",
            "108/108 - 7s - loss: 4.2496 - val_loss: 4.1900\n",
            "108/108 - 7s - loss: 4.2408 - val_loss: 4.1901\n",
            "108/108 - 7s - loss: 4.2581 - val_loss: 4.1908\n",
            "108/108 - 7s - loss: 4.1993 - val_loss: 4.1909\n",
            "108/108 - 7s - loss: 4.2609 - val_loss: 4.1905\n",
            "108/108 - 7s - loss: 4.2300 - val_loss: 4.1911\n",
            "108/108 - 7s - loss: 4.2429 - val_loss: 4.1918\n",
            "108/108 - 7s - loss: 4.2434 - val_loss: 4.1917\n",
            "108/108 - 7s - loss: 4.3079 - val_loss: 4.1915\n",
            "108/108 - 7s - loss: 4.2516 - val_loss: 4.1910\n",
            "108/108 - 7s - loss: 4.2582 - val_loss: 4.1913\n",
            "108/108 - 7s - loss: 4.2730 - val_loss: 4.1922\n",
            "108/108 - 7s - loss: 4.2115 - val_loss: 4.1912\n",
            "108/108 - 7s - loss: 4.2525 - val_loss: 4.1918\n",
            "108/108 - 7s - loss: 4.2144 - val_loss: 4.1918\n",
            "108/108 - 7s - loss: 4.2260 - val_loss: 4.1916\n",
            "108/108 - 7s - loss: 4.2412 - val_loss: 4.1917\n",
            "108/108 - 7s - loss: 4.2486 - val_loss: 4.1913\n",
            "108/108 - 7s - loss: 4.1761 - val_loss: 4.1908\n",
            "108/108 - 7s - loss: 4.1908 - val_loss: 4.1902\n",
            "108/108 - 7s - loss: 4.2245 - val_loss: 4.1897\n",
            "108/108 - 7s - loss: 4.2684 - val_loss: 4.1911\n",
            "108/108 - 7s - loss: 4.2141 - val_loss: 4.1911\n",
            "108/108 - 7s - loss: 4.2503 - val_loss: 4.1912\n",
            "108/108 - 7s - loss: 4.1852 - val_loss: 4.1905\n",
            "108/108 - 7s - loss: 4.2221 - val_loss: 4.1908\n",
            "108/108 - 7s - loss: 4.2430 - val_loss: 4.1910\n",
            "108/108 - 7s - loss: 4.2588 - val_loss: 4.1914\n",
            "108/108 - 7s - loss: 4.2272 - val_loss: 4.1905\n",
            "108/108 - 7s - loss: 4.2229 - val_loss: 4.1905\n",
            "108/108 - 7s - loss: 4.2399 - val_loss: 4.1905\n",
            "108/108 - 7s - loss: 4.2626 - val_loss: 4.1908\n",
            "108/108 - 7s - loss: 4.2570 - val_loss: 4.1914\n",
            "108/108 - 7s - loss: 4.1911 - val_loss: 4.1913\n",
            "108/108 - 7s - loss: 4.2515 - val_loss: 4.1909\n",
            "108/108 - 7s - loss: 4.2565 - val_loss: 4.1906\n",
            "108/108 - 7s - loss: 4.2302 - val_loss: 4.1907\n",
            "108/108 - 7s - loss: 4.2219 - val_loss: 4.1909\n",
            "108/108 - 7s - loss: 4.2955 - val_loss: 4.1908\n",
            "108/108 - 7s - loss: 4.2229 - val_loss: 4.1897\n",
            "108/108 - 7s - loss: 4.1309 - val_loss: 4.0517\n",
            "108/108 - 7s - loss: 4.1092 - val_loss: 3.9824\n",
            "108/108 - 7s - loss: 4.0454 - val_loss: 3.9021\n",
            "108/108 - 7s - loss: 3.9904 - val_loss: 3.9113\n",
            "108/108 - 7s - loss: 3.8960 - val_loss: 3.8123\n",
            "108/108 - 7s - loss: 3.8989 - val_loss: 3.8153\n",
            "108/108 - 7s - loss: 3.8635 - val_loss: 3.7550\n",
            "108/108 - 7s - loss: 3.8504 - val_loss: 3.7656\n",
            "108/108 - 7s - loss: 3.7368 - val_loss: 3.6230\n",
            "108/108 - 7s - loss: 3.6878 - val_loss: 3.6830\n",
            "108/108 - 7s - loss: 3.6316 - val_loss: 3.5968\n",
            "108/108 - 7s - loss: 3.6084 - val_loss: 3.5973\n",
            "108/108 - 7s - loss: 3.6174 - val_loss: 3.4770\n",
            "108/108 - 7s - loss: 3.5516 - val_loss: 3.4533\n",
            "108/108 - 7s - loss: 3.5333 - val_loss: 3.4489\n",
            "108/108 - 7s - loss: 3.5287 - val_loss: 3.4502\n",
            "108/108 - 7s - loss: 3.5063 - val_loss: 3.3428\n",
            "108/108 - 7s - loss: 3.4613 - val_loss: 3.3113\n",
            "108/108 - 7s - loss: 3.4334 - val_loss: 3.3019\n",
            "108/108 - 7s - loss: 3.4398 - val_loss: 3.2722\n",
            "108/108 - 7s - loss: 3.4299 - val_loss: 3.2635\n",
            "108/108 - 7s - loss: 3.4129 - val_loss: 3.2440\n",
            "108/108 - 7s - loss: 3.4251 - val_loss: 3.2385\n",
            "108/108 - 7s - loss: 3.4101 - val_loss: 3.2821\n",
            "108/108 - 7s - loss: 3.4085 - val_loss: 3.2856\n",
            "108/108 - 7s - loss: 3.2911 - val_loss: 3.1642\n",
            "108/108 - 7s - loss: 3.3678 - val_loss: 3.2101\n",
            "108/108 - 7s - loss: 3.3394 - val_loss: 3.1405\n",
            "108/108 - 7s - loss: 3.2568 - val_loss: 3.1360\n",
            "108/108 - 7s - loss: 3.3763 - val_loss: 3.2456\n",
            "108/108 - 7s - loss: 3.3256 - val_loss: 3.1780\n",
            "108/108 - 7s - loss: 3.3284 - val_loss: 3.1375\n",
            "108/108 - 7s - loss: 3.2514 - val_loss: 3.1299\n",
            "108/108 - 7s - loss: 3.3146 - val_loss: 3.1231\n",
            "108/108 - 7s - loss: 3.1839 - val_loss: 3.1368\n",
            "108/108 - 7s - loss: 3.2979 - val_loss: 3.0861\n",
            "108/108 - 7s - loss: 3.2148 - val_loss: 3.0993\n",
            "108/108 - 7s - loss: 3.2572 - val_loss: 3.0464\n",
            "108/108 - 7s - loss: 3.2682 - val_loss: 3.1309\n",
            "108/108 - 7s - loss: 3.2941 - val_loss: 3.1022\n",
            "108/108 - 7s - loss: 3.2230 - val_loss: 3.0853\n",
            "108/108 - 7s - loss: 3.2655 - val_loss: 3.0894\n",
            "108/108 - 7s - loss: 3.1664 - val_loss: 3.0913\n",
            "108/108 - 7s - loss: 3.2385 - val_loss: 3.0594\n",
            "108/108 - 7s - loss: 3.2433 - val_loss: 3.0749\n",
            "108/108 - 7s - loss: 3.1999 - val_loss: 3.0871\n",
            "108/108 - 7s - loss: 3.2151 - val_loss: 3.0189\n",
            "108/108 - 7s - loss: 3.2401 - val_loss: 3.0594\n",
            "108/108 - 7s - loss: 3.2337 - val_loss: 3.0702\n",
            "108/108 - 7s - loss: 3.1685 - val_loss: 3.0408\n",
            "108/108 - 7s - loss: 3.2531 - val_loss: 3.0436\n",
            "108/108 - 7s - loss: 3.2216 - val_loss: 3.0145\n",
            "108/108 - 7s - loss: 3.2447 - val_loss: 3.0887\n",
            "108/108 - 7s - loss: 3.2567 - val_loss: 3.0391\n",
            "108/108 - 7s - loss: 3.1937 - val_loss: 3.0498\n",
            "108/108 - 7s - loss: 3.2438 - val_loss: 2.9893\n",
            "108/108 - 7s - loss: 3.2115 - val_loss: 3.0023\n",
            "108/108 - 7s - loss: 3.1554 - val_loss: 2.9949\n",
            "108/108 - 7s - loss: 3.1891 - val_loss: 2.9911\n",
            "108/108 - 7s - loss: 3.1730 - val_loss: 2.9885\n",
            "108/108 - 7s - loss: 3.1778 - val_loss: 3.0788\n",
            "108/108 - 7s - loss: 3.1828 - val_loss: 3.0218\n",
            "108/108 - 7s - loss: 3.2114 - val_loss: 3.0338\n",
            "108/108 - 7s - loss: 3.1994 - val_loss: 3.0551\n",
            "108/108 - 7s - loss: 3.2390 - val_loss: 2.9729\n",
            "108/108 - 7s - loss: 3.1808 - val_loss: 3.0340\n",
            "108/108 - 7s - loss: 3.2310 - val_loss: 2.9850\n",
            "108/108 - 7s - loss: 3.1822 - val_loss: 3.0179\n",
            "108/108 - 7s - loss: 3.1852 - val_loss: 3.0021\n",
            "108/108 - 7s - loss: 3.1530 - val_loss: 3.0106\n",
            "108/108 - 7s - loss: 3.1734 - val_loss: 2.9713\n",
            "108/108 - 7s - loss: 3.1784 - val_loss: 3.0277\n",
            "108/108 - 7s - loss: 3.1752 - val_loss: 2.9925\n",
            "108/108 - 7s - loss: 3.1132 - val_loss: 2.9749\n",
            "108/108 - 7s - loss: 3.1671 - val_loss: 2.9987\n",
            "108/108 - 7s - loss: 3.1878 - val_loss: 2.9771\n",
            "108/108 - 7s - loss: 3.1976 - val_loss: 2.9988\n",
            "108/108 - 7s - loss: 3.1239 - val_loss: 3.0287\n",
            "108/108 - 7s - loss: 3.1485 - val_loss: 3.0021\n",
            "108/108 - 7s - loss: 3.1495 - val_loss: 2.9766\n",
            "108/108 - 7s - loss: 3.1263 - val_loss: 2.9579\n",
            "108/108 - 7s - loss: 3.1522 - val_loss: 2.9444\n",
            "108/108 - 7s - loss: 3.1323 - val_loss: 2.9814\n",
            "108/108 - 7s - loss: 3.1599 - val_loss: 2.9957\n",
            "108/108 - 7s - loss: 3.1509 - val_loss: 3.0144\n",
            "108/108 - 7s - loss: 3.1808 - val_loss: 3.0261\n",
            "108/108 - 7s - loss: 3.1313 - val_loss: 3.0251\n",
            "108/108 - 7s - loss: 3.1286 - val_loss: 3.0090\n",
            "108/108 - 7s - loss: 3.1844 - val_loss: 3.0109\n",
            "108/108 - 7s - loss: 3.1003 - val_loss: 3.0308\n",
            "108/108 - 7s - loss: 3.1620 - val_loss: 3.0632\n",
            "108/108 - 7s - loss: 3.1278 - val_loss: 3.0093\n",
            "108/108 - 7s - loss: 3.1251 - val_loss: 3.0002\n",
            "108/108 - 7s - loss: 3.1649 - val_loss: 3.0199\n",
            "108/108 - 7s - loss: 3.1446 - val_loss: 3.0674\n",
            "108/108 - 7s - loss: 3.1233 - val_loss: 3.0116\n",
            "108/108 - 7s - loss: 3.0740 - val_loss: 2.9843\n",
            "108/108 - 7s - loss: 3.0950 - val_loss: 2.9627\n",
            "108/108 - 7s - loss: 3.1409 - val_loss: 3.0272\n",
            "108/108 - 7s - loss: 3.1100 - val_loss: 3.0719\n",
            "108/108 - 7s - loss: 3.1237 - val_loss: 2.9822\n",
            "108/108 - 7s - loss: 3.1191 - val_loss: 3.0708\n",
            "108/108 - 7s - loss: 3.1218 - val_loss: 3.0176\n",
            "108/108 - 7s - loss: 3.1130 - val_loss: 3.0342\n",
            "108/108 - 7s - loss: 3.1541 - val_loss: 2.9874\n",
            "108/108 - 7s - loss: 3.1119 - val_loss: 2.9784\n",
            "108/108 - 7s - loss: 3.0777 - val_loss: 3.0162\n",
            "108/108 - 7s - loss: 3.1405 - val_loss: 3.0122\n",
            "108/108 - 7s - loss: 3.1485 - val_loss: 2.9872\n",
            "108/108 - 7s - loss: 3.0377 - val_loss: 3.0457\n",
            "108/108 - 7s - loss: 3.1162 - val_loss: 2.9732\n",
            "108/108 - 7s - loss: 3.1113 - val_loss: 3.0176\n",
            "108/108 - 7s - loss: 3.0770 - val_loss: 3.0428\n",
            "108/108 - 7s - loss: 3.1319 - val_loss: 3.0554\n",
            "108/108 - 7s - loss: 3.0704 - val_loss: 3.0461\n",
            "108/108 - 7s - loss: 3.0377 - val_loss: 3.0127\n",
            "108/108 - 7s - loss: 3.0773 - val_loss: 3.0046\n",
            "108/108 - 7s - loss: 3.0527 - val_loss: 2.9872\n",
            "108/108 - 7s - loss: 3.0838 - val_loss: 2.9996\n",
            "108/108 - 7s - loss: 3.0298 - val_loss: 3.0269\n",
            "108/108 - 7s - loss: 3.1111 - val_loss: 3.0472\n",
            "108/108 - 7s - loss: 3.0940 - val_loss: 3.1172\n",
            "108/108 - 7s - loss: 3.0315 - val_loss: 3.0310\n",
            "108/108 - 7s - loss: 3.0395 - val_loss: 3.0404\n",
            "108/108 - 7s - loss: 3.0728 - val_loss: 3.0289\n",
            "108/108 - 7s - loss: 3.0424 - val_loss: 3.0114\n",
            "108/108 - 7s - loss: 3.0497 - val_loss: 2.9870\n",
            "108/108 - 7s - loss: 3.0294 - val_loss: 3.0795\n",
            "108/108 - 7s - loss: 2.9802 - val_loss: 3.0529\n",
            "108/108 - 7s - loss: 3.0519 - val_loss: 2.9920\n",
            "108/108 - 7s - loss: 3.0797 - val_loss: 3.0562\n",
            "108/108 - 7s - loss: 3.0246 - val_loss: 3.0729\n",
            "108/108 - 7s - loss: 3.0666 - val_loss: 3.0531\n",
            "108/108 - 7s - loss: 2.9990 - val_loss: 3.0555\n",
            "108/108 - 7s - loss: 3.0429 - val_loss: 3.0350\n",
            "108/108 - 7s - loss: 3.0625 - val_loss: 3.0744\n",
            "108/108 - 7s - loss: 3.0128 - val_loss: 3.0449\n",
            "108/108 - 7s - loss: 3.0352 - val_loss: 3.0790\n",
            "108/108 - 7s - loss: 3.0693 - val_loss: 2.9978\n",
            "108/108 - 7s - loss: 3.0113 - val_loss: 3.0141\n",
            "108/108 - 7s - loss: 3.0287 - val_loss: 3.0057\n",
            "108/108 - 7s - loss: 3.0800 - val_loss: 3.0126\n",
            "108/108 - 7s - loss: 3.0666 - val_loss: 3.0668\n",
            "108/108 - 7s - loss: 3.0666 - val_loss: 3.0414\n",
            "108/108 - 7s - loss: 2.9944 - val_loss: 2.9884\n",
            "108/108 - 7s - loss: 3.0225 - val_loss: 3.0452\n",
            "108/108 - 7s - loss: 2.9915 - val_loss: 2.9999\n",
            "108/108 - 7s - loss: 3.0071 - val_loss: 3.0352\n",
            "108/108 - 7s - loss: 3.0296 - val_loss: 3.0801\n",
            "108/108 - 7s - loss: 2.9917 - val_loss: 3.0600\n",
            "108/108 - 7s - loss: 2.9878 - val_loss: 3.0015\n",
            "108/108 - 7s - loss: 2.9965 - val_loss: 2.9170\n",
            "108/108 - 7s - loss: 3.0113 - val_loss: 3.0427\n",
            "108/108 - 7s - loss: 3.0072 - val_loss: 2.9810\n",
            "108/108 - 7s - loss: 3.0738 - val_loss: 2.9665\n",
            "108/108 - 7s - loss: 3.0304 - val_loss: 2.9909\n",
            "108/108 - 7s - loss: 2.9931 - val_loss: 2.9671\n",
            "108/108 - 7s - loss: 3.0055 - val_loss: 3.0034\n",
            "108/108 - 7s - loss: 3.0242 - val_loss: 3.0464\n",
            "108/108 - 7s - loss: 3.0022 - val_loss: 3.1014\n",
            "108/108 - 7s - loss: 3.0223 - val_loss: 3.0116\n",
            "108/108 - 7s - loss: 3.0146 - val_loss: 3.0355\n",
            "108/108 - 7s - loss: 3.0063 - val_loss: 3.0638\n",
            "108/108 - 7s - loss: 2.9688 - val_loss: 3.0290\n",
            "108/108 - 7s - loss: 2.9850 - val_loss: 2.9558\n",
            "108/108 - 7s - loss: 2.9505 - val_loss: 2.9654\n",
            "108/108 - 7s - loss: 2.9548 - val_loss: 2.9692\n",
            "108/108 - 7s - loss: 3.0020 - val_loss: 3.0662\n",
            "108/108 - 7s - loss: 3.0033 - val_loss: 2.9311\n",
            "108/108 - 7s - loss: 2.9816 - val_loss: 3.0259\n",
            "108/108 - 7s - loss: 2.9196 - val_loss: 3.0027\n",
            "108/108 - 7s - loss: 2.9965 - val_loss: 3.0173\n",
            "108/108 - 7s - loss: 2.9941 - val_loss: 3.0255\n",
            "108/108 - 7s - loss: 2.9831 - val_loss: 2.9309\n",
            "108/108 - 7s - loss: 2.9818 - val_loss: 2.9396\n",
            "108/108 - 7s - loss: 2.9520 - val_loss: 3.0267\n",
            "108/108 - 7s - loss: 2.9829 - val_loss: 3.0014\n",
            "108/108 - 7s - loss: 3.0170 - val_loss: 3.0016\n",
            "108/108 - 7s - loss: 2.9856 - val_loss: 3.0723\n",
            "108/108 - 7s - loss: 2.9527 - val_loss: 3.0399\n",
            "108/108 - 7s - loss: 2.9160 - val_loss: 3.0081\n",
            "108/108 - 7s - loss: 3.0143 - val_loss: 2.9148\n",
            "108/108 - 7s - loss: 2.9937 - val_loss: 2.9762\n",
            "108/108 - 7s - loss: 2.9404 - val_loss: 2.9998\n",
            "108/108 - 7s - loss: 3.0083 - val_loss: 3.0119\n",
            "108/108 - 7s - loss: 2.9772 - val_loss: 2.9459\n",
            "108/108 - 7s - loss: 3.0309 - val_loss: 2.9681\n",
            "108/108 - 7s - loss: 2.9414 - val_loss: 2.9598\n",
            "108/108 - 7s - loss: 2.9721 - val_loss: 2.9962\n",
            "108/108 - 7s - loss: 2.9711 - val_loss: 2.9373\n",
            "108/108 - 7s - loss: 2.9433 - val_loss: 2.9120\n",
            "108/108 - 7s - loss: 3.0115 - val_loss: 2.9692\n",
            "108/108 - 7s - loss: 2.9589 - val_loss: 2.9365\n",
            "108/108 - 7s - loss: 2.9100 - val_loss: 3.0536\n",
            "108/108 - 7s - loss: 3.0165 - val_loss: 2.9294\n",
            "108/108 - 7s - loss: 2.9799 - val_loss: 2.9455\n",
            "108/108 - 7s - loss: 2.9858 - val_loss: 2.9363\n",
            "108/108 - 7s - loss: 2.9932 - val_loss: 2.9811\n",
            "108/108 - 7s - loss: 2.9340 - val_loss: 2.9499\n",
            "108/108 - 7s - loss: 2.9500 - val_loss: 2.9516\n",
            "108/108 - 7s - loss: 2.9279 - val_loss: 3.0145\n",
            "108/108 - 7s - loss: 2.9363 - val_loss: 3.0268\n",
            "108/108 - 7s - loss: 2.9390 - val_loss: 3.0408\n",
            "108/108 - 7s - loss: 2.9225 - val_loss: 2.9712\n",
            "108/108 - 7s - loss: 2.9243 - val_loss: 3.0109\n",
            "108/108 - 7s - loss: 2.9481 - val_loss: 3.0086\n",
            "108/108 - 7s - loss: 2.9129 - val_loss: 3.0119\n",
            "108/108 - 7s - loss: 2.9423 - val_loss: 2.9841\n",
            "108/108 - 7s - loss: 2.9436 - val_loss: 3.0386\n",
            "108/108 - 7s - loss: 2.8891 - val_loss: 3.1160\n",
            "108/108 - 7s - loss: 2.9351 - val_loss: 2.9752\n",
            "108/108 - 7s - loss: 2.9343 - val_loss: 2.9567\n",
            "108/108 - 7s - loss: 2.9460 - val_loss: 3.0762\n",
            "108/108 - 7s - loss: 2.9252 - val_loss: 2.9874\n",
            "108/108 - 7s - loss: 2.8492 - val_loss: 2.9900\n",
            "108/108 - 7s - loss: 2.8551 - val_loss: 3.0254\n",
            "108/108 - 7s - loss: 2.9355 - val_loss: 3.0593\n",
            "108/108 - 7s - loss: 2.8930 - val_loss: 3.0493\n",
            "108/108 - 7s - loss: 2.9666 - val_loss: 3.0879\n",
            "108/108 - 7s - loss: 2.8850 - val_loss: 3.0438\n",
            "108/108 - 7s - loss: 2.8949 - val_loss: 2.9985\n",
            "108/108 - 7s - loss: 2.8876 - val_loss: 2.9915\n",
            "108/108 - 7s - loss: 2.8676 - val_loss: 2.9734\n",
            "108/108 - 7s - loss: 2.9191 - val_loss: 2.9757\n",
            "108/108 - 7s - loss: 2.8624 - val_loss: 3.0009\n",
            "108/108 - 7s - loss: 2.8956 - val_loss: 2.9923\n",
            "108/108 - 7s - loss: 2.8403 - val_loss: 3.0944\n",
            "108/108 - 7s - loss: 2.9155 - val_loss: 2.9771\n",
            "108/108 - 7s - loss: 2.8555 - val_loss: 2.9869\n",
            "108/108 - 7s - loss: 2.8594 - val_loss: 3.0149\n",
            "108/108 - 7s - loss: 2.9144 - val_loss: 2.9391\n",
            "108/108 - 7s - loss: 2.8917 - val_loss: 2.9934\n",
            "108/108 - 7s - loss: 2.8833 - val_loss: 3.0315\n",
            "108/108 - 7s - loss: 2.8300 - val_loss: 2.9523\n",
            "108/108 - 7s - loss: 2.8932 - val_loss: 2.9060\n",
            "108/108 - 7s - loss: 2.9097 - val_loss: 3.0066\n",
            "108/108 - 7s - loss: 2.8979 - val_loss: 2.9998\n",
            "108/108 - 7s - loss: 2.8913 - val_loss: 3.0144\n",
            "108/108 - 7s - loss: 2.8369 - val_loss: 3.0048\n",
            "108/108 - 7s - loss: 2.9457 - val_loss: 3.0647\n",
            "108/108 - 7s - loss: 2.9647 - val_loss: 3.0895\n",
            "108/108 - 7s - loss: 2.8787 - val_loss: 3.0390\n",
            "108/108 - 7s - loss: 2.8484 - val_loss: 3.0945\n",
            "108/108 - 7s - loss: 2.8913 - val_loss: 2.9626\n",
            "108/108 - 7s - loss: 2.8217 - val_loss: 2.9155\n",
            "108/108 - 7s - loss: 2.8585 - val_loss: 2.9591\n",
            "108/108 - 7s - loss: 2.7988 - val_loss: 2.9305\n",
            "108/108 - 7s - loss: 2.8642 - val_loss: 2.9784\n",
            "108/108 - 7s - loss: 2.8169 - val_loss: 2.9624\n",
            "108/108 - 7s - loss: 2.8243 - val_loss: 2.9373\n",
            "108/108 - 7s - loss: 2.8162 - val_loss: 2.9371\n",
            "108/108 - 7s - loss: 2.8127 - val_loss: 2.9262\n",
            "108/108 - 7s - loss: 2.8884 - val_loss: 2.9647\n",
            "108/108 - 7s - loss: 2.8786 - val_loss: 2.9391\n",
            "108/108 - 7s - loss: 2.8273 - val_loss: 2.9508\n",
            "108/108 - 7s - loss: 2.8648 - val_loss: 2.9784\n",
            "108/108 - 7s - loss: 2.8278 - val_loss: 3.0339\n",
            "108/108 - 7s - loss: 2.8829 - val_loss: 3.0463\n",
            "108/108 - 7s - loss: 2.8742 - val_loss: 3.0579\n",
            "108/108 - 7s - loss: 2.8605 - val_loss: 2.9021\n",
            "108/108 - 7s - loss: 2.8861 - val_loss: 2.9253\n",
            "108/108 - 7s - loss: 2.8101 - val_loss: 2.9991\n",
            "108/108 - 7s - loss: 2.8490 - val_loss: 2.9293\n",
            "108/108 - 7s - loss: 2.8055 - val_loss: 2.9691\n",
            "108/108 - 7s - loss: 2.7929 - val_loss: 3.0209\n",
            "108/108 - 7s - loss: 2.8520 - val_loss: 2.9120\n",
            "108/108 - 7s - loss: 2.8230 - val_loss: 2.9329\n",
            "108/108 - 7s - loss: 2.8442 - val_loss: 2.9171\n",
            "108/108 - 7s - loss: 2.8817 - val_loss: 2.9964\n",
            "108/108 - 7s - loss: 2.7951 - val_loss: 2.9544\n",
            "108/108 - 7s - loss: 2.7909 - val_loss: 2.9018\n",
            "108/108 - 7s - loss: 2.7789 - val_loss: 2.9285\n",
            "108/108 - 7s - loss: 2.8531 - val_loss: 2.8668\n",
            "108/108 - 7s - loss: 2.7403 - val_loss: 2.9779\n",
            "108/108 - 7s - loss: 2.8345 - val_loss: 2.9902\n",
            "108/108 - 7s - loss: 2.7927 - val_loss: 2.9671\n",
            "108/108 - 7s - loss: 2.7835 - val_loss: 2.9890\n",
            "108/108 - 7s - loss: 2.8130 - val_loss: 2.9298\n",
            "108/108 - 7s - loss: 2.7993 - val_loss: 3.0165\n",
            "108/108 - 7s - loss: 2.7906 - val_loss: 2.9146\n",
            "108/108 - 7s - loss: 2.7841 - val_loss: 3.0581\n",
            "108/108 - 7s - loss: 2.8150 - val_loss: 2.9132\n",
            "108/108 - 7s - loss: 2.8001 - val_loss: 2.9422\n",
            "108/108 - 7s - loss: 2.8099 - val_loss: 2.9910\n",
            "108/108 - 7s - loss: 2.7847 - val_loss: 2.9502\n",
            "108/108 - 7s - loss: 2.7826 - val_loss: 2.9377\n",
            "108/108 - 7s - loss: 2.7942 - val_loss: 2.9291\n",
            "108/108 - 7s - loss: 2.7469 - val_loss: 2.9456\n",
            "108/108 - 7s - loss: 2.8072 - val_loss: 2.9408\n",
            "108/108 - 7s - loss: 2.7902 - val_loss: 2.9043\n",
            "108/108 - 7s - loss: 2.7935 - val_loss: 2.9363\n",
            "108/108 - 7s - loss: 2.7987 - val_loss: 2.9336\n",
            "108/108 - 7s - loss: 2.7709 - val_loss: 2.9295\n",
            "108/108 - 7s - loss: 2.7965 - val_loss: 2.9858\n",
            "108/108 - 7s - loss: 2.8015 - val_loss: 2.9297\n",
            "108/108 - 7s - loss: 2.7643 - val_loss: 2.9834\n",
            "108/108 - 7s - loss: 2.7966 - val_loss: 2.9430\n",
            "108/108 - 7s - loss: 2.7808 - val_loss: 2.9691\n",
            "108/108 - 7s - loss: 2.7950 - val_loss: 2.9490\n",
            "108/108 - 7s - loss: 2.7575 - val_loss: 2.8916\n",
            "108/108 - 7s - loss: 2.7656 - val_loss: 2.9479\n",
            "108/108 - 7s - loss: 2.7802 - val_loss: 2.9739\n",
            "108/108 - 7s - loss: 2.8146 - val_loss: 2.9693\n",
            "108/108 - 7s - loss: 2.7845 - val_loss: 2.9532\n",
            "108/108 - 7s - loss: 2.7467 - val_loss: 2.9332\n",
            "108/108 - 7s - loss: 2.7833 - val_loss: 2.8871\n",
            "108/108 - 7s - loss: 2.7778 - val_loss: 2.9224\n",
            "108/108 - 7s - loss: 2.7831 - val_loss: 2.9196\n",
            "108/108 - 7s - loss: 2.8179 - val_loss: 2.9802\n",
            "108/108 - 7s - loss: 2.7688 - val_loss: 2.9442\n",
            "108/108 - 7s - loss: 2.7198 - val_loss: 2.9886\n",
            "108/108 - 7s - loss: 2.7469 - val_loss: 2.9250\n",
            "108/108 - 7s - loss: 2.7521 - val_loss: 2.9884\n",
            "108/108 - 7s - loss: 2.7517 - val_loss: 3.0846\n",
            "108/108 - 7s - loss: 2.8047 - val_loss: 2.9829\n",
            "108/108 - 7s - loss: 2.7663 - val_loss: 2.9948\n",
            "108/108 - 7s - loss: 2.7465 - val_loss: 2.9498\n",
            "108/108 - 7s - loss: 2.7809 - val_loss: 2.9162\n",
            "108/108 - 7s - loss: 2.7273 - val_loss: 2.9160\n",
            "108/108 - 7s - loss: 2.7166 - val_loss: 2.8598\n",
            "108/108 - 7s - loss: 2.7669 - val_loss: 2.9196\n",
            "108/108 - 7s - loss: 2.7660 - val_loss: 2.9802\n",
            "108/108 - 7s - loss: 2.7751 - val_loss: 3.0245\n",
            "108/108 - 7s - loss: 2.7551 - val_loss: 3.0294\n",
            "108/108 - 7s - loss: 2.7778 - val_loss: 2.9864\n",
            "108/108 - 7s - loss: 2.7736 - val_loss: 2.9542\n",
            "108/108 - 7s - loss: 2.7380 - val_loss: 2.8750\n",
            "108/108 - 7s - loss: 2.7698 - val_loss: 2.9470\n",
            "108/108 - 7s - loss: 2.7555 - val_loss: 2.9714\n",
            "108/108 - 7s - loss: 2.7763 - val_loss: 2.9835\n",
            "108/108 - 7s - loss: 2.8032 - val_loss: 2.8477\n",
            "108/108 - 7s - loss: 2.7172 - val_loss: 2.9173\n",
            "108/108 - 7s - loss: 2.7711 - val_loss: 2.8947\n",
            "108/108 - 7s - loss: 2.7787 - val_loss: 2.9213\n",
            "108/108 - 7s - loss: 2.7415 - val_loss: 2.9170\n",
            "108/108 - 7s - loss: 2.7598 - val_loss: 2.9683\n",
            "108/108 - 7s - loss: 2.7282 - val_loss: 2.9612\n",
            "108/108 - 7s - loss: 2.7782 - val_loss: 2.9632\n",
            "108/108 - 7s - loss: 2.7678 - val_loss: 3.0315\n",
            "108/108 - 7s - loss: 2.7307 - val_loss: 2.8286\n",
            "108/108 - 7s - loss: 2.7476 - val_loss: 3.0132\n",
            "108/108 - 7s - loss: 2.6999 - val_loss: 2.9686\n",
            "108/108 - 7s - loss: 2.7115 - val_loss: 2.9585\n",
            "108/108 - 7s - loss: 2.7518 - val_loss: 2.9396\n",
            "108/108 - 7s - loss: 2.7222 - val_loss: 2.9436\n",
            "108/108 - 7s - loss: 2.7383 - val_loss: 2.8746\n",
            "108/108 - 7s - loss: 2.7575 - val_loss: 2.8791\n",
            "108/108 - 7s - loss: 2.7310 - val_loss: 2.8630\n",
            "108/108 - 7s - loss: 2.7031 - val_loss: 2.9075\n",
            "108/108 - 7s - loss: 2.7204 - val_loss: 2.8866\n",
            "108/108 - 7s - loss: 2.7556 - val_loss: 2.9060\n",
            "108/108 - 7s - loss: 2.7536 - val_loss: 2.9745\n",
            "108/108 - 7s - loss: 2.7701 - val_loss: 2.9434\n",
            "108/108 - 7s - loss: 2.7258 - val_loss: 2.9101\n",
            "108/108 - 7s - loss: 2.7581 - val_loss: 2.8929\n",
            "108/108 - 7s - loss: 2.7111 - val_loss: 2.9170\n",
            "108/108 - 7s - loss: 2.7439 - val_loss: 2.9253\n",
            "108/108 - 7s - loss: 2.7311 - val_loss: 2.9294\n",
            "108/108 - 7s - loss: 2.7430 - val_loss: 2.9393\n",
            "108/108 - 7s - loss: 2.7135 - val_loss: 2.8794\n",
            "108/108 - 7s - loss: 2.7389 - val_loss: 2.9255\n",
            "108/108 - 7s - loss: 2.7161 - val_loss: 2.9017\n",
            "0.6\n",
            "108/108 - 14s - loss: 10.4344 - val_loss: 10.4379\n",
            "108/108 - 7s - loss: 9.8739 - val_loss: 9.9698\n",
            "108/108 - 7s - loss: 9.4297 - val_loss: 9.5613\n",
            "108/108 - 7s - loss: 9.0353 - val_loss: 9.1872\n",
            "108/108 - 7s - loss: 8.6846 - val_loss: 8.8414\n",
            "108/108 - 7s - loss: 8.3466 - val_loss: 8.5168\n",
            "108/108 - 7s - loss: 8.0500 - val_loss: 8.2139\n",
            "108/108 - 7s - loss: 7.7581 - val_loss: 7.9275\n",
            "108/108 - 7s - loss: 7.4668 - val_loss: 7.6547\n",
            "108/108 - 7s - loss: 7.2359 - val_loss: 7.3999\n",
            "108/108 - 7s - loss: 6.9930 - val_loss: 7.1586\n",
            "108/108 - 7s - loss: 6.7988 - val_loss: 6.9317\n",
            "108/108 - 7s - loss: 6.5417 - val_loss: 6.7157\n",
            "108/108 - 7s - loss: 6.3634 - val_loss: 6.5121\n",
            "108/108 - 7s - loss: 6.1668 - val_loss: 6.3199\n",
            "108/108 - 7s - loss: 5.9895 - val_loss: 6.1392\n",
            "108/108 - 7s - loss: 5.8204 - val_loss: 5.9681\n",
            "108/108 - 7s - loss: 5.6321 - val_loss: 5.8075\n",
            "108/108 - 7s - loss: 5.5295 - val_loss: 5.6573\n",
            "108/108 - 7s - loss: 5.4018 - val_loss: 5.5172\n",
            "108/108 - 7s - loss: 5.2841 - val_loss: 5.3871\n",
            "108/108 - 7s - loss: 5.1498 - val_loss: 5.2635\n",
            "108/108 - 7s - loss: 5.0445 - val_loss: 5.1497\n",
            "108/108 - 7s - loss: 4.9746 - val_loss: 5.0440\n",
            "108/108 - 7s - loss: 4.8973 - val_loss: 4.9484\n",
            "108/108 - 7s - loss: 4.7913 - val_loss: 4.8591\n",
            "108/108 - 7s - loss: 4.7335 - val_loss: 4.7785\n",
            "108/108 - 7s - loss: 4.7154 - val_loss: 4.7048\n",
            "108/108 - 7s - loss: 4.5839 - val_loss: 4.6379\n",
            "108/108 - 7s - loss: 4.5280 - val_loss: 4.5774\n",
            "108/108 - 7s - loss: 4.5431 - val_loss: 4.5238\n",
            "108/108 - 7s - loss: 4.5619 - val_loss: 4.4773\n",
            "108/108 - 7s - loss: 4.4584 - val_loss: 4.4358\n",
            "108/108 - 7s - loss: 4.4718 - val_loss: 4.3992\n",
            "108/108 - 7s - loss: 4.4320 - val_loss: 4.3654\n",
            "108/108 - 7s - loss: 4.3939 - val_loss: 4.3350\n",
            "108/108 - 7s - loss: 4.3358 - val_loss: 4.3072\n",
            "108/108 - 7s - loss: 4.2981 - val_loss: 4.2812\n",
            "108/108 - 7s - loss: 4.3121 - val_loss: 4.2583\n",
            "108/108 - 7s - loss: 4.2552 - val_loss: 4.2392\n",
            "108/108 - 7s - loss: 4.2696 - val_loss: 4.2239\n",
            "108/108 - 7s - loss: 4.2602 - val_loss: 4.2077\n",
            "108/108 - 7s - loss: 4.3025 - val_loss: 4.1948\n",
            "108/108 - 7s - loss: 4.2647 - val_loss: 4.1837\n",
            "108/108 - 7s - loss: 4.2276 - val_loss: 4.1732\n",
            "108/108 - 7s - loss: 4.2262 - val_loss: 4.1639\n",
            "108/108 - 7s - loss: 4.2561 - val_loss: 4.1566\n",
            "108/108 - 7s - loss: 4.1751 - val_loss: 4.1486\n",
            "108/108 - 7s - loss: 4.2576 - val_loss: 4.1433\n",
            "108/108 - 7s - loss: 4.1893 - val_loss: 4.1381\n",
            "108/108 - 7s - loss: 4.2176 - val_loss: 4.1329\n",
            "108/108 - 7s - loss: 4.1730 - val_loss: 4.1267\n",
            "108/108 - 7s - loss: 4.2623 - val_loss: 4.1219\n",
            "108/108 - 7s - loss: 4.2581 - val_loss: 4.1176\n",
            "108/108 - 7s - loss: 4.1860 - val_loss: 4.1141\n",
            "108/108 - 7s - loss: 4.2302 - val_loss: 4.1104\n",
            "108/108 - 7s - loss: 4.2019 - val_loss: 4.1067\n",
            "108/108 - 7s - loss: 4.2372 - val_loss: 4.1072\n",
            "108/108 - 7s - loss: 4.2336 - val_loss: 4.1035\n",
            "108/108 - 7s - loss: 4.2174 - val_loss: 4.1012\n",
            "108/108 - 7s - loss: 4.2062 - val_loss: 4.0984\n",
            "108/108 - 7s - loss: 4.2222 - val_loss: 4.0973\n",
            "108/108 - 7s - loss: 4.1693 - val_loss: 4.0965\n",
            "108/108 - 7s - loss: 4.2330 - val_loss: 4.0955\n",
            "108/108 - 7s - loss: 4.2232 - val_loss: 4.0955\n",
            "108/108 - 7s - loss: 4.2348 - val_loss: 4.0940\n",
            "108/108 - 7s - loss: 4.1779 - val_loss: 4.0919\n",
            "108/108 - 7s - loss: 4.1966 - val_loss: 4.0922\n",
            "108/108 - 7s - loss: 4.2047 - val_loss: 4.0914\n",
            "108/108 - 7s - loss: 4.2440 - val_loss: 4.0894\n",
            "108/108 - 7s - loss: 4.2203 - val_loss: 4.0891\n",
            "108/108 - 7s - loss: 4.1472 - val_loss: 4.0893\n",
            "108/108 - 7s - loss: 4.2061 - val_loss: 4.0893\n",
            "108/108 - 7s - loss: 4.1665 - val_loss: 4.0895\n",
            "108/108 - 7s - loss: 4.1876 - val_loss: 4.0886\n",
            "108/108 - 7s - loss: 4.2613 - val_loss: 4.0898\n",
            "108/108 - 7s - loss: 4.1493 - val_loss: 4.0897\n",
            "108/108 - 7s - loss: 4.1759 - val_loss: 4.0908\n",
            "108/108 - 7s - loss: 4.1976 - val_loss: 4.0915\n",
            "108/108 - 7s - loss: 4.2558 - val_loss: 4.0914\n",
            "108/108 - 7s - loss: 4.2253 - val_loss: 4.0904\n",
            "108/108 - 7s - loss: 4.2083 - val_loss: 4.0902\n",
            "108/108 - 7s - loss: 4.2511 - val_loss: 4.0903\n",
            "108/108 - 7s - loss: 4.2645 - val_loss: 4.0902\n",
            "108/108 - 7s - loss: 4.1692 - val_loss: 4.0905\n",
            "108/108 - 7s - loss: 4.2128 - val_loss: 4.0884\n",
            "108/108 - 7s - loss: 4.2072 - val_loss: 4.0886\n",
            "108/108 - 7s - loss: 4.1900 - val_loss: 4.0890\n",
            "108/108 - 7s - loss: 4.2301 - val_loss: 4.0898\n",
            "108/108 - 7s - loss: 4.2659 - val_loss: 4.0905\n",
            "108/108 - 7s - loss: 4.2267 - val_loss: 4.0914\n",
            "108/108 - 7s - loss: 4.1741 - val_loss: 4.0909\n",
            "108/108 - 7s - loss: 4.2464 - val_loss: 4.0909\n",
            "108/108 - 7s - loss: 4.2129 - val_loss: 4.0902\n",
            "108/108 - 7s - loss: 4.2620 - val_loss: 4.0901\n",
            "108/108 - 7s - loss: 4.2139 - val_loss: 4.0895\n",
            "108/108 - 7s - loss: 4.2028 - val_loss: 4.0894\n",
            "108/108 - 7s - loss: 4.1931 - val_loss: 4.0898\n",
            "108/108 - 7s - loss: 4.2031 - val_loss: 4.0906\n",
            "108/108 - 7s - loss: 4.2003 - val_loss: 4.0913\n",
            "108/108 - 7s - loss: 4.2254 - val_loss: 4.0920\n",
            "108/108 - 7s - loss: 4.1543 - val_loss: 4.0908\n",
            "108/108 - 7s - loss: 4.2329 - val_loss: 4.0911\n",
            "108/108 - 7s - loss: 4.1406 - val_loss: 4.0909\n",
            "108/108 - 7s - loss: 4.1762 - val_loss: 4.0899\n",
            "108/108 - 7s - loss: 4.2550 - val_loss: 4.0909\n",
            "108/108 - 7s - loss: 4.2687 - val_loss: 4.0910\n",
            "108/108 - 7s - loss: 4.2032 - val_loss: 4.0915\n",
            "108/108 - 7s - loss: 4.2071 - val_loss: 4.0906\n",
            "108/108 - 7s - loss: 4.2216 - val_loss: 4.0905\n",
            "108/108 - 7s - loss: 4.1933 - val_loss: 4.0916\n",
            "108/108 - 7s - loss: 4.2308 - val_loss: 4.0901\n",
            "108/108 - 7s - loss: 4.1540 - val_loss: 4.0907\n",
            "108/108 - 7s - loss: 4.1642 - val_loss: 4.0908\n",
            "108/108 - 7s - loss: 4.1958 - val_loss: 4.0899\n",
            "108/108 - 7s - loss: 4.1347 - val_loss: 4.0891\n",
            "108/108 - 7s - loss: 4.2067 - val_loss: 4.0895\n",
            "108/108 - 7s - loss: 4.1811 - val_loss: 4.0901\n",
            "108/108 - 7s - loss: 4.1921 - val_loss: 4.0909\n",
            "108/108 - 7s - loss: 4.1635 - val_loss: 4.0904\n",
            "108/108 - 7s - loss: 4.2484 - val_loss: 4.0903\n",
            "108/108 - 7s - loss: 4.1782 - val_loss: 4.0913\n",
            "108/108 - 7s - loss: 4.1594 - val_loss: 4.0912\n",
            "108/108 - 7s - loss: 4.2221 - val_loss: 4.0903\n",
            "108/108 - 7s - loss: 4.1865 - val_loss: 4.0912\n",
            "108/108 - 7s - loss: 4.1729 - val_loss: 4.0920\n",
            "108/108 - 7s - loss: 4.2000 - val_loss: 4.0917\n",
            "108/108 - 7s - loss: 4.2976 - val_loss: 4.0921\n",
            "108/108 - 7s - loss: 4.1913 - val_loss: 4.0924\n",
            "108/108 - 7s - loss: 4.2070 - val_loss: 4.0934\n",
            "108/108 - 7s - loss: 4.1735 - val_loss: 4.0935\n",
            "108/108 - 7s - loss: 4.2376 - val_loss: 4.0931\n",
            "108/108 - 7s - loss: 4.1504 - val_loss: 4.0910\n",
            "108/108 - 7s - loss: 4.1650 - val_loss: 4.0896\n",
            "108/108 - 7s - loss: 4.1524 - val_loss: 4.0897\n",
            "108/108 - 7s - loss: 4.2010 - val_loss: 4.0886\n",
            "108/108 - 7s - loss: 4.1791 - val_loss: 4.0882\n",
            "108/108 - 7s - loss: 4.1742 - val_loss: 4.0895\n",
            "108/108 - 7s - loss: 4.1859 - val_loss: 4.0898\n",
            "108/108 - 7s - loss: 4.1791 - val_loss: 4.0877\n",
            "108/108 - 7s - loss: 4.1404 - val_loss: 4.0873\n",
            "108/108 - 7s - loss: 4.2703 - val_loss: 4.0877\n",
            "108/108 - 7s - loss: 4.2222 - val_loss: 4.0890\n",
            "108/108 - 7s - loss: 4.1851 - val_loss: 4.0874\n",
            "108/108 - 7s - loss: 4.2117 - val_loss: 4.0878\n",
            "108/108 - 7s - loss: 4.2574 - val_loss: 4.0887\n",
            "108/108 - 7s - loss: 4.2413 - val_loss: 4.0887\n",
            "108/108 - 7s - loss: 4.2038 - val_loss: 4.0886\n",
            "108/108 - 7s - loss: 4.2057 - val_loss: 4.0888\n",
            "108/108 - 7s - loss: 4.2001 - val_loss: 4.0882\n",
            "108/108 - 7s - loss: 4.2032 - val_loss: 4.0878\n",
            "108/108 - 7s - loss: 4.1952 - val_loss: 4.0864\n",
            "108/108 - 7s - loss: 4.1999 - val_loss: 4.0870\n",
            "108/108 - 7s - loss: 4.1635 - val_loss: 4.0875\n",
            "108/108 - 7s - loss: 4.1627 - val_loss: 4.0880\n",
            "108/108 - 7s - loss: 4.1879 - val_loss: 4.0879\n",
            "108/108 - 7s - loss: 4.2160 - val_loss: 4.0888\n",
            "108/108 - 7s - loss: 4.2850 - val_loss: 4.0904\n",
            "108/108 - 7s - loss: 4.2030 - val_loss: 4.0913\n",
            "108/108 - 7s - loss: 4.1412 - val_loss: 4.0908\n",
            "108/108 - 7s - loss: 4.1593 - val_loss: 4.0907\n",
            "108/108 - 7s - loss: 4.1591 - val_loss: 4.0887\n",
            "108/108 - 7s - loss: 4.2267 - val_loss: 4.0888\n",
            "108/108 - 7s - loss: 4.2037 - val_loss: 4.0891\n",
            "108/108 - 7s - loss: 4.2164 - val_loss: 4.0891\n",
            "108/108 - 7s - loss: 4.1997 - val_loss: 4.0900\n",
            "108/108 - 7s - loss: 4.2095 - val_loss: 4.0898\n",
            "108/108 - 7s - loss: 4.1903 - val_loss: 4.0905\n",
            "108/108 - 7s - loss: 4.2007 - val_loss: 4.0897\n",
            "108/108 - 7s - loss: 4.1972 - val_loss: 4.0906\n",
            "108/108 - 7s - loss: 4.1990 - val_loss: 4.0917\n",
            "108/108 - 7s - loss: 4.2042 - val_loss: 4.0911\n",
            "108/108 - 7s - loss: 4.1882 - val_loss: 4.0911\n",
            "108/108 - 7s - loss: 4.1460 - val_loss: 4.0904\n",
            "108/108 - 7s - loss: 4.1973 - val_loss: 4.0896\n",
            "108/108 - 7s - loss: 4.1838 - val_loss: 4.0887\n",
            "108/108 - 7s - loss: 4.2105 - val_loss: 4.0873\n",
            "108/108 - 7s - loss: 4.1699 - val_loss: 4.0868\n",
            "108/108 - 7s - loss: 4.2043 - val_loss: 4.0863\n",
            "108/108 - 7s - loss: 4.2021 - val_loss: 4.0865\n",
            "108/108 - 7s - loss: 4.2212 - val_loss: 4.0856\n",
            "108/108 - 7s - loss: 4.1858 - val_loss: 4.0863\n",
            "108/108 - 7s - loss: 4.3843 - val_loss: 4.1577\n",
            "108/108 - 7s - loss: 4.2429 - val_loss: 4.1480\n",
            "108/108 - 7s - loss: 4.2025 - val_loss: 4.1391\n",
            "108/108 - 7s - loss: 4.2748 - val_loss: 4.1363\n",
            "108/108 - 7s - loss: 4.2474 - val_loss: 4.1304\n",
            "108/108 - 7s - loss: 4.2377 - val_loss: 4.1268\n",
            "108/108 - 7s - loss: 4.1986 - val_loss: 4.1220\n",
            "108/108 - 7s - loss: 4.2550 - val_loss: 4.0529\n",
            "108/108 - 7s - loss: 3.9943 - val_loss: 3.7170\n",
            "108/108 - 7s - loss: 3.7992 - val_loss: 3.6880\n",
            "108/108 - 7s - loss: 3.6951 - val_loss: 3.5950\n",
            "108/108 - 7s - loss: 3.6762 - val_loss: 3.5222\n",
            "108/108 - 7s - loss: 3.6457 - val_loss: 3.4696\n",
            "108/108 - 7s - loss: 3.5921 - val_loss: 3.3933\n",
            "108/108 - 7s - loss: 3.5624 - val_loss: 3.3488\n",
            "108/108 - 7s - loss: 3.5025 - val_loss: 3.3633\n",
            "108/108 - 7s - loss: 3.5028 - val_loss: 3.2869\n",
            "108/108 - 7s - loss: 3.4216 - val_loss: 3.2165\n",
            "108/108 - 7s - loss: 3.4315 - val_loss: 3.2308\n",
            "108/108 - 7s - loss: 3.3976 - val_loss: 3.1590\n",
            "108/108 - 7s - loss: 3.4121 - val_loss: 3.1791\n",
            "108/108 - 7s - loss: 3.3734 - val_loss: 3.1623\n",
            "108/108 - 7s - loss: 3.3284 - val_loss: 3.1193\n",
            "108/108 - 7s - loss: 3.2678 - val_loss: 3.1088\n",
            "108/108 - 7s - loss: 3.2439 - val_loss: 2.9798\n",
            "108/108 - 7s - loss: 3.2649 - val_loss: 3.0268\n",
            "108/108 - 7s - loss: 3.2411 - val_loss: 2.9572\n",
            "108/108 - 7s - loss: 3.2026 - val_loss: 2.9188\n",
            "108/108 - 7s - loss: 3.2307 - val_loss: 2.9568\n",
            "108/108 - 7s - loss: 3.2571 - val_loss: 2.9625\n",
            "108/108 - 7s - loss: 3.1914 - val_loss: 3.0041\n",
            "108/108 - 7s - loss: 3.2017 - val_loss: 2.9211\n",
            "108/108 - 7s - loss: 3.1830 - val_loss: 2.8821\n",
            "108/108 - 7s - loss: 3.1959 - val_loss: 2.9041\n",
            "108/108 - 7s - loss: 3.1101 - val_loss: 2.9116\n",
            "108/108 - 7s - loss: 3.1758 - val_loss: 2.8543\n",
            "108/108 - 7s - loss: 3.1956 - val_loss: 2.8958\n",
            "108/108 - 7s - loss: 3.2005 - val_loss: 2.8932\n",
            "108/108 - 7s - loss: 3.1637 - val_loss: 2.8940\n",
            "108/108 - 7s - loss: 3.0828 - val_loss: 2.8580\n",
            "108/108 - 7s - loss: 3.0996 - val_loss: 2.8464\n",
            "108/108 - 7s - loss: 3.1650 - val_loss: 2.8158\n",
            "108/108 - 7s - loss: 3.0263 - val_loss: 2.8498\n",
            "108/108 - 7s - loss: 3.0277 - val_loss: 2.8367\n",
            "108/108 - 7s - loss: 3.0874 - val_loss: 2.8661\n",
            "108/108 - 7s - loss: 3.1081 - val_loss: 2.8114\n",
            "108/108 - 7s - loss: 3.0679 - val_loss: 2.7466\n",
            "108/108 - 7s - loss: 3.0877 - val_loss: 2.7333\n",
            "108/108 - 7s - loss: 3.0605 - val_loss: 2.7915\n",
            "108/108 - 7s - loss: 3.0679 - val_loss: 2.7798\n",
            "108/108 - 7s - loss: 3.0617 - val_loss: 2.7861\n",
            "108/108 - 7s - loss: 2.9997 - val_loss: 2.7856\n",
            "108/108 - 7s - loss: 3.0875 - val_loss: 2.8719\n",
            "108/108 - 7s - loss: 3.0995 - val_loss: 2.7822\n",
            "108/108 - 7s - loss: 3.1389 - val_loss: 2.7458\n",
            "108/108 - 7s - loss: 3.0725 - val_loss: 2.7405\n",
            "108/108 - 7s - loss: 3.0989 - val_loss: 2.8057\n",
            "108/108 - 7s - loss: 3.0886 - val_loss: 2.7667\n",
            "108/108 - 7s - loss: 3.0314 - val_loss: 2.7406\n",
            "108/108 - 7s - loss: 3.0099 - val_loss: 2.7628\n",
            "108/108 - 7s - loss: 3.0194 - val_loss: 2.8277\n",
            "108/108 - 7s - loss: 3.0114 - val_loss: 2.7634\n",
            "108/108 - 7s - loss: 2.9870 - val_loss: 2.7094\n",
            "108/108 - 7s - loss: 2.9853 - val_loss: 2.7457\n",
            "108/108 - 7s - loss: 3.0086 - val_loss: 2.7562\n",
            "108/108 - 7s - loss: 2.9181 - val_loss: 2.7105\n",
            "108/108 - 7s - loss: 2.9738 - val_loss: 2.7089\n",
            "108/108 - 7s - loss: 2.9640 - val_loss: 2.7151\n",
            "108/108 - 7s - loss: 3.0246 - val_loss: 2.7122\n",
            "108/108 - 7s - loss: 2.9913 - val_loss: 2.7466\n",
            "108/108 - 7s - loss: 3.0391 - val_loss: 2.7159\n",
            "108/108 - 7s - loss: 3.0083 - val_loss: 2.7502\n",
            "108/108 - 7s - loss: 2.9954 - val_loss: 2.7045\n",
            "108/108 - 7s - loss: 2.9823 - val_loss: 2.7293\n",
            "108/108 - 7s - loss: 2.9892 - val_loss: 2.7879\n",
            "108/108 - 7s - loss: 3.0173 - val_loss: 2.7385\n",
            "108/108 - 7s - loss: 2.9805 - val_loss: 2.8069\n",
            "108/108 - 7s - loss: 2.9775 - val_loss: 2.8089\n",
            "108/108 - 7s - loss: 3.0097 - val_loss: 2.7009\n",
            "108/108 - 7s - loss: 2.9794 - val_loss: 2.6953\n",
            "108/108 - 7s - loss: 2.9899 - val_loss: 2.6767\n",
            "108/108 - 7s - loss: 2.9436 - val_loss: 2.6774\n",
            "108/108 - 7s - loss: 2.9583 - val_loss: 2.7196\n",
            "108/108 - 7s - loss: 2.9653 - val_loss: 2.7143\n",
            "108/108 - 7s - loss: 2.9227 - val_loss: 2.7238\n",
            "108/108 - 7s - loss: 2.9274 - val_loss: 2.7297\n",
            "108/108 - 7s - loss: 2.9757 - val_loss: 2.6963\n",
            "108/108 - 7s - loss: 2.9722 - val_loss: 2.6713\n",
            "108/108 - 7s - loss: 2.9426 - val_loss: 2.7201\n",
            "108/108 - 7s - loss: 2.9454 - val_loss: 2.6589\n",
            "108/108 - 7s - loss: 2.9142 - val_loss: 2.7030\n",
            "108/108 - 7s - loss: 3.0311 - val_loss: 2.6495\n",
            "108/108 - 7s - loss: 3.0108 - val_loss: 2.6793\n",
            "108/108 - 7s - loss: 2.9621 - val_loss: 2.6784\n",
            "108/108 - 7s - loss: 2.9423 - val_loss: 2.7011\n",
            "108/108 - 7s - loss: 2.9100 - val_loss: 2.6531\n",
            "108/108 - 7s - loss: 2.9370 - val_loss: 2.6638\n",
            "108/108 - 7s - loss: 2.9415 - val_loss: 2.6695\n",
            "108/108 - 7s - loss: 2.9873 - val_loss: 2.6684\n",
            "108/108 - 7s - loss: 2.9830 - val_loss: 2.6462\n",
            "108/108 - 7s - loss: 2.9392 - val_loss: 2.7094\n",
            "108/108 - 7s - loss: 2.9381 - val_loss: 2.6167\n",
            "108/108 - 7s - loss: 2.9435 - val_loss: 2.6275\n",
            "108/108 - 7s - loss: 2.9622 - val_loss: 2.6042\n",
            "108/108 - 7s - loss: 2.9578 - val_loss: 2.7065\n",
            "108/108 - 7s - loss: 2.9185 - val_loss: 2.6752\n",
            "108/108 - 7s - loss: 2.9846 - val_loss: 2.6256\n",
            "108/108 - 7s - loss: 2.9244 - val_loss: 2.6418\n",
            "108/108 - 7s - loss: 2.9393 - val_loss: 2.7258\n",
            "108/108 - 7s - loss: 2.9828 - val_loss: 2.6362\n",
            "108/108 - 7s - loss: 2.9143 - val_loss: 2.6301\n",
            "108/108 - 7s - loss: 2.8980 - val_loss: 2.6449\n",
            "108/108 - 7s - loss: 2.9179 - val_loss: 2.6286\n",
            "108/108 - 7s - loss: 2.9574 - val_loss: 2.6886\n",
            "108/108 - 7s - loss: 2.8880 - val_loss: 2.6562\n",
            "108/108 - 7s - loss: 2.9076 - val_loss: 2.6748\n",
            "108/108 - 7s - loss: 2.9834 - val_loss: 2.6311\n",
            "108/108 - 7s - loss: 2.8807 - val_loss: 2.6527\n",
            "108/108 - 7s - loss: 2.9165 - val_loss: 2.6320\n",
            "108/108 - 7s - loss: 2.9078 - val_loss: 2.6321\n",
            "108/108 - 7s - loss: 2.9109 - val_loss: 2.6936\n",
            "108/108 - 7s - loss: 2.9303 - val_loss: 2.6402\n",
            "108/108 - 7s - loss: 2.8767 - val_loss: 2.7204\n",
            "108/108 - 7s - loss: 2.9597 - val_loss: 2.6034\n",
            "108/108 - 7s - loss: 2.9591 - val_loss: 2.6606\n",
            "108/108 - 7s - loss: 2.9015 - val_loss: 2.6446\n",
            "108/108 - 7s - loss: 2.9275 - val_loss: 2.6350\n",
            "108/108 - 7s - loss: 2.9084 - val_loss: 2.6430\n",
            "108/108 - 7s - loss: 2.9664 - val_loss: 2.7701\n",
            "108/108 - 7s - loss: 2.9313 - val_loss: 2.6910\n",
            "108/108 - 7s - loss: 2.9188 - val_loss: 2.6631\n",
            "108/108 - 7s - loss: 2.9173 - val_loss: 2.6783\n",
            "108/108 - 7s - loss: 2.9390 - val_loss: 2.6785\n",
            "108/108 - 7s - loss: 2.8695 - val_loss: 2.6909\n",
            "108/108 - 7s - loss: 2.9571 - val_loss: 2.6903\n",
            "108/108 - 7s - loss: 2.8809 - val_loss: 2.6697\n",
            "108/108 - 7s - loss: 2.9532 - val_loss: 2.7275\n",
            "108/108 - 7s - loss: 2.9509 - val_loss: 2.6770\n",
            "108/108 - 7s - loss: 2.9120 - val_loss: 2.7225\n",
            "108/108 - 7s - loss: 2.9278 - val_loss: 2.6760\n",
            "108/108 - 7s - loss: 2.8592 - val_loss: 2.7481\n",
            "108/108 - 7s - loss: 2.8831 - val_loss: 2.7140\n",
            "108/108 - 7s - loss: 2.8398 - val_loss: 2.7267\n",
            "108/108 - 7s - loss: 2.8460 - val_loss: 2.7380\n",
            "108/108 - 7s - loss: 2.9093 - val_loss: 2.7338\n",
            "108/108 - 7s - loss: 2.8908 - val_loss: 2.7274\n",
            "108/108 - 7s - loss: 2.8976 - val_loss: 2.7019\n",
            "108/108 - 7s - loss: 2.9055 - val_loss: 2.7542\n",
            "108/108 - 7s - loss: 2.8872 - val_loss: 2.6783\n",
            "108/108 - 7s - loss: 2.8939 - val_loss: 2.6842\n",
            "108/108 - 7s - loss: 2.8358 - val_loss: 2.8183\n",
            "108/108 - 7s - loss: 2.8236 - val_loss: 2.7756\n",
            "108/108 - 7s - loss: 2.8729 - val_loss: 2.7535\n",
            "108/108 - 7s - loss: 2.9284 - val_loss: 2.6375\n",
            "108/108 - 7s - loss: 2.9265 - val_loss: 2.7439\n",
            "108/108 - 7s - loss: 2.9147 - val_loss: 2.7773\n",
            "108/108 - 7s - loss: 2.9146 - val_loss: 2.7604\n",
            "108/108 - 7s - loss: 2.8407 - val_loss: 2.6496\n",
            "108/108 - 7s - loss: 2.8955 - val_loss: 2.6784\n",
            "108/108 - 7s - loss: 2.9344 - val_loss: 2.6358\n",
            "108/108 - 7s - loss: 2.8861 - val_loss: 2.6947\n",
            "108/108 - 7s - loss: 2.8731 - val_loss: 2.7853\n",
            "108/108 - 7s - loss: 2.8411 - val_loss: 2.6889\n",
            "108/108 - 7s - loss: 2.8594 - val_loss: 2.6969\n",
            "108/108 - 7s - loss: 2.8504 - val_loss: 2.7822\n",
            "108/108 - 7s - loss: 2.8828 - val_loss: 2.7035\n",
            "108/108 - 7s - loss: 2.8129 - val_loss: 2.7158\n",
            "108/108 - 7s - loss: 2.9282 - val_loss: 2.6961\n",
            "108/108 - 7s - loss: 2.8415 - val_loss: 2.7007\n",
            "108/108 - 7s - loss: 2.8530 - val_loss: 2.7129\n",
            "108/108 - 7s - loss: 2.8562 - val_loss: 2.7633\n",
            "108/108 - 7s - loss: 2.8550 - val_loss: 2.7570\n",
            "108/108 - 7s - loss: 2.8291 - val_loss: 2.7203\n",
            "108/108 - 7s - loss: 2.8282 - val_loss: 2.7467\n",
            "108/108 - 7s - loss: 2.8312 - val_loss: 2.7267\n",
            "108/108 - 7s - loss: 2.8499 - val_loss: 2.6959\n",
            "108/108 - 7s - loss: 2.8743 - val_loss: 2.7129\n",
            "108/108 - 7s - loss: 2.8372 - val_loss: 2.6672\n",
            "108/108 - 7s - loss: 2.8720 - val_loss: 2.7780\n",
            "108/108 - 7s - loss: 2.8850 - val_loss: 2.6773\n",
            "108/108 - 7s - loss: 2.7924 - val_loss: 2.7416\n",
            "108/108 - 7s - loss: 2.8225 - val_loss: 2.7785\n",
            "108/108 - 7s - loss: 2.8236 - val_loss: 2.6773\n",
            "108/108 - 7s - loss: 2.8723 - val_loss: 2.8245\n",
            "108/108 - 7s - loss: 2.8223 - val_loss: 2.7398\n",
            "108/108 - 7s - loss: 2.8101 - val_loss: 2.7870\n",
            "108/108 - 7s - loss: 2.8306 - val_loss: 2.7193\n",
            "108/108 - 7s - loss: 2.8101 - val_loss: 2.8339\n",
            "108/108 - 7s - loss: 2.8101 - val_loss: 2.7085\n",
            "108/108 - 7s - loss: 2.7863 - val_loss: 2.7859\n",
            "108/108 - 7s - loss: 2.7916 - val_loss: 2.8108\n",
            "108/108 - 7s - loss: 2.8233 - val_loss: 2.7469\n",
            "108/108 - 7s - loss: 2.8297 - val_loss: 2.7444\n",
            "108/108 - 7s - loss: 2.8096 - val_loss: 2.7549\n",
            "108/108 - 7s - loss: 2.8057 - val_loss: 2.8164\n",
            "108/108 - 7s - loss: 2.7768 - val_loss: 2.6554\n",
            "108/108 - 7s - loss: 2.8218 - val_loss: 2.7293\n",
            "108/108 - 7s - loss: 2.8086 - val_loss: 2.6829\n",
            "108/108 - 7s - loss: 2.8103 - val_loss: 2.7644\n",
            "108/108 - 7s - loss: 2.7763 - val_loss: 2.8338\n",
            "108/108 - 7s - loss: 2.7838 - val_loss: 2.8219\n",
            "108/108 - 7s - loss: 2.7658 - val_loss: 2.8085\n",
            "108/108 - 7s - loss: 2.8327 - val_loss: 2.7329\n",
            "108/108 - 7s - loss: 2.8197 - val_loss: 2.8230\n",
            "108/108 - 7s - loss: 2.8894 - val_loss: 2.6762\n",
            "108/108 - 7s - loss: 2.8701 - val_loss: 2.7386\n",
            "108/108 - 7s - loss: 2.8045 - val_loss: 2.7953\n",
            "108/108 - 7s - loss: 2.8007 - val_loss: 2.7143\n",
            "108/108 - 7s - loss: 2.8572 - val_loss: 2.7467\n",
            "108/108 - 7s - loss: 2.7664 - val_loss: 2.7466\n",
            "108/108 - 7s - loss: 2.8221 - val_loss: 2.7928\n",
            "108/108 - 7s - loss: 2.8189 - val_loss: 2.7273\n",
            "108/108 - 7s - loss: 2.7676 - val_loss: 2.8839\n",
            "108/108 - 7s - loss: 2.8325 - val_loss: 2.7287\n",
            "108/108 - 7s - loss: 2.7885 - val_loss: 2.8115\n",
            "108/108 - 7s - loss: 2.8273 - val_loss: 2.8146\n",
            "108/108 - 7s - loss: 2.7944 - val_loss: 2.8148\n",
            "108/108 - 7s - loss: 2.8052 - val_loss: 2.7640\n",
            "108/108 - 7s - loss: 2.8495 - val_loss: 2.8446\n",
            "108/108 - 7s - loss: 2.7558 - val_loss: 2.7904\n",
            "108/108 - 7s - loss: 2.7992 - val_loss: 2.7306\n",
            "108/108 - 7s - loss: 2.8505 - val_loss: 2.7021\n",
            "108/108 - 7s - loss: 2.7875 - val_loss: 2.7056\n",
            "108/108 - 7s - loss: 2.8150 - val_loss: 2.7283\n",
            "108/108 - 7s - loss: 2.8424 - val_loss: 2.7119\n",
            "108/108 - 7s - loss: 2.7747 - val_loss: 2.7490\n",
            "108/108 - 7s - loss: 2.7470 - val_loss: 2.8140\n",
            "108/108 - 7s - loss: 2.7966 - val_loss: 2.8027\n",
            "108/108 - 7s - loss: 2.7662 - val_loss: 2.7588\n",
            "108/108 - 7s - loss: 2.7547 - val_loss: 2.6588\n",
            "108/108 - 7s - loss: 2.7816 - val_loss: 2.7690\n",
            "108/108 - 7s - loss: 2.7996 - val_loss: 2.9769\n",
            "108/108 - 7s - loss: 2.7699 - val_loss: 2.8294\n",
            "108/108 - 7s - loss: 2.7867 - val_loss: 2.7867\n",
            "108/108 - 7s - loss: 2.7201 - val_loss: 2.7978\n",
            "108/108 - 7s - loss: 2.7967 - val_loss: 2.8018\n",
            "108/108 - 7s - loss: 2.8236 - val_loss: 2.7470\n",
            "108/108 - 7s - loss: 2.7985 - val_loss: 2.7585\n",
            "108/108 - 7s - loss: 2.7557 - val_loss: 2.8248\n",
            "108/108 - 7s - loss: 2.8005 - val_loss: 2.6983\n",
            "108/108 - 7s - loss: 2.7797 - val_loss: 2.8220\n",
            "108/108 - 7s - loss: 2.8094 - val_loss: 2.8359\n",
            "108/108 - 7s - loss: 2.7657 - val_loss: 2.6884\n",
            "108/108 - 7s - loss: 2.7238 - val_loss: 2.7399\n",
            "108/108 - 7s - loss: 2.7766 - val_loss: 2.7200\n",
            "108/108 - 7s - loss: 2.7414 - val_loss: 2.7308\n",
            "108/108 - 7s - loss: 2.7761 - val_loss: 2.7317\n",
            "108/108 - 7s - loss: 2.7351 - val_loss: 2.7186\n",
            "108/108 - 7s - loss: 2.7759 - val_loss: 2.7314\n",
            "108/108 - 7s - loss: 2.7343 - val_loss: 2.7407\n",
            "108/108 - 7s - loss: 2.7631 - val_loss: 2.7166\n",
            "108/108 - 7s - loss: 2.7481 - val_loss: 2.7090\n",
            "108/108 - 7s - loss: 2.7538 - val_loss: 2.7223\n",
            "108/108 - 7s - loss: 2.7499 - val_loss: 2.7295\n",
            "108/108 - 7s - loss: 2.7617 - val_loss: 2.8122\n",
            "108/108 - 7s - loss: 2.7481 - val_loss: 2.7778\n",
            "108/108 - 7s - loss: 2.7533 - val_loss: 2.8192\n",
            "108/108 - 7s - loss: 2.7336 - val_loss: 2.7414\n",
            "108/108 - 7s - loss: 2.6968 - val_loss: 2.7212\n",
            "108/108 - 7s - loss: 2.7251 - val_loss: 2.7697\n",
            "108/108 - 7s - loss: 2.7225 - val_loss: 2.7750\n",
            "108/108 - 7s - loss: 2.7158 - val_loss: 2.7746\n",
            "108/108 - 7s - loss: 2.7244 - val_loss: 2.6994\n",
            "108/108 - 7s - loss: 2.7916 - val_loss: 2.7788\n",
            "108/108 - 7s - loss: 2.7491 - val_loss: 2.7135\n",
            "108/108 - 7s - loss: 2.8014 - val_loss: 2.8588\n",
            "108/108 - 7s - loss: 2.7565 - val_loss: 2.8149\n",
            "108/108 - 7s - loss: 2.7122 - val_loss: 2.7929\n",
            "108/108 - 7s - loss: 2.7916 - val_loss: 2.7335\n",
            "108/108 - 7s - loss: 2.7316 - val_loss: 2.7691\n",
            "108/108 - 7s - loss: 2.7372 - val_loss: 2.7880\n",
            "108/108 - 7s - loss: 2.7221 - val_loss: 2.9003\n",
            "108/108 - 7s - loss: 2.7203 - val_loss: 2.7467\n",
            "108/108 - 7s - loss: 2.7941 - val_loss: 2.7818\n",
            "108/108 - 7s - loss: 2.7124 - val_loss: 2.8171\n",
            "108/108 - 7s - loss: 2.7042 - val_loss: 2.7743\n",
            "108/108 - 7s - loss: 2.7147 - val_loss: 2.7693\n",
            "108/108 - 7s - loss: 2.7107 - val_loss: 2.7578\n",
            "108/108 - 7s - loss: 2.7736 - val_loss: 2.7153\n",
            "108/108 - 7s - loss: 2.6695 - val_loss: 2.8472\n",
            "108/108 - 7s - loss: 2.7466 - val_loss: 2.6961\n",
            "108/108 - 7s - loss: 2.7214 - val_loss: 2.7813\n",
            "108/108 - 7s - loss: 2.7491 - val_loss: 2.7100\n",
            "108/108 - 7s - loss: 2.7267 - val_loss: 2.6751\n",
            "108/108 - 7s - loss: 2.7420 - val_loss: 2.6741\n",
            "108/108 - 7s - loss: 2.6995 - val_loss: 2.7069\n",
            "108/108 - 7s - loss: 2.7719 - val_loss: 2.6626\n",
            "108/108 - 7s - loss: 2.6827 - val_loss: 2.7246\n",
            "108/108 - 7s - loss: 2.7367 - val_loss: 2.7071\n",
            "108/108 - 7s - loss: 2.7278 - val_loss: 2.7095\n",
            "108/108 - 7s - loss: 2.7088 - val_loss: 2.7161\n",
            "108/108 - 7s - loss: 2.6962 - val_loss: 2.7443\n",
            "108/108 - 7s - loss: 2.7265 - val_loss: 2.6981\n",
            "108/108 - 7s - loss: 2.6768 - val_loss: 2.6996\n",
            "108/108 - 7s - loss: 2.6757 - val_loss: 2.6870\n",
            "108/108 - 7s - loss: 2.7111 - val_loss: 2.7853\n",
            "108/108 - 7s - loss: 2.6737 - val_loss: 2.7352\n",
            "108/108 - 7s - loss: 2.7332 - val_loss: 2.6795\n",
            "108/108 - 7s - loss: 2.7029 - val_loss: 2.6629\n",
            "108/108 - 7s - loss: 2.6959 - val_loss: 2.7184\n",
            "108/108 - 7s - loss: 2.7023 - val_loss: 2.7004\n",
            "108/108 - 7s - loss: 2.6764 - val_loss: 2.7999\n",
            "108/108 - 7s - loss: 2.6855 - val_loss: 2.7312\n",
            "108/108 - 7s - loss: 2.6913 - val_loss: 2.6949\n",
            "108/108 - 7s - loss: 2.7370 - val_loss: 2.7252\n",
            "108/108 - 7s - loss: 2.7009 - val_loss: 2.7746\n",
            "108/108 - 7s - loss: 2.6793 - val_loss: 2.6979\n",
            "108/108 - 7s - loss: 2.6467 - val_loss: 2.7791\n",
            "108/108 - 7s - loss: 2.6698 - val_loss: 2.7382\n",
            "108/108 - 7s - loss: 2.6940 - val_loss: 2.8293\n",
            "108/108 - 7s - loss: 2.6662 - val_loss: 2.7150\n",
            "108/108 - 7s - loss: 2.6795 - val_loss: 2.7713\n",
            "108/108 - 7s - loss: 2.7140 - val_loss: 2.6304\n",
            "108/108 - 7s - loss: 2.7136 - val_loss: 2.7593\n",
            "108/108 - 7s - loss: 2.7050 - val_loss: 2.7407\n",
            "108/108 - 7s - loss: 2.6986 - val_loss: 2.7390\n",
            "108/108 - 7s - loss: 2.6884 - val_loss: 2.7235\n",
            "108/108 - 7s - loss: 2.7077 - val_loss: 2.7016\n",
            "0.7000000000000001\n",
            "108/108 - 14s - loss: 12.1595 - val_loss: 12.1859\n",
            "108/108 - 7s - loss: 11.5072 - val_loss: 11.6199\n",
            "108/108 - 7s - loss: 10.9578 - val_loss: 11.1191\n",
            "108/108 - 7s - loss: 10.4982 - val_loss: 10.6622\n",
            "108/108 - 7s - loss: 10.0513 - val_loss: 10.2343\n",
            "108/108 - 7s - loss: 9.6419 - val_loss: 9.8314\n",
            "108/108 - 7s - loss: 9.2398 - val_loss: 9.4482\n",
            "108/108 - 7s - loss: 8.9108 - val_loss: 9.0875\n",
            "108/108 - 7s - loss: 8.5780 - val_loss: 8.7458\n",
            "108/108 - 7s - loss: 8.2092 - val_loss: 8.4184\n",
            "108/108 - 7s - loss: 7.9438 - val_loss: 8.1076\n",
            "108/108 - 7s - loss: 7.6201 - val_loss: 7.8130\n",
            "108/108 - 7s - loss: 7.3872 - val_loss: 7.5336\n",
            "108/108 - 7s - loss: 7.0895 - val_loss: 7.2660\n",
            "108/108 - 7s - loss: 6.8377 - val_loss: 7.0116\n",
            "108/108 - 7s - loss: 6.6420 - val_loss: 6.7735\n",
            "108/108 - 7s - loss: 6.4120 - val_loss: 6.5463\n",
            "108/108 - 7s - loss: 6.2015 - val_loss: 6.3300\n",
            "108/108 - 7s - loss: 6.0122 - val_loss: 6.1239\n",
            "108/108 - 7s - loss: 5.7738 - val_loss: 5.9275\n",
            "108/108 - 7s - loss: 5.6254 - val_loss: 5.7457\n",
            "108/108 - 7s - loss: 5.5196 - val_loss: 5.5735\n",
            "108/108 - 7s - loss: 5.3159 - val_loss: 5.4105\n",
            "108/108 - 7s - loss: 5.2335 - val_loss: 5.2575\n",
            "108/108 - 7s - loss: 5.0738 - val_loss: 5.1173\n",
            "108/108 - 7s - loss: 4.9863 - val_loss: 4.9881\n",
            "108/108 - 7s - loss: 4.8618 - val_loss: 4.8663\n",
            "108/108 - 7s - loss: 4.8461 - val_loss: 4.7573\n",
            "108/108 - 7s - loss: 4.6664 - val_loss: 4.6573\n",
            "108/108 - 7s - loss: 4.6352 - val_loss: 4.5620\n",
            "108/108 - 7s - loss: 4.4931 - val_loss: 4.4740\n",
            "108/108 - 7s - loss: 4.4366 - val_loss: 4.3994\n",
            "108/108 - 7s - loss: 4.4123 - val_loss: 4.3281\n",
            "108/108 - 7s - loss: 4.2931 - val_loss: 4.2625\n",
            "108/108 - 7s - loss: 4.2782 - val_loss: 4.2006\n",
            "108/108 - 7s - loss: 4.1735 - val_loss: 4.1453\n",
            "108/108 - 7s - loss: 4.1402 - val_loss: 4.0939\n",
            "108/108 - 7s - loss: 4.1447 - val_loss: 4.0444\n",
            "108/108 - 7s - loss: 4.0573 - val_loss: 4.0032\n",
            "108/108 - 7s - loss: 4.0582 - val_loss: 3.9647\n",
            "108/108 - 7s - loss: 4.0199 - val_loss: 3.9293\n",
            "108/108 - 7s - loss: 3.9273 - val_loss: 3.8949\n",
            "108/108 - 7s - loss: 3.9830 - val_loss: 3.8665\n",
            "108/108 - 7s - loss: 3.9349 - val_loss: 3.8402\n",
            "108/108 - 7s - loss: 3.8958 - val_loss: 3.8153\n",
            "108/108 - 7s - loss: 3.9018 - val_loss: 3.7913\n",
            "108/108 - 7s - loss: 3.8884 - val_loss: 3.7727\n",
            "108/108 - 7s - loss: 3.9343 - val_loss: 3.7546\n",
            "108/108 - 7s - loss: 3.9226 - val_loss: 3.7382\n",
            "108/108 - 7s - loss: 3.8722 - val_loss: 3.7231\n",
            "108/108 - 7s - loss: 3.8229 - val_loss: 3.7099\n",
            "108/108 - 7s - loss: 3.8984 - val_loss: 3.6965\n",
            "108/108 - 7s - loss: 3.9080 - val_loss: 3.6870\n",
            "108/108 - 7s - loss: 3.8218 - val_loss: 3.6765\n",
            "108/108 - 7s - loss: 3.8241 - val_loss: 3.6692\n",
            "108/108 - 7s - loss: 3.8151 - val_loss: 3.6627\n",
            "108/108 - 7s - loss: 3.8770 - val_loss: 3.6558\n",
            "108/108 - 7s - loss: 3.7567 - val_loss: 3.6512\n",
            "108/108 - 7s - loss: 3.9033 - val_loss: 3.6462\n",
            "108/108 - 7s - loss: 3.8025 - val_loss: 3.6410\n",
            "108/108 - 7s - loss: 3.8477 - val_loss: 3.6373\n",
            "108/108 - 7s - loss: 3.8761 - val_loss: 3.6329\n",
            "108/108 - 7s - loss: 3.8361 - val_loss: 3.6286\n",
            "108/108 - 7s - loss: 3.8417 - val_loss: 3.6254\n",
            "108/108 - 7s - loss: 3.8343 - val_loss: 3.6201\n",
            "108/108 - 7s - loss: 3.8501 - val_loss: 3.6177\n",
            "108/108 - 7s - loss: 3.8743 - val_loss: 3.6148\n",
            "108/108 - 7s - loss: 3.8133 - val_loss: 3.6134\n",
            "108/108 - 7s - loss: 3.7628 - val_loss: 3.6110\n",
            "108/108 - 7s - loss: 3.8033 - val_loss: 3.6098\n",
            "108/108 - 7s - loss: 3.8188 - val_loss: 3.6092\n",
            "108/108 - 7s - loss: 3.8243 - val_loss: 3.6070\n",
            "108/108 - 7s - loss: 3.8705 - val_loss: 3.6069\n",
            "108/108 - 7s - loss: 3.8020 - val_loss: 3.6052\n",
            "108/108 - 7s - loss: 3.8153 - val_loss: 3.6050\n",
            "108/108 - 7s - loss: 3.8129 - val_loss: 3.6044\n",
            "108/108 - 7s - loss: 3.7941 - val_loss: 3.6046\n",
            "108/108 - 7s - loss: 3.8279 - val_loss: 3.6026\n",
            "108/108 - 7s - loss: 3.8383 - val_loss: 3.6019\n",
            "108/108 - 7s - loss: 3.8323 - val_loss: 3.6015\n",
            "108/108 - 7s - loss: 3.7816 - val_loss: 3.6007\n",
            "108/108 - 7s - loss: 3.8825 - val_loss: 3.6025\n",
            "108/108 - 7s - loss: 3.8109 - val_loss: 3.6025\n",
            "108/108 - 7s - loss: 3.8473 - val_loss: 3.6026\n",
            "108/108 - 7s - loss: 3.7859 - val_loss: 3.6031\n",
            "108/108 - 7s - loss: 3.8089 - val_loss: 3.6018\n",
            "108/108 - 7s - loss: 3.8294 - val_loss: 3.6006\n",
            "108/108 - 7s - loss: 3.8550 - val_loss: 3.6013\n",
            "108/108 - 7s - loss: 3.8215 - val_loss: 3.6021\n",
            "108/108 - 7s - loss: 3.8431 - val_loss: 3.6019\n",
            "108/108 - 7s - loss: 3.7913 - val_loss: 3.5992\n",
            "108/108 - 7s - loss: 3.8570 - val_loss: 3.5983\n",
            "108/108 - 7s - loss: 3.7776 - val_loss: 3.5986\n",
            "108/108 - 7s - loss: 3.7577 - val_loss: 3.5976\n",
            "108/108 - 7s - loss: 3.8301 - val_loss: 3.5999\n",
            "108/108 - 7s - loss: 3.8709 - val_loss: 3.6003\n",
            "108/108 - 7s - loss: 3.8667 - val_loss: 3.6010\n",
            "108/108 - 7s - loss: 3.8071 - val_loss: 3.6004\n",
            "108/108 - 7s - loss: 3.8192 - val_loss: 3.6008\n",
            "108/108 - 7s - loss: 3.7937 - val_loss: 3.6000\n",
            "108/108 - 7s - loss: 3.7997 - val_loss: 3.5995\n",
            "108/108 - 7s - loss: 3.8609 - val_loss: 3.5989\n",
            "108/108 - 7s - loss: 3.8517 - val_loss: 3.5975\n",
            "108/108 - 7s - loss: 3.7640 - val_loss: 3.5977\n",
            "108/108 - 7s - loss: 3.8220 - val_loss: 3.5981\n",
            "108/108 - 7s - loss: 3.8015 - val_loss: 3.5991\n",
            "108/108 - 7s - loss: 3.8454 - val_loss: 3.5984\n",
            "108/108 - 7s - loss: 3.9151 - val_loss: 3.5994\n",
            "108/108 - 7s - loss: 3.7980 - val_loss: 3.5983\n",
            "108/108 - 7s - loss: 3.7254 - val_loss: 3.5984\n",
            "108/108 - 7s - loss: 3.8311 - val_loss: 3.5982\n",
            "108/108 - 7s - loss: 3.8149 - val_loss: 3.5984\n",
            "108/108 - 7s - loss: 3.7851 - val_loss: 3.5969\n",
            "108/108 - 7s - loss: 3.7689 - val_loss: 3.5975\n",
            "108/108 - 7s - loss: 3.7973 - val_loss: 3.5988\n",
            "108/108 - 7s - loss: 3.8149 - val_loss: 3.5981\n",
            "108/108 - 7s - loss: 3.8193 - val_loss: 3.5984\n",
            "108/108 - 7s - loss: 3.7628 - val_loss: 3.5985\n",
            "108/108 - 7s - loss: 3.8155 - val_loss: 3.5989\n",
            "108/108 - 7s - loss: 3.8259 - val_loss: 3.5980\n",
            "108/108 - 7s - loss: 3.8085 - val_loss: 3.5983\n",
            "108/108 - 7s - loss: 3.8534 - val_loss: 3.5979\n",
            "108/108 - 7s - loss: 3.8826 - val_loss: 3.5975\n",
            "108/108 - 7s - loss: 3.7769 - val_loss: 3.5958\n",
            "108/108 - 7s - loss: 3.8257 - val_loss: 3.5964\n",
            "108/108 - 7s - loss: 3.8352 - val_loss: 3.5972\n",
            "108/108 - 7s - loss: 3.8450 - val_loss: 3.5989\n",
            "108/108 - 7s - loss: 3.7844 - val_loss: 3.5978\n",
            "108/108 - 7s - loss: 3.8323 - val_loss: 3.5967\n",
            "108/108 - 7s - loss: 3.8726 - val_loss: 3.5967\n",
            "108/108 - 7s - loss: 3.8392 - val_loss: 3.5989\n",
            "108/108 - 7s - loss: 3.7971 - val_loss: 3.5975\n",
            "108/108 - 7s - loss: 3.8116 - val_loss: 3.5997\n",
            "108/108 - 7s - loss: 3.8701 - val_loss: 3.5991\n",
            "108/108 - 7s - loss: 3.7713 - val_loss: 3.5976\n",
            "108/108 - 7s - loss: 3.7959 - val_loss: 3.5997\n",
            "108/108 - 7s - loss: 3.8719 - val_loss: 3.6005\n",
            "108/108 - 7s - loss: 3.8394 - val_loss: 3.6001\n",
            "108/108 - 7s - loss: 3.8882 - val_loss: 3.7082\n",
            "108/108 - 7s - loss: 3.7770 - val_loss: 3.6738\n",
            "108/108 - 7s - loss: 3.7877 - val_loss: 3.6644\n",
            "108/108 - 7s - loss: 3.7577 - val_loss: 3.6549\n",
            "108/108 - 7s - loss: 3.8354 - val_loss: 3.6466\n",
            "108/108 - 7s - loss: 3.8244 - val_loss: 3.6397\n",
            "108/108 - 7s - loss: 3.8394 - val_loss: 3.6329\n",
            "108/108 - 7s - loss: 3.7925 - val_loss: 3.6278\n",
            "108/108 - 7s - loss: 3.8853 - val_loss: 3.6249\n",
            "108/108 - 7s - loss: 3.7743 - val_loss: 3.6231\n",
            "108/108 - 7s - loss: 3.9103 - val_loss: 3.6184\n",
            "108/108 - 7s - loss: 3.7909 - val_loss: 3.6172\n",
            "108/108 - 7s - loss: 3.7485 - val_loss: 3.6142\n",
            "108/108 - 7s - loss: 3.8581 - val_loss: 3.6148\n",
            "108/108 - 7s - loss: 3.8214 - val_loss: 3.6124\n",
            "108/108 - 7s - loss: 3.8894 - val_loss: 3.6101\n",
            "108/108 - 7s - loss: 3.9070 - val_loss: 3.6090\n",
            "108/108 - 7s - loss: 3.8184 - val_loss: 3.6089\n",
            "108/108 - 7s - loss: 3.8138 - val_loss: 3.6090\n",
            "108/108 - 7s - loss: 3.8382 - val_loss: 3.6088\n",
            "108/108 - 7s - loss: 3.8421 - val_loss: 3.6064\n",
            "108/108 - 7s - loss: 3.9580 - val_loss: 3.6052\n",
            "108/108 - 7s - loss: 3.8291 - val_loss: 3.6046\n",
            "108/108 - 7s - loss: 3.8333 - val_loss: 3.6030\n",
            "108/108 - 7s - loss: 3.8080 - val_loss: 3.6026\n",
            "108/108 - 7s - loss: 3.8333 - val_loss: 3.6044\n",
            "108/108 - 7s - loss: 3.8421 - val_loss: 3.6012\n",
            "108/108 - 7s - loss: 3.8222 - val_loss: 3.6017\n",
            "108/108 - 7s - loss: 3.8013 - val_loss: 3.6011\n",
            "108/108 - 7s - loss: 3.8932 - val_loss: 3.6023\n",
            "108/108 - 7s - loss: 3.7886 - val_loss: 3.6023\n",
            "108/108 - 7s - loss: 3.8303 - val_loss: 3.6015\n",
            "108/108 - 7s - loss: 3.8049 - val_loss: 3.6015\n",
            "108/108 - 7s - loss: 3.8603 - val_loss: 3.6017\n",
            "108/108 - 7s - loss: 3.8291 - val_loss: 3.6000\n",
            "108/108 - 7s - loss: 3.7505 - val_loss: 3.5990\n",
            "108/108 - 7s - loss: 3.8488 - val_loss: 3.6014\n",
            "108/108 - 7s - loss: 3.7960 - val_loss: 3.6022\n",
            "108/108 - 7s - loss: 3.8254 - val_loss: 3.6023\n",
            "108/108 - 7s - loss: 3.8843 - val_loss: 3.6032\n",
            "108/108 - 7s - loss: 3.8969 - val_loss: 3.6024\n",
            "108/108 - 7s - loss: 3.8190 - val_loss: 3.6029\n",
            "108/108 - 7s - loss: 3.7630 - val_loss: 3.6021\n",
            "108/108 - 7s - loss: 3.7745 - val_loss: 3.6011\n",
            "108/108 - 7s - loss: 3.8076 - val_loss: 3.5999\n",
            "108/108 - 7s - loss: 3.8922 - val_loss: 3.6021\n",
            "108/108 - 7s - loss: 3.8250 - val_loss: 3.6003\n",
            "108/108 - 7s - loss: 3.8647 - val_loss: 3.6002\n",
            "108/108 - 7s - loss: 3.8420 - val_loss: 3.5999\n",
            "108/108 - 7s - loss: 3.7659 - val_loss: 3.6012\n",
            "108/108 - 7s - loss: 3.8355 - val_loss: 3.6016\n",
            "108/108 - 7s - loss: 3.8142 - val_loss: 3.6033\n",
            "108/108 - 7s - loss: 3.8323 - val_loss: 3.6026\n",
            "108/108 - 7s - loss: 3.8354 - val_loss: 3.6020\n",
            "108/108 - 7s - loss: 3.8273 - val_loss: 3.6029\n",
            "108/108 - 7s - loss: 3.8157 - val_loss: 3.6028\n",
            "108/108 - 7s - loss: 3.7548 - val_loss: 3.6015\n",
            "108/108 - 7s - loss: 3.8917 - val_loss: 3.6013\n",
            "108/108 - 7s - loss: 3.8449 - val_loss: 3.6004\n",
            "108/108 - 7s - loss: 3.8532 - val_loss: 3.6012\n",
            "108/108 - 7s - loss: 3.8450 - val_loss: 3.5768\n",
            "108/108 - 7s - loss: 3.6711 - val_loss: 3.4026\n",
            "108/108 - 7s - loss: 3.5045 - val_loss: 3.0743\n",
            "108/108 - 7s - loss: 3.4050 - val_loss: 3.0221\n",
            "108/108 - 7s - loss: 3.2887 - val_loss: 2.8954\n",
            "108/108 - 7s - loss: 3.2470 - val_loss: 2.8771\n",
            "108/108 - 7s - loss: 3.1698 - val_loss: 2.8025\n",
            "108/108 - 7s - loss: 3.1275 - val_loss: 2.8475\n",
            "108/108 - 7s - loss: 3.0738 - val_loss: 2.7794\n",
            "108/108 - 7s - loss: 3.0636 - val_loss: 2.7562\n",
            "108/108 - 7s - loss: 3.1213 - val_loss: 2.6772\n",
            "108/108 - 7s - loss: 3.0646 - val_loss: 2.6750\n",
            "108/108 - 7s - loss: 2.9952 - val_loss: 2.6370\n",
            "108/108 - 7s - loss: 3.0004 - val_loss: 2.6143\n",
            "108/108 - 7s - loss: 2.9358 - val_loss: 2.6214\n",
            "108/108 - 7s - loss: 2.9128 - val_loss: 2.5772\n",
            "108/108 - 7s - loss: 2.8951 - val_loss: 2.5976\n",
            "108/108 - 7s - loss: 2.9481 - val_loss: 2.5430\n",
            "108/108 - 7s - loss: 2.9215 - val_loss: 2.5108\n",
            "108/108 - 7s - loss: 2.9276 - val_loss: 2.4869\n",
            "108/108 - 7s - loss: 2.8854 - val_loss: 2.5154\n",
            "108/108 - 7s - loss: 2.8841 - val_loss: 2.6002\n",
            "108/108 - 7s - loss: 2.8399 - val_loss: 2.5098\n",
            "108/108 - 7s - loss: 2.8768 - val_loss: 2.4717\n",
            "108/108 - 7s - loss: 2.8530 - val_loss: 2.4847\n",
            "108/108 - 7s - loss: 2.8647 - val_loss: 2.4883\n",
            "108/108 - 7s - loss: 2.8586 - val_loss: 2.5428\n",
            "108/108 - 7s - loss: 2.8696 - val_loss: 2.4537\n",
            "108/108 - 7s - loss: 2.8268 - val_loss: 2.4423\n",
            "108/108 - 7s - loss: 2.8450 - val_loss: 2.5028\n",
            "108/108 - 7s - loss: 2.8452 - val_loss: 2.4473\n",
            "108/108 - 7s - loss: 2.8115 - val_loss: 2.4241\n",
            "108/108 - 7s - loss: 2.8658 - val_loss: 2.4699\n",
            "108/108 - 7s - loss: 2.7285 - val_loss: 2.4276\n",
            "108/108 - 7s - loss: 2.7947 - val_loss: 2.5332\n",
            "108/108 - 7s - loss: 2.7885 - val_loss: 2.4236\n",
            "108/108 - 7s - loss: 2.8542 - val_loss: 2.3598\n",
            "108/108 - 7s - loss: 2.8241 - val_loss: 2.3505\n",
            "108/108 - 7s - loss: 2.7288 - val_loss: 2.3075\n",
            "108/108 - 7s - loss: 2.7883 - val_loss: 2.3264\n",
            "108/108 - 7s - loss: 2.7349 - val_loss: 2.3948\n",
            "108/108 - 7s - loss: 2.8029 - val_loss: 2.3869\n",
            "108/108 - 7s - loss: 2.7879 - val_loss: 2.3232\n",
            "108/108 - 7s - loss: 2.7662 - val_loss: 2.3696\n",
            "108/108 - 7s - loss: 2.7452 - val_loss: 2.3488\n",
            "108/108 - 7s - loss: 2.7289 - val_loss: 2.3784\n",
            "108/108 - 7s - loss: 2.7401 - val_loss: 2.3019\n",
            "108/108 - 7s - loss: 2.7776 - val_loss: 2.3007\n",
            "108/108 - 7s - loss: 2.6942 - val_loss: 2.3265\n",
            "108/108 - 7s - loss: 2.7225 - val_loss: 2.3191\n",
            "108/108 - 7s - loss: 2.7361 - val_loss: 2.3276\n",
            "108/108 - 7s - loss: 2.7080 - val_loss: 2.3394\n",
            "108/108 - 7s - loss: 2.6932 - val_loss: 2.2493\n",
            "108/108 - 7s - loss: 2.7392 - val_loss: 2.3375\n",
            "108/108 - 7s - loss: 2.6668 - val_loss: 2.2423\n",
            "108/108 - 7s - loss: 2.6293 - val_loss: 2.2484\n",
            "108/108 - 7s - loss: 2.6872 - val_loss: 2.2435\n",
            "108/108 - 7s - loss: 2.7662 - val_loss: 2.2422\n",
            "108/108 - 7s - loss: 2.7168 - val_loss: 2.2932\n",
            "108/108 - 7s - loss: 2.6722 - val_loss: 2.2461\n",
            "108/108 - 7s - loss: 2.7146 - val_loss: 2.2520\n",
            "108/108 - 7s - loss: 2.7492 - val_loss: 2.2643\n",
            "108/108 - 7s - loss: 2.6797 - val_loss: 2.2138\n",
            "108/108 - 7s - loss: 2.6884 - val_loss: 2.2423\n",
            "108/108 - 7s - loss: 2.7252 - val_loss: 2.2213\n",
            "108/108 - 7s - loss: 2.6874 - val_loss: 2.2141\n",
            "108/108 - 7s - loss: 2.6728 - val_loss: 2.2409\n",
            "108/108 - 7s - loss: 2.6931 - val_loss: 2.1930\n",
            "108/108 - 7s - loss: 2.6679 - val_loss: 2.2193\n",
            "108/108 - 7s - loss: 2.7169 - val_loss: 2.2292\n",
            "108/108 - 7s - loss: 2.6858 - val_loss: 2.2613\n",
            "108/108 - 7s - loss: 2.6493 - val_loss: 2.2199\n",
            "108/108 - 7s - loss: 2.6717 - val_loss: 2.3516\n",
            "108/108 - 7s - loss: 2.6660 - val_loss: 2.2559\n",
            "108/108 - 7s - loss: 2.6111 - val_loss: 2.1955\n",
            "108/108 - 7s - loss: 2.6514 - val_loss: 2.1683\n",
            "108/108 - 7s - loss: 2.6141 - val_loss: 2.2004\n",
            "108/108 - 7s - loss: 2.6651 - val_loss: 2.2274\n",
            "108/108 - 7s - loss: 2.7049 - val_loss: 2.1791\n",
            "108/108 - 7s - loss: 2.6265 - val_loss: 2.2249\n",
            "108/108 - 7s - loss: 2.7303 - val_loss: 2.2459\n",
            "108/108 - 7s - loss: 2.6696 - val_loss: 2.1740\n",
            "108/108 - 7s - loss: 2.6441 - val_loss: 2.2672\n",
            "108/108 - 7s - loss: 2.6364 - val_loss: 2.2533\n",
            "108/108 - 7s - loss: 2.6589 - val_loss: 2.2410\n",
            "108/108 - 7s - loss: 2.6121 - val_loss: 2.1948\n",
            "108/108 - 7s - loss: 2.6579 - val_loss: 2.2241\n",
            "108/108 - 7s - loss: 2.6286 - val_loss: 2.2236\n",
            "108/108 - 7s - loss: 2.6184 - val_loss: 2.2031\n",
            "108/108 - 7s - loss: 2.6566 - val_loss: 2.2776\n",
            "108/108 - 7s - loss: 2.6101 - val_loss: 2.2317\n",
            "108/108 - 7s - loss: 2.6965 - val_loss: 2.1630\n",
            "108/108 - 7s - loss: 2.6722 - val_loss: 2.2645\n",
            "108/108 - 7s - loss: 2.6148 - val_loss: 2.2164\n",
            "108/108 - 7s - loss: 2.7040 - val_loss: 2.1703\n",
            "108/108 - 7s - loss: 2.6063 - val_loss: 2.1930\n",
            "108/108 - 7s - loss: 2.6116 - val_loss: 2.1607\n",
            "108/108 - 7s - loss: 2.5875 - val_loss: 2.1954\n",
            "108/108 - 7s - loss: 2.6170 - val_loss: 2.1671\n",
            "108/108 - 7s - loss: 2.5834 - val_loss: 2.1804\n",
            "108/108 - 7s - loss: 2.6194 - val_loss: 2.2168\n",
            "108/108 - 7s - loss: 2.5819 - val_loss: 2.1517\n",
            "108/108 - 7s - loss: 2.5910 - val_loss: 2.2167\n",
            "108/108 - 7s - loss: 2.6019 - val_loss: 2.1732\n",
            "108/108 - 7s - loss: 2.6107 - val_loss: 2.1979\n",
            "108/108 - 7s - loss: 2.5797 - val_loss: 2.1757\n",
            "108/108 - 7s - loss: 2.5567 - val_loss: 2.1807\n",
            "108/108 - 7s - loss: 2.6268 - val_loss: 2.1380\n",
            "108/108 - 7s - loss: 3.0060 - val_loss: 2.2310\n",
            "108/108 - 7s - loss: 2.6079 - val_loss: 2.2015\n",
            "108/108 - 7s - loss: 2.5842 - val_loss: 2.2408\n",
            "108/108 - 7s - loss: 2.5762 - val_loss: 2.1778\n",
            "108/108 - 7s - loss: 2.5735 - val_loss: 2.1898\n",
            "108/108 - 7s - loss: 2.5395 - val_loss: 2.2004\n",
            "108/108 - 7s - loss: 2.6554 - val_loss: 2.1798\n",
            "108/108 - 7s - loss: 2.5817 - val_loss: 2.2229\n",
            "108/108 - 7s - loss: 2.6181 - val_loss: 2.1864\n",
            "108/108 - 7s - loss: 2.6079 - val_loss: 2.1682\n",
            "108/108 - 7s - loss: 2.5332 - val_loss: 2.2201\n",
            "108/108 - 7s - loss: 2.5719 - val_loss: 2.2235\n",
            "108/108 - 7s - loss: 2.5822 - val_loss: 2.1582\n",
            "108/108 - 7s - loss: 2.6328 - val_loss: 2.1780\n",
            "108/108 - 7s - loss: 2.5732 - val_loss: 2.1590\n",
            "108/108 - 7s - loss: 2.5603 - val_loss: 2.1679\n",
            "108/108 - 7s - loss: 2.5815 - val_loss: 2.1532\n",
            "108/108 - 7s - loss: 2.6125 - val_loss: 2.1433\n",
            "108/108 - 7s - loss: 2.5959 - val_loss: 2.1319\n",
            "108/108 - 7s - loss: 2.5984 - val_loss: 2.1957\n",
            "108/108 - 7s - loss: 2.6033 - val_loss: 2.1802\n",
            "108/108 - 7s - loss: 2.5722 - val_loss: 2.1783\n",
            "108/108 - 7s - loss: 2.5599 - val_loss: 2.1653\n",
            "108/108 - 7s - loss: 2.5697 - val_loss: 2.1834\n",
            "108/108 - 7s - loss: 2.6094 - val_loss: 2.2834\n",
            "108/108 - 7s - loss: 2.6301 - val_loss: 2.1426\n",
            "108/108 - 7s - loss: 2.6103 - val_loss: 2.1817\n",
            "108/108 - 7s - loss: 2.5673 - val_loss: 2.1354\n",
            "108/108 - 7s - loss: 2.5846 - val_loss: 2.1941\n",
            "108/108 - 7s - loss: 2.5456 - val_loss: 2.1942\n",
            "108/108 - 7s - loss: 2.6180 - val_loss: 2.2347\n",
            "108/108 - 7s - loss: 2.6089 - val_loss: 2.1252\n",
            "108/108 - 7s - loss: 2.5397 - val_loss: 2.1660\n",
            "108/108 - 7s - loss: 2.5617 - val_loss: 2.1711\n",
            "108/108 - 7s - loss: 2.5080 - val_loss: 2.1360\n",
            "108/108 - 7s - loss: 2.5782 - val_loss: 2.1885\n",
            "108/108 - 7s - loss: 2.5899 - val_loss: 2.1387\n",
            "108/108 - 7s - loss: 2.6578 - val_loss: 2.1650\n",
            "108/108 - 7s - loss: 2.5750 - val_loss: 2.1518\n",
            "108/108 - 7s - loss: 2.5861 - val_loss: 2.1729\n",
            "108/108 - 7s - loss: 2.6064 - val_loss: 2.1943\n",
            "108/108 - 7s - loss: 2.5865 - val_loss: 2.2291\n",
            "108/108 - 7s - loss: 2.5167 - val_loss: 2.2028\n",
            "108/108 - 7s - loss: 2.5656 - val_loss: 2.1599\n",
            "108/108 - 7s - loss: 2.5569 - val_loss: 2.1762\n",
            "108/108 - 7s - loss: 2.5493 - val_loss: 2.1146\n",
            "108/108 - 7s - loss: 2.5364 - val_loss: 2.1245\n",
            "108/108 - 7s - loss: 2.5819 - val_loss: 2.1767\n",
            "108/108 - 7s - loss: 2.5265 - val_loss: 2.1795\n",
            "108/108 - 7s - loss: 2.5473 - val_loss: 2.1567\n",
            "108/108 - 7s - loss: 2.5567 - val_loss: 2.1370\n",
            "108/108 - 7s - loss: 2.5343 - val_loss: 2.1401\n",
            "108/108 - 7s - loss: 2.5903 - val_loss: 2.1528\n",
            "108/108 - 7s - loss: 2.5477 - val_loss: 2.1390\n",
            "108/108 - 7s - loss: 2.5576 - val_loss: 2.1333\n",
            "108/108 - 7s - loss: 2.5412 - val_loss: 2.1498\n",
            "108/108 - 7s - loss: 2.5662 - val_loss: 2.1214\n",
            "108/108 - 7s - loss: 2.5077 - val_loss: 2.1588\n",
            "108/108 - 7s - loss: 2.5749 - val_loss: 2.1317\n",
            "108/108 - 7s - loss: 2.5588 - val_loss: 2.2399\n",
            "108/108 - 7s - loss: 2.5411 - val_loss: 2.2178\n",
            "108/108 - 7s - loss: 2.5726 - val_loss: 2.1657\n",
            "108/108 - 7s - loss: 2.5456 - val_loss: 2.1608\n",
            "108/108 - 7s - loss: 2.5805 - val_loss: 2.2073\n",
            "108/108 - 7s - loss: 2.5460 - val_loss: 2.2142\n",
            "108/108 - 7s - loss: 2.5298 - val_loss: 2.2338\n",
            "108/108 - 7s - loss: 2.5387 - val_loss: 2.2072\n",
            "108/108 - 7s - loss: 2.5472 - val_loss: 2.2767\n",
            "108/108 - 7s - loss: 2.5894 - val_loss: 2.1945\n",
            "108/108 - 7s - loss: 2.5393 - val_loss: 2.1415\n",
            "108/108 - 7s - loss: 2.5011 - val_loss: 2.1800\n",
            "108/108 - 7s - loss: 2.4687 - val_loss: 2.1352\n",
            "108/108 - 7s - loss: 2.5126 - val_loss: 2.1840\n",
            "108/108 - 7s - loss: 2.4937 - val_loss: 2.1359\n",
            "108/108 - 7s - loss: 2.5971 - val_loss: 2.2326\n",
            "108/108 - 7s - loss: 2.5244 - val_loss: 2.1740\n",
            "108/108 - 7s - loss: 2.5288 - val_loss: 2.2059\n",
            "108/108 - 7s - loss: 2.5861 - val_loss: 2.1699\n",
            "108/108 - 7s - loss: 2.5938 - val_loss: 2.1388\n",
            "108/108 - 7s - loss: 2.5317 - val_loss: 2.1552\n",
            "108/108 - 7s - loss: 2.5294 - val_loss: 2.1490\n",
            "108/108 - 7s - loss: 2.5426 - val_loss: 2.1346\n",
            "108/108 - 7s - loss: 2.5160 - val_loss: 2.1774\n",
            "108/108 - 7s - loss: 2.5598 - val_loss: 2.2108\n",
            "108/108 - 7s - loss: 2.5170 - val_loss: 2.1301\n",
            "108/108 - 7s - loss: 2.5178 - val_loss: 2.1935\n",
            "108/108 - 7s - loss: 2.5225 - val_loss: 2.1392\n",
            "108/108 - 7s - loss: 2.5791 - val_loss: 2.1787\n",
            "108/108 - 7s - loss: 2.5169 - val_loss: 2.1445\n",
            "108/108 - 7s - loss: 2.5131 - val_loss: 2.1152\n",
            "108/108 - 7s - loss: 2.5089 - val_loss: 2.1761\n",
            "108/108 - 7s - loss: 2.5547 - val_loss: 2.1356\n",
            "108/108 - 7s - loss: 2.4863 - val_loss: 2.1431\n",
            "108/108 - 7s - loss: 2.5130 - val_loss: 2.1309\n",
            "108/108 - 7s - loss: 2.4813 - val_loss: 2.1393\n",
            "108/108 - 7s - loss: 2.5587 - val_loss: 2.1495\n",
            "108/108 - 7s - loss: 2.4762 - val_loss: 2.1544\n",
            "108/108 - 7s - loss: 2.5097 - val_loss: 2.1259\n",
            "108/108 - 7s - loss: 2.5141 - val_loss: 2.1325\n",
            "108/108 - 7s - loss: 2.5312 - val_loss: 2.1020\n",
            "108/108 - 7s - loss: 2.5072 - val_loss: 2.1492\n",
            "108/108 - 7s - loss: 2.5253 - val_loss: 2.1932\n",
            "108/108 - 7s - loss: 2.4602 - val_loss: 2.1414\n",
            "108/108 - 7s - loss: 2.4907 - val_loss: 2.1541\n",
            "108/108 - 7s - loss: 2.4691 - val_loss: 2.1930\n",
            "108/108 - 7s - loss: 2.4952 - val_loss: 2.1904\n",
            "108/108 - 7s - loss: 2.5142 - val_loss: 2.1165\n",
            "108/108 - 7s - loss: 2.4792 - val_loss: 2.2094\n",
            "108/108 - 7s - loss: 2.5264 - val_loss: 2.1642\n",
            "108/108 - 7s - loss: 2.5474 - val_loss: 2.1283\n",
            "108/108 - 7s - loss: 2.5115 - val_loss: 2.1639\n",
            "108/108 - 7s - loss: 2.5052 - val_loss: 2.1238\n",
            "108/108 - 7s - loss: 2.5037 - val_loss: 2.0977\n",
            "108/108 - 7s - loss: 2.5170 - val_loss: 2.1770\n",
            "108/108 - 7s - loss: 2.5152 - val_loss: 2.2620\n",
            "108/108 - 7s - loss: 2.4935 - val_loss: 2.2300\n",
            "108/108 - 7s - loss: 2.4719 - val_loss: 2.1761\n",
            "108/108 - 7s - loss: 2.5240 - val_loss: 2.2170\n",
            "108/108 - 7s - loss: 2.5011 - val_loss: 2.1735\n",
            "108/108 - 7s - loss: 2.5116 - val_loss: 2.2262\n",
            "108/108 - 7s - loss: 2.4948 - val_loss: 2.1743\n",
            "108/108 - 7s - loss: 2.5403 - val_loss: 2.1755\n",
            "108/108 - 7s - loss: 2.4564 - val_loss: 2.1676\n",
            "108/108 - 7s - loss: 2.5066 - val_loss: 2.0956\n",
            "108/108 - 7s - loss: 2.5176 - val_loss: 2.1069\n",
            "108/108 - 7s - loss: 2.5399 - val_loss: 2.1690\n",
            "108/108 - 7s - loss: 2.5097 - val_loss: 2.1591\n",
            "108/108 - 7s - loss: 2.5010 - val_loss: 2.1484\n",
            "108/108 - 7s - loss: 2.4717 - val_loss: 2.1127\n",
            "108/108 - 7s - loss: 2.5472 - val_loss: 2.1322\n",
            "108/108 - 7s - loss: 2.5481 - val_loss: 2.2022\n",
            "108/108 - 7s - loss: 2.4760 - val_loss: 2.1520\n",
            "108/108 - 7s - loss: 2.5018 - val_loss: 2.1737\n",
            "108/108 - 7s - loss: 2.5167 - val_loss: 2.1365\n",
            "108/108 - 7s - loss: 2.4640 - val_loss: 2.1430\n",
            "108/108 - 7s - loss: 2.4931 - val_loss: 2.1171\n",
            "108/108 - 7s - loss: 2.5026 - val_loss: 2.1101\n",
            "108/108 - 7s - loss: 2.5176 - val_loss: 2.1423\n",
            "108/108 - 7s - loss: 2.4493 - val_loss: 2.1975\n",
            "108/108 - 7s - loss: 2.5022 - val_loss: 2.1065\n",
            "108/108 - 7s - loss: 2.4571 - val_loss: 2.1340\n",
            "108/108 - 7s - loss: 2.5361 - val_loss: 2.1577\n",
            "108/108 - 7s - loss: 2.4852 - val_loss: 2.1156\n",
            "108/108 - 7s - loss: 2.5569 - val_loss: 2.1541\n",
            "108/108 - 7s - loss: 2.4895 - val_loss: 2.1666\n",
            "108/108 - 7s - loss: 2.4375 - val_loss: 2.1398\n",
            "108/108 - 7s - loss: 2.4545 - val_loss: 2.1206\n",
            "108/108 - 7s - loss: 2.4463 - val_loss: 2.1375\n",
            "108/108 - 7s - loss: 2.4301 - val_loss: 2.1153\n",
            "108/108 - 7s - loss: 2.4618 - val_loss: 2.1340\n",
            "108/108 - 7s - loss: 2.5146 - val_loss: 2.1080\n",
            "108/108 - 7s - loss: 2.4483 - val_loss: 2.1683\n",
            "108/108 - 7s - loss: 2.4520 - val_loss: 2.1248\n",
            "108/108 - 7s - loss: 2.4417 - val_loss: 2.1607\n",
            "108/108 - 7s - loss: 2.4518 - val_loss: 2.0884\n",
            "108/108 - 7s - loss: 2.4321 - val_loss: 2.1492\n",
            "108/108 - 7s - loss: 2.4496 - val_loss: 2.0813\n",
            "108/108 - 7s - loss: 2.4798 - val_loss: 2.1004\n",
            "108/108 - 7s - loss: 2.3870 - val_loss: 2.0888\n",
            "108/108 - 7s - loss: 2.4659 - val_loss: 2.1132\n",
            "108/108 - 7s - loss: 2.4638 - val_loss: 2.0981\n",
            "108/108 - 7s - loss: 2.4401 - val_loss: 2.1636\n",
            "108/108 - 7s - loss: 2.4973 - val_loss: 2.1208\n",
            "108/108 - 7s - loss: 2.5131 - val_loss: 2.1581\n",
            "108/108 - 7s - loss: 2.4340 - val_loss: 2.1455\n",
            "108/108 - 7s - loss: 2.4643 - val_loss: 2.1145\n",
            "108/108 - 7s - loss: 2.4555 - val_loss: 2.1211\n",
            "108/108 - 7s - loss: 2.4767 - val_loss: 2.1590\n",
            "108/108 - 7s - loss: 2.4591 - val_loss: 2.1604\n",
            "108/108 - 7s - loss: 2.4839 - val_loss: 2.0782\n",
            "108/108 - 7s - loss: 2.4748 - val_loss: 2.1246\n",
            "108/108 - 7s - loss: 2.4289 - val_loss: 2.1233\n",
            "108/108 - 7s - loss: 2.3964 - val_loss: 2.1120\n",
            "108/108 - 7s - loss: 2.4732 - val_loss: 2.0861\n",
            "108/108 - 7s - loss: 2.4473 - val_loss: 2.1163\n",
            "108/108 - 7s - loss: 2.4240 - val_loss: 2.1304\n",
            "108/108 - 7s - loss: 2.4374 - val_loss: 2.0941\n",
            "108/108 - 7s - loss: 2.4472 - val_loss: 2.1674\n",
            "108/108 - 7s - loss: 2.4057 - val_loss: 2.1343\n",
            "108/108 - 7s - loss: 2.4347 - val_loss: 2.1102\n",
            "108/108 - 7s - loss: 2.4386 - val_loss: 2.1549\n",
            "108/108 - 7s - loss: 2.4621 - val_loss: 2.0772\n",
            "108/108 - 7s - loss: 2.3841 - val_loss: 2.1026\n",
            "108/108 - 7s - loss: 2.4074 - val_loss: 2.1153\n",
            "108/108 - 7s - loss: 2.4288 - val_loss: 2.1489\n",
            "108/108 - 7s - loss: 2.4255 - val_loss: 2.1264\n",
            "108/108 - 7s - loss: 2.4416 - val_loss: 2.1504\n",
            "108/108 - 7s - loss: 2.4478 - val_loss: 2.1484\n",
            "108/108 - 7s - loss: 2.4348 - val_loss: 2.1241\n",
            "108/108 - 7s - loss: 2.4729 - val_loss: 2.1328\n",
            "108/108 - 7s - loss: 2.4424 - val_loss: 2.1708\n",
            "108/108 - 7s - loss: 2.4197 - val_loss: 2.1252\n",
            "108/108 - 7s - loss: 2.4743 - val_loss: 2.1462\n",
            "108/108 - 7s - loss: 2.3788 - val_loss: 2.1416\n",
            "0.8\n",
            "108/108 - 14s - loss: 13.8886 - val_loss: 13.9170\n",
            "108/108 - 7s - loss: 13.1233 - val_loss: 13.2497\n",
            "108/108 - 7s - loss: 12.5034 - val_loss: 12.6580\n",
            "108/108 - 7s - loss: 11.9321 - val_loss: 12.1072\n",
            "108/108 - 7s - loss: 11.4093 - val_loss: 11.5902\n",
            "108/108 - 7s - loss: 10.9091 - val_loss: 11.0996\n",
            "108/108 - 7s - loss: 10.4677 - val_loss: 10.6376\n",
            "108/108 - 7s - loss: 9.9883 - val_loss: 10.1937\n",
            "108/108 - 7s - loss: 9.5673 - val_loss: 9.7714\n",
            "108/108 - 7s - loss: 9.1719 - val_loss: 9.3688\n",
            "108/108 - 7s - loss: 8.7845 - val_loss: 8.9823\n",
            "108/108 - 7s - loss: 8.4226 - val_loss: 8.6137\n",
            "108/108 - 7s - loss: 8.0930 - val_loss: 8.2639\n",
            "108/108 - 7s - loss: 7.7967 - val_loss: 7.9288\n",
            "108/108 - 7s - loss: 7.4450 - val_loss: 7.6084\n",
            "108/108 - 7s - loss: 7.1683 - val_loss: 7.3041\n",
            "108/108 - 7s - loss: 6.8596 - val_loss: 7.0136\n",
            "108/108 - 7s - loss: 6.6085 - val_loss: 6.7356\n",
            "108/108 - 7s - loss: 6.3287 - val_loss: 6.4695\n",
            "108/108 - 7s - loss: 6.1322 - val_loss: 6.2178\n",
            "108/108 - 7s - loss: 5.9413 - val_loss: 5.9795\n",
            "108/108 - 7s - loss: 5.6694 - val_loss: 5.7498\n",
            "108/108 - 7s - loss: 5.4688 - val_loss: 5.5355\n",
            "108/108 - 7s - loss: 5.2967 - val_loss: 5.3341\n",
            "108/108 - 7s - loss: 5.1031 - val_loss: 5.1450\n",
            "108/108 - 7s - loss: 4.9697 - val_loss: 4.9687\n",
            "108/108 - 7s - loss: 4.7164 - val_loss: 4.8035\n",
            "108/108 - 7s - loss: 4.6854 - val_loss: 4.6513\n",
            "108/108 - 7s - loss: 4.5133 - val_loss: 4.5049\n",
            "108/108 - 7s - loss: 4.4226 - val_loss: 4.3715\n",
            "108/108 - 7s - loss: 4.3197 - val_loss: 4.2479\n",
            "108/108 - 7s - loss: 4.1884 - val_loss: 4.1344\n",
            "108/108 - 7s - loss: 4.1036 - val_loss: 4.0279\n",
            "108/108 - 7s - loss: 4.0267 - val_loss: 3.9255\n",
            "108/108 - 7s - loss: 3.9230 - val_loss: 3.8310\n",
            "108/108 - 7s - loss: 3.8600 - val_loss: 3.7429\n",
            "108/108 - 7s - loss: 3.8195 - val_loss: 3.6610\n",
            "108/108 - 7s - loss: 3.6693 - val_loss: 3.5860\n",
            "108/108 - 7s - loss: 3.6002 - val_loss: 3.5174\n",
            "108/108 - 7s - loss: 3.5241 - val_loss: 3.4533\n",
            "108/108 - 7s - loss: 3.5358 - val_loss: 3.3909\n",
            "108/108 - 7s - loss: 3.5302 - val_loss: 3.3324\n",
            "108/108 - 7s - loss: 3.4287 - val_loss: 3.2783\n",
            "108/108 - 7s - loss: 3.4354 - val_loss: 3.2266\n",
            "108/108 - 7s - loss: 3.3401 - val_loss: 3.1840\n",
            "108/108 - 7s - loss: 3.2867 - val_loss: 3.1432\n",
            "108/108 - 7s - loss: 3.3244 - val_loss: 3.1065\n",
            "108/108 - 7s - loss: 3.2665 - val_loss: 3.0700\n",
            "108/108 - 7s - loss: 3.2410 - val_loss: 3.0398\n",
            "108/108 - 7s - loss: 3.2809 - val_loss: 3.0108\n",
            "108/108 - 7s - loss: 3.1551 - val_loss: 2.9835\n",
            "108/108 - 7s - loss: 3.1707 - val_loss: 2.9597\n",
            "108/108 - 7s - loss: 3.1430 - val_loss: 2.9388\n",
            "108/108 - 7s - loss: 3.1299 - val_loss: 2.9184\n",
            "108/108 - 7s - loss: 3.1387 - val_loss: 2.9018\n",
            "108/108 - 7s - loss: 3.1068 - val_loss: 2.8855\n",
            "108/108 - 7s - loss: 3.1460 - val_loss: 2.8709\n",
            "108/108 - 7s - loss: 3.0421 - val_loss: 2.8581\n",
            "108/108 - 7s - loss: 3.1166 - val_loss: 2.8473\n",
            "108/108 - 7s - loss: 3.1445 - val_loss: 2.8369\n",
            "108/108 - 7s - loss: 3.1413 - val_loss: 2.8283\n",
            "108/108 - 7s - loss: 3.0738 - val_loss: 2.8201\n",
            "108/108 - 7s - loss: 3.0950 - val_loss: 2.8131\n",
            "108/108 - 7s - loss: 3.0960 - val_loss: 2.8062\n",
            "108/108 - 7s - loss: 3.0799 - val_loss: 2.7988\n",
            "108/108 - 7s - loss: 3.0510 - val_loss: 2.7944\n",
            "108/108 - 7s - loss: 3.0574 - val_loss: 2.7900\n",
            "108/108 - 7s - loss: 3.0267 - val_loss: 2.7868\n",
            "108/108 - 7s - loss: 3.0961 - val_loss: 2.7824\n",
            "108/108 - 7s - loss: 3.1198 - val_loss: 2.7791\n",
            "108/108 - 7s - loss: 3.0794 - val_loss: 2.7762\n",
            "108/108 - 7s - loss: 3.1016 - val_loss: 2.7726\n",
            "108/108 - 7s - loss: 3.0962 - val_loss: 2.7707\n",
            "108/108 - 7s - loss: 3.0896 - val_loss: 2.7701\n",
            "108/108 - 7s - loss: 3.0763 - val_loss: 2.7702\n",
            "108/108 - 7s - loss: 3.0651 - val_loss: 2.7693\n",
            "108/108 - 7s - loss: 3.1060 - val_loss: 2.7677\n",
            "108/108 - 7s - loss: 3.0198 - val_loss: 2.7672\n",
            "108/108 - 7s - loss: 3.1131 - val_loss: 2.7659\n",
            "108/108 - 7s - loss: 3.1082 - val_loss: 2.7634\n",
            "108/108 - 7s - loss: 2.9862 - val_loss: 2.7639\n",
            "108/108 - 7s - loss: 3.1417 - val_loss: 2.7622\n",
            "108/108 - 7s - loss: 3.1382 - val_loss: 2.7609\n",
            "108/108 - 7s - loss: 3.0676 - val_loss: 2.7609\n",
            "108/108 - 7s - loss: 3.0275 - val_loss: 2.7619\n",
            "108/108 - 7s - loss: 3.0935 - val_loss: 2.7606\n",
            "108/108 - 7s - loss: 3.1204 - val_loss: 2.7603\n",
            "108/108 - 7s - loss: 3.0551 - val_loss: 2.7595\n",
            "108/108 - 7s - loss: 3.1111 - val_loss: 2.7593\n",
            "108/108 - 7s - loss: 3.1089 - val_loss: 2.7604\n",
            "108/108 - 7s - loss: 2.9945 - val_loss: 2.7599\n",
            "108/108 - 7s - loss: 3.0474 - val_loss: 2.7593\n",
            "108/108 - 7s - loss: 3.0355 - val_loss: 2.7594\n",
            "108/108 - 7s - loss: 3.0680 - val_loss: 2.7588\n",
            "108/108 - 7s - loss: 3.0374 - val_loss: 2.7583\n",
            "108/108 - 7s - loss: 3.0729 - val_loss: 2.7588\n",
            "108/108 - 7s - loss: 3.0238 - val_loss: 2.7585\n",
            "108/108 - 7s - loss: 3.0185 - val_loss: 2.7578\n",
            "108/108 - 7s - loss: 3.0058 - val_loss: 2.7587\n",
            "108/108 - 7s - loss: 3.1426 - val_loss: 2.7587\n",
            "108/108 - 7s - loss: 3.0447 - val_loss: 2.7585\n",
            "108/108 - 7s - loss: 3.1448 - val_loss: 2.7584\n",
            "108/108 - 7s - loss: 3.0525 - val_loss: 2.7579\n",
            "108/108 - 7s - loss: 2.9788 - val_loss: 2.7585\n",
            "108/108 - 7s - loss: 3.1367 - val_loss: 2.7578\n",
            "108/108 - 7s - loss: 3.1070 - val_loss: 2.7571\n",
            "108/108 - 7s - loss: 3.1265 - val_loss: 2.7562\n",
            "108/108 - 7s - loss: 3.1024 - val_loss: 2.7553\n",
            "108/108 - 7s - loss: 3.0762 - val_loss: 2.7547\n",
            "108/108 - 7s - loss: 3.1000 - val_loss: 2.7557\n",
            "108/108 - 7s - loss: 3.0554 - val_loss: 2.7566\n",
            "108/108 - 7s - loss: 3.0414 - val_loss: 2.7583\n",
            "108/108 - 7s - loss: 3.1043 - val_loss: 2.7592\n",
            "108/108 - 7s - loss: 3.1676 - val_loss: 2.7596\n",
            "108/108 - 7s - loss: 3.0409 - val_loss: 2.7585\n",
            "108/108 - 7s - loss: 3.1227 - val_loss: 2.7570\n",
            "108/108 - 7s - loss: 3.0782 - val_loss: 2.7563\n",
            "108/108 - 7s - loss: 3.0529 - val_loss: 2.7562\n",
            "108/108 - 7s - loss: 3.1166 - val_loss: 2.7564\n",
            "108/108 - 7s - loss: 3.0281 - val_loss: 2.7571\n",
            "108/108 - 7s - loss: 2.9544 - val_loss: 2.7570\n",
            "108/108 - 7s - loss: 3.1150 - val_loss: 2.7575\n",
            "108/108 - 7s - loss: 3.0945 - val_loss: 2.7580\n",
            "108/108 - 7s - loss: 3.1117 - val_loss: 2.7573\n",
            "108/108 - 7s - loss: 3.0389 - val_loss: 2.7577\n",
            "108/108 - 7s - loss: 3.0705 - val_loss: 2.7584\n",
            "108/108 - 7s - loss: 3.0206 - val_loss: 2.7569\n",
            "108/108 - 7s - loss: 3.0466 - val_loss: 2.7567\n",
            "108/108 - 7s - loss: 3.0517 - val_loss: 2.7572\n",
            "108/108 - 7s - loss: 3.0594 - val_loss: 2.4918\n",
            "108/108 - 7s - loss: 2.8974 - val_loss: 2.5174\n",
            "108/108 - 7s - loss: 2.8149 - val_loss: 2.5025\n",
            "108/108 - 7s - loss: 2.8523 - val_loss: 2.5270\n",
            "108/108 - 7s - loss: 2.8382 - val_loss: 2.4561\n",
            "108/108 - 7s - loss: 2.8164 - val_loss: 2.4196\n",
            "108/108 - 7s - loss: 2.7695 - val_loss: 2.4289\n",
            "108/108 - 7s - loss: 2.7772 - val_loss: 2.3867\n",
            "108/108 - 7s - loss: 2.7803 - val_loss: 2.4191\n",
            "108/108 - 7s - loss: 2.7049 - val_loss: 2.2061\n",
            "108/108 - 7s - loss: 2.6346 - val_loss: 2.1435\n",
            "108/108 - 7s - loss: 2.6300 - val_loss: 2.1186\n",
            "108/108 - 7s - loss: 2.5081 - val_loss: 2.0662\n",
            "108/108 - 7s - loss: 2.5423 - val_loss: 2.0259\n",
            "108/108 - 7s - loss: 2.3961 - val_loss: 1.9884\n",
            "108/108 - 7s - loss: 2.4529 - val_loss: 1.9648\n",
            "108/108 - 7s - loss: 2.3969 - val_loss: 1.9371\n",
            "108/108 - 7s - loss: 2.3567 - val_loss: 1.9403\n",
            "108/108 - 7s - loss: 2.4332 - val_loss: 1.9499\n",
            "108/108 - 7s - loss: 2.3196 - val_loss: 1.8778\n",
            "108/108 - 7s - loss: 2.4621 - val_loss: 1.9371\n",
            "108/108 - 7s - loss: 2.3728 - val_loss: 1.8879\n",
            "108/108 - 7s - loss: 2.4220 - val_loss: 1.9163\n",
            "108/108 - 7s - loss: 2.3503 - val_loss: 1.8710\n",
            "108/108 - 7s - loss: 2.3771 - val_loss: 1.8848\n",
            "108/108 - 7s - loss: 2.3045 - val_loss: 1.9111\n",
            "108/108 - 7s - loss: 2.2667 - val_loss: 1.8424\n",
            "108/108 - 7s - loss: 2.2781 - val_loss: 1.8546\n",
            "108/108 - 7s - loss: 2.3446 - val_loss: 1.8512\n",
            "108/108 - 7s - loss: 2.2932 - val_loss: 1.8188\n",
            "108/108 - 7s - loss: 2.3179 - val_loss: 1.8594\n",
            "108/108 - 7s - loss: 2.3299 - val_loss: 1.7996\n",
            "108/108 - 7s - loss: 2.2365 - val_loss: 1.8865\n",
            "108/108 - 7s - loss: 2.2841 - val_loss: 1.7332\n",
            "108/108 - 7s - loss: 2.3247 - val_loss: 1.8011\n",
            "108/108 - 7s - loss: 2.2049 - val_loss: 1.7644\n",
            "108/108 - 7s - loss: 2.2999 - val_loss: 1.8021\n",
            "108/108 - 7s - loss: 2.2596 - val_loss: 1.8021\n",
            "108/108 - 7s - loss: 2.2684 - val_loss: 1.7918\n",
            "108/108 - 7s - loss: 2.2421 - val_loss: 1.7171\n",
            "108/108 - 7s - loss: 2.1501 - val_loss: 1.7418\n",
            "108/108 - 7s - loss: 2.2515 - val_loss: 1.7548\n",
            "108/108 - 7s - loss: 2.2155 - val_loss: 1.7296\n",
            "108/108 - 7s - loss: 2.2604 - val_loss: 1.7676\n",
            "108/108 - 7s - loss: 2.2242 - val_loss: 1.7002\n",
            "108/108 - 7s - loss: 2.2660 - val_loss: 1.7728\n",
            "108/108 - 7s - loss: 2.2396 - val_loss: 1.7218\n",
            "108/108 - 7s - loss: 2.2477 - val_loss: 1.7257\n",
            "108/108 - 7s - loss: 2.1899 - val_loss: 1.7565\n",
            "108/108 - 7s - loss: 2.2065 - val_loss: 1.8102\n",
            "108/108 - 7s - loss: 2.2210 - val_loss: 1.6734\n",
            "108/108 - 7s - loss: 2.1908 - val_loss: 1.7861\n",
            "108/108 - 7s - loss: 2.2579 - val_loss: 1.7359\n",
            "108/108 - 7s - loss: 2.1872 - val_loss: 1.7878\n",
            "108/108 - 7s - loss: 2.2137 - val_loss: 1.7218\n",
            "108/108 - 7s - loss: 2.1719 - val_loss: 1.6545\n",
            "108/108 - 7s - loss: 2.1391 - val_loss: 1.6803\n",
            "108/108 - 7s - loss: 2.1903 - val_loss: 1.6636\n",
            "108/108 - 7s - loss: 2.1667 - val_loss: 1.7035\n",
            "108/108 - 7s - loss: 2.1908 - val_loss: 1.6827\n",
            "108/108 - 7s - loss: 2.1960 - val_loss: 1.7775\n",
            "108/108 - 7s - loss: 2.1494 - val_loss: 1.6798\n",
            "108/108 - 7s - loss: 2.1570 - val_loss: 1.6714\n",
            "108/108 - 7s - loss: 2.1352 - val_loss: 1.6527\n",
            "108/108 - 7s - loss: 2.1592 - val_loss: 1.7185\n",
            "108/108 - 7s - loss: 2.1348 - val_loss: 1.6865\n",
            "108/108 - 7s - loss: 2.1471 - val_loss: 1.7318\n",
            "108/108 - 7s - loss: 2.1340 - val_loss: 1.6494\n",
            "108/108 - 7s - loss: 2.1547 - val_loss: 1.6468\n",
            "108/108 - 7s - loss: 2.1850 - val_loss: 1.7309\n",
            "108/108 - 7s - loss: 2.1157 - val_loss: 1.7450\n",
            "108/108 - 7s - loss: 2.1549 - val_loss: 1.6398\n",
            "108/108 - 7s - loss: 2.1745 - val_loss: 1.7723\n",
            "108/108 - 7s - loss: 2.0736 - val_loss: 1.6061\n",
            "108/108 - 7s - loss: 2.0992 - val_loss: 1.6985\n",
            "108/108 - 7s - loss: 2.1872 - val_loss: 1.6321\n",
            "108/108 - 7s - loss: 2.1516 - val_loss: 1.6465\n",
            "108/108 - 7s - loss: 2.1315 - val_loss: 1.6150\n",
            "108/108 - 7s - loss: 2.1412 - val_loss: 1.6511\n",
            "108/108 - 7s - loss: 2.1808 - val_loss: 1.7002\n",
            "108/108 - 7s - loss: 2.1101 - val_loss: 1.6167\n",
            "108/108 - 7s - loss: 2.0938 - val_loss: 1.6114\n",
            "108/108 - 7s - loss: 2.1312 - val_loss: 1.6983\n",
            "108/108 - 7s - loss: 2.1179 - val_loss: 1.7115\n",
            "108/108 - 7s - loss: 2.0757 - val_loss: 1.6377\n",
            "108/108 - 7s - loss: 2.0769 - val_loss: 1.5912\n",
            "108/108 - 7s - loss: 2.0924 - val_loss: 1.6634\n",
            "108/108 - 7s - loss: 2.0866 - val_loss: 1.6238\n",
            "108/108 - 7s - loss: 2.1150 - val_loss: 1.6109\n",
            "108/108 - 7s - loss: 2.1200 - val_loss: 1.6088\n",
            "108/108 - 7s - loss: 2.1927 - val_loss: 1.6230\n",
            "108/108 - 7s - loss: 2.0912 - val_loss: 1.5801\n",
            "108/108 - 7s - loss: 2.1549 - val_loss: 1.6481\n",
            "108/108 - 7s - loss: 2.0910 - val_loss: 1.6796\n",
            "108/108 - 7s - loss: 2.1135 - val_loss: 1.6456\n",
            "108/108 - 7s - loss: 2.1237 - val_loss: 1.5758\n",
            "108/108 - 7s - loss: 2.0230 - val_loss: 1.5989\n",
            "108/108 - 7s - loss: 2.1638 - val_loss: 1.6201\n",
            "108/108 - 7s - loss: 2.0582 - val_loss: 1.5958\n",
            "108/108 - 7s - loss: 2.0836 - val_loss: 1.7267\n",
            "108/108 - 7s - loss: 2.0481 - val_loss: 1.6215\n",
            "108/108 - 7s - loss: 2.0883 - val_loss: 1.5956\n",
            "108/108 - 7s - loss: 2.0545 - val_loss: 1.6141\n",
            "108/108 - 7s - loss: 2.1083 - val_loss: 1.6776\n",
            "108/108 - 7s - loss: 2.1131 - val_loss: 1.5802\n",
            "108/108 - 7s - loss: 2.1460 - val_loss: 1.5831\n",
            "108/108 - 7s - loss: 2.0873 - val_loss: 1.6755\n",
            "108/108 - 7s - loss: 2.0820 - val_loss: 1.6012\n",
            "108/108 - 7s - loss: 2.1702 - val_loss: 1.5594\n",
            "108/108 - 7s - loss: 2.0376 - val_loss: 1.6190\n",
            "108/108 - 7s - loss: 2.0894 - val_loss: 1.5582\n",
            "108/108 - 7s - loss: 2.1356 - val_loss: 1.6211\n",
            "108/108 - 7s - loss: 2.0655 - val_loss: 1.6434\n",
            "108/108 - 7s - loss: 2.0755 - val_loss: 1.7501\n",
            "108/108 - 7s - loss: 2.0856 - val_loss: 1.5818\n",
            "108/108 - 7s - loss: 2.0832 - val_loss: 1.5528\n",
            "108/108 - 7s - loss: 2.0652 - val_loss: 1.5500\n",
            "108/108 - 7s - loss: 2.0538 - val_loss: 1.5791\n",
            "108/108 - 7s - loss: 2.0898 - val_loss: 1.5636\n",
            "108/108 - 7s - loss: 2.0513 - val_loss: 1.5375\n",
            "108/108 - 7s - loss: 2.1135 - val_loss: 1.6084\n",
            "108/108 - 7s - loss: 2.1825 - val_loss: 1.6315\n",
            "108/108 - 7s - loss: 2.0703 - val_loss: 1.5847\n",
            "108/108 - 7s - loss: 2.0745 - val_loss: 1.6113\n",
            "108/108 - 7s - loss: 2.0490 - val_loss: 1.5967\n",
            "108/108 - 7s - loss: 2.0830 - val_loss: 1.6342\n",
            "108/108 - 7s - loss: 1.9999 - val_loss: 1.5812\n",
            "108/108 - 7s - loss: 2.0560 - val_loss: 1.6102\n",
            "108/108 - 7s - loss: 2.0421 - val_loss: 1.5740\n",
            "108/108 - 7s - loss: 2.0288 - val_loss: 1.5822\n",
            "108/108 - 7s - loss: 2.0297 - val_loss: 1.6104\n",
            "108/108 - 7s - loss: 2.0423 - val_loss: 1.5782\n",
            "108/108 - 7s - loss: 2.0686 - val_loss: 1.7548\n",
            "108/108 - 7s - loss: 2.0269 - val_loss: 1.5684\n",
            "108/108 - 7s - loss: 2.0725 - val_loss: 1.6544\n",
            "108/108 - 7s - loss: 2.0085 - val_loss: 1.5868\n",
            "108/108 - 7s - loss: 2.0505 - val_loss: 1.6358\n",
            "108/108 - 7s - loss: 2.1446 - val_loss: 1.5436\n",
            "108/108 - 7s - loss: 2.0686 - val_loss: 1.5850\n",
            "108/108 - 7s - loss: 2.0275 - val_loss: 1.5941\n",
            "108/108 - 7s - loss: 2.0903 - val_loss: 1.5578\n",
            "108/108 - 7s - loss: 2.0863 - val_loss: 1.5534\n",
            "108/108 - 7s - loss: 2.0627 - val_loss: 1.5593\n",
            "108/108 - 7s - loss: 2.0328 - val_loss: 1.5219\n",
            "108/108 - 7s - loss: 2.1329 - val_loss: 1.5864\n",
            "108/108 - 7s - loss: 2.0162 - val_loss: 1.5456\n",
            "108/108 - 7s - loss: 2.0346 - val_loss: 1.5721\n",
            "108/108 - 7s - loss: 2.0153 - val_loss: 1.7616\n",
            "108/108 - 7s - loss: 2.0150 - val_loss: 1.6134\n",
            "108/108 - 7s - loss: 2.0483 - val_loss: 1.5776\n",
            "108/108 - 7s - loss: 2.0414 - val_loss: 1.5409\n",
            "108/108 - 7s - loss: 2.0648 - val_loss: 1.6007\n",
            "108/108 - 7s - loss: 2.0960 - val_loss: 1.5661\n",
            "108/108 - 7s - loss: 2.0095 - val_loss: 1.6047\n",
            "108/108 - 7s - loss: 2.0131 - val_loss: 1.5842\n",
            "108/108 - 7s - loss: 2.0505 - val_loss: 1.5452\n",
            "108/108 - 7s - loss: 2.0879 - val_loss: 1.5355\n",
            "108/108 - 7s - loss: 1.9911 - val_loss: 1.5534\n",
            "108/108 - 7s - loss: 2.0740 - val_loss: 1.5652\n",
            "108/108 - 7s - loss: 2.0971 - val_loss: 1.5493\n",
            "108/108 - 7s - loss: 1.9718 - val_loss: 1.5687\n",
            "108/108 - 7s - loss: 1.9780 - val_loss: 1.5534\n",
            "108/108 - 7s - loss: 1.9889 - val_loss: 1.5612\n",
            "108/108 - 7s - loss: 2.0268 - val_loss: 1.5750\n",
            "108/108 - 7s - loss: 2.0087 - val_loss: 1.5704\n",
            "108/108 - 7s - loss: 1.9978 - val_loss: 1.5694\n",
            "108/108 - 7s - loss: 2.0153 - val_loss: 1.5499\n",
            "108/108 - 7s - loss: 1.9943 - val_loss: 1.5450\n",
            "108/108 - 7s - loss: 2.0329 - val_loss: 1.5292\n",
            "108/108 - 7s - loss: 1.9840 - val_loss: 1.5700\n",
            "108/108 - 7s - loss: 1.9920 - val_loss: 1.5408\n",
            "108/108 - 7s - loss: 1.9774 - val_loss: 1.5688\n",
            "108/108 - 7s - loss: 2.0413 - val_loss: 1.5860\n",
            "108/108 - 7s - loss: 2.0192 - val_loss: 1.5859\n",
            "108/108 - 7s - loss: 2.0207 - val_loss: 1.6491\n",
            "108/108 - 7s - loss: 2.0167 - val_loss: 1.5604\n",
            "108/108 - 7s - loss: 2.0201 - val_loss: 1.6100\n",
            "108/108 - 7s - loss: 1.9944 - val_loss: 1.5341\n",
            "108/108 - 7s - loss: 2.0790 - val_loss: 1.5750\n",
            "108/108 - 7s - loss: 2.0320 - val_loss: 1.6205\n",
            "108/108 - 7s - loss: 2.0247 - val_loss: 1.6893\n",
            "108/108 - 7s - loss: 2.0532 - val_loss: 1.5568\n",
            "108/108 - 7s - loss: 2.0382 - val_loss: 1.5816\n",
            "108/108 - 7s - loss: 1.9932 - val_loss: 1.5547\n",
            "108/108 - 7s - loss: 2.0022 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9727 - val_loss: 1.5761\n",
            "108/108 - 7s - loss: 2.0081 - val_loss: 1.5835\n",
            "108/108 - 7s - loss: 2.0120 - val_loss: 1.5267\n",
            "108/108 - 7s - loss: 2.0003 - val_loss: 1.5974\n",
            "108/108 - 7s - loss: 2.0558 - val_loss: 1.6064\n",
            "108/108 - 7s - loss: 2.0368 - val_loss: 1.5975\n",
            "108/108 - 7s - loss: 2.0175 - val_loss: 1.5864\n",
            "108/108 - 7s - loss: 1.9643 - val_loss: 1.5656\n",
            "108/108 - 7s - loss: 2.0120 - val_loss: 1.5717\n",
            "108/108 - 7s - loss: 2.0229 - val_loss: 1.5981\n",
            "108/108 - 7s - loss: 1.9549 - val_loss: 1.5568\n",
            "108/108 - 7s - loss: 1.9808 - val_loss: 1.6483\n",
            "108/108 - 7s - loss: 1.9920 - val_loss: 1.5819\n",
            "108/108 - 7s - loss: 2.0279 - val_loss: 1.6025\n",
            "108/108 - 7s - loss: 1.9841 - val_loss: 1.6151\n",
            "108/108 - 7s - loss: 2.0224 - val_loss: 1.5661\n",
            "108/108 - 7s - loss: 2.0154 - val_loss: 1.5782\n",
            "108/108 - 7s - loss: 2.0046 - val_loss: 1.5622\n",
            "108/108 - 7s - loss: 2.0339 - val_loss: 1.5926\n",
            "108/108 - 7s - loss: 2.0009 - val_loss: 1.6192\n",
            "108/108 - 7s - loss: 1.9814 - val_loss: 1.5708\n",
            "108/108 - 7s - loss: 1.9668 - val_loss: 1.5595\n",
            "108/108 - 7s - loss: 1.9413 - val_loss: 1.5435\n",
            "108/108 - 7s - loss: 1.9224 - val_loss: 1.5583\n",
            "108/108 - 7s - loss: 2.0014 - val_loss: 1.6582\n",
            "108/108 - 7s - loss: 2.0064 - val_loss: 1.5763\n",
            "108/108 - 7s - loss: 1.9703 - val_loss: 1.5914\n",
            "108/108 - 7s - loss: 1.9677 - val_loss: 1.5448\n",
            "108/108 - 7s - loss: 1.9301 - val_loss: 1.5873\n",
            "108/108 - 7s - loss: 1.9828 - val_loss: 1.5297\n",
            "108/108 - 7s - loss: 1.9248 - val_loss: 1.5706\n",
            "108/108 - 7s - loss: 1.9766 - val_loss: 1.5610\n",
            "108/108 - 7s - loss: 2.0380 - val_loss: 1.6170\n",
            "108/108 - 7s - loss: 1.9721 - val_loss: 1.6026\n",
            "108/108 - 7s - loss: 1.9548 - val_loss: 1.5861\n",
            "108/108 - 7s - loss: 1.9739 - val_loss: 1.6612\n",
            "108/108 - 7s - loss: 1.9233 - val_loss: 1.5828\n",
            "108/108 - 7s - loss: 1.9149 - val_loss: 1.5840\n",
            "108/108 - 7s - loss: 1.9285 - val_loss: 1.6410\n",
            "108/108 - 7s - loss: 2.0101 - val_loss: 1.6111\n",
            "108/108 - 7s - loss: 1.9369 - val_loss: 1.7034\n",
            "108/108 - 7s - loss: 1.9240 - val_loss: 1.6407\n",
            "108/108 - 7s - loss: 1.9446 - val_loss: 1.6136\n",
            "108/108 - 7s - loss: 2.0301 - val_loss: 1.5390\n",
            "108/108 - 7s - loss: 1.9521 - val_loss: 1.6819\n",
            "108/108 - 7s - loss: 1.9742 - val_loss: 1.5972\n",
            "108/108 - 7s - loss: 1.9800 - val_loss: 1.6280\n",
            "108/108 - 7s - loss: 1.9666 - val_loss: 1.6286\n",
            "108/108 - 7s - loss: 1.9942 - val_loss: 1.5873\n",
            "108/108 - 7s - loss: 1.9544 - val_loss: 1.6208\n",
            "108/108 - 7s - loss: 1.9437 - val_loss: 1.5845\n",
            "108/108 - 7s - loss: 1.9169 - val_loss: 1.6243\n",
            "108/108 - 7s - loss: 1.9516 - val_loss: 1.6019\n",
            "108/108 - 7s - loss: 1.9249 - val_loss: 1.5465\n",
            "108/108 - 7s - loss: 1.9893 - val_loss: 1.5467\n",
            "108/108 - 7s - loss: 2.0780 - val_loss: 1.5338\n",
            "108/108 - 7s - loss: 2.0213 - val_loss: 1.6261\n",
            "108/108 - 7s - loss: 1.9760 - val_loss: 1.6908\n",
            "108/108 - 7s - loss: 1.9788 - val_loss: 1.5515\n",
            "108/108 - 7s - loss: 1.9791 - val_loss: 1.5847\n",
            "108/108 - 7s - loss: 1.9521 - val_loss: 1.6789\n",
            "108/108 - 7s - loss: 1.9538 - val_loss: 1.6162\n",
            "108/108 - 7s - loss: 1.9476 - val_loss: 1.5869\n",
            "108/108 - 7s - loss: 1.9771 - val_loss: 1.5589\n",
            "108/108 - 7s - loss: 1.9465 - val_loss: 1.5564\n",
            "108/108 - 7s - loss: 1.9728 - val_loss: 1.5553\n",
            "108/108 - 7s - loss: 1.9864 - val_loss: 1.5803\n",
            "108/108 - 7s - loss: 1.9812 - val_loss: 1.5745\n",
            "108/108 - 7s - loss: 1.9432 - val_loss: 1.5908\n",
            "108/108 - 7s - loss: 1.9466 - val_loss: 1.6647\n",
            "108/108 - 7s - loss: 1.9324 - val_loss: 1.5623\n",
            "108/108 - 7s - loss: 1.9483 - val_loss: 1.5743\n",
            "108/108 - 7s - loss: 1.9973 - val_loss: 1.6004\n",
            "108/108 - 7s - loss: 1.9193 - val_loss: 1.5586\n",
            "108/108 - 7s - loss: 1.9018 - val_loss: 1.5724\n",
            "108/108 - 7s - loss: 1.9281 - val_loss: 1.5990\n",
            "108/108 - 7s - loss: 1.9863 - val_loss: 1.5403\n",
            "108/108 - 7s - loss: 1.9890 - val_loss: 1.6526\n",
            "108/108 - 7s - loss: 1.9507 - val_loss: 1.6039\n",
            "108/108 - 7s - loss: 2.0201 - val_loss: 1.5563\n",
            "108/108 - 7s - loss: 1.9274 - val_loss: 1.5663\n",
            "108/108 - 7s - loss: 1.9433 - val_loss: 1.6250\n",
            "108/108 - 7s - loss: 1.9757 - val_loss: 1.6602\n",
            "108/108 - 7s - loss: 1.9404 - val_loss: 1.6758\n",
            "108/108 - 7s - loss: 1.8503 - val_loss: 1.6276\n",
            "108/108 - 7s - loss: 1.9039 - val_loss: 1.6585\n",
            "108/108 - 7s - loss: 1.9014 - val_loss: 1.6038\n",
            "108/108 - 7s - loss: 1.9491 - val_loss: 1.6442\n",
            "108/108 - 7s - loss: 1.9526 - val_loss: 1.6657\n",
            "108/108 - 7s - loss: 1.9353 - val_loss: 1.6888\n",
            "108/108 - 7s - loss: 1.9743 - val_loss: 1.7227\n",
            "108/108 - 7s - loss: 1.9317 - val_loss: 1.5941\n",
            "108/108 - 7s - loss: 1.9863 - val_loss: 1.6595\n",
            "108/108 - 7s - loss: 1.9518 - val_loss: 1.6789\n",
            "108/108 - 7s - loss: 1.9144 - val_loss: 1.6712\n",
            "108/108 - 7s - loss: 1.9911 - val_loss: 1.6637\n",
            "108/108 - 7s - loss: 1.9897 - val_loss: 1.6352\n",
            "108/108 - 7s - loss: 1.9669 - val_loss: 1.6065\n",
            "108/108 - 7s - loss: 1.9205 - val_loss: 1.6326\n",
            "108/108 - 7s - loss: 1.9753 - val_loss: 1.6408\n",
            "108/108 - 7s - loss: 1.9219 - val_loss: 1.5981\n",
            "108/108 - 7s - loss: 1.9731 - val_loss: 1.5830\n",
            "108/108 - 7s - loss: 1.9268 - val_loss: 1.6054\n",
            "108/108 - 7s - loss: 1.9207 - val_loss: 1.6688\n",
            "108/108 - 7s - loss: 1.9432 - val_loss: 1.6030\n",
            "108/108 - 7s - loss: 1.9104 - val_loss: 1.6510\n",
            "108/108 - 7s - loss: 1.9095 - val_loss: 1.6231\n",
            "108/108 - 7s - loss: 1.8724 - val_loss: 1.6209\n",
            "108/108 - 7s - loss: 1.9257 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.9351 - val_loss: 1.5583\n",
            "108/108 - 7s - loss: 1.8991 - val_loss: 1.6279\n",
            "108/108 - 7s - loss: 1.8628 - val_loss: 1.5783\n",
            "108/108 - 7s - loss: 1.9558 - val_loss: 1.8052\n",
            "108/108 - 7s - loss: 1.9218 - val_loss: 1.6221\n",
            "108/108 - 7s - loss: 1.9138 - val_loss: 1.5900\n",
            "108/108 - 7s - loss: 1.9158 - val_loss: 1.6566\n",
            "108/108 - 7s - loss: 1.9473 - val_loss: 1.7154\n",
            "108/108 - 7s - loss: 1.8905 - val_loss: 1.6625\n",
            "108/108 - 7s - loss: 1.9357 - val_loss: 1.6359\n",
            "108/108 - 7s - loss: 1.9274 - val_loss: 1.6257\n",
            "108/108 - 7s - loss: 1.9020 - val_loss: 1.5807\n",
            "108/108 - 7s - loss: 1.9394 - val_loss: 1.5946\n",
            "108/108 - 7s - loss: 1.8987 - val_loss: 1.6713\n",
            "108/108 - 7s - loss: 1.8906 - val_loss: 1.6987\n",
            "108/108 - 7s - loss: 1.8932 - val_loss: 1.6408\n",
            "108/108 - 7s - loss: 1.9478 - val_loss: 1.6399\n",
            "108/108 - 7s - loss: 1.9273 - val_loss: 1.6326\n",
            "108/108 - 7s - loss: 1.8992 - val_loss: 1.8225\n",
            "108/108 - 7s - loss: 1.8804 - val_loss: 1.6354\n",
            "108/108 - 7s - loss: 1.9139 - val_loss: 1.6349\n",
            "108/108 - 7s - loss: 1.9240 - val_loss: 1.6383\n",
            "108/108 - 7s - loss: 1.8590 - val_loss: 1.6385\n",
            "108/108 - 7s - loss: 1.9407 - val_loss: 1.6446\n",
            "108/108 - 7s - loss: 1.9422 - val_loss: 1.7439\n",
            "108/108 - 7s - loss: 1.9242 - val_loss: 1.6503\n",
            "108/108 - 7s - loss: 1.9206 - val_loss: 1.5654\n",
            "108/108 - 7s - loss: 1.9210 - val_loss: 1.6076\n",
            "108/108 - 7s - loss: 1.8633 - val_loss: 1.6272\n",
            "108/108 - 7s - loss: 1.9453 - val_loss: 1.6139\n",
            "108/108 - 7s - loss: 1.8989 - val_loss: 1.6295\n",
            "108/108 - 7s - loss: 1.9072 - val_loss: 1.7284\n",
            "108/108 - 7s - loss: 1.8695 - val_loss: 1.6572\n",
            "108/108 - 7s - loss: 1.9615 - val_loss: 1.6252\n",
            "108/108 - 7s - loss: 1.9097 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9489 - val_loss: 1.6483\n",
            "108/108 - 7s - loss: 1.9521 - val_loss: 1.6264\n",
            "108/108 - 7s - loss: 1.9012 - val_loss: 1.6061\n",
            "108/108 - 7s - loss: 1.9082 - val_loss: 1.6548\n",
            "108/108 - 7s - loss: 1.9181 - val_loss: 1.6666\n",
            "108/108 - 7s - loss: 1.9429 - val_loss: 1.7122\n",
            "108/108 - 7s - loss: 1.9468 - val_loss: 1.6401\n",
            "108/108 - 7s - loss: 1.9080 - val_loss: 1.6660\n",
            "108/108 - 7s - loss: 1.9213 - val_loss: 1.6567\n",
            "108/108 - 7s - loss: 1.9018 - val_loss: 1.6457\n",
            "108/108 - 7s - loss: 1.8725 - val_loss: 1.6774\n",
            "108/108 - 7s - loss: 1.9128 - val_loss: 1.8481\n",
            "108/108 - 7s - loss: 1.9009 - val_loss: 1.5979\n",
            "108/108 - 7s - loss: 1.8966 - val_loss: 1.6994\n",
            "108/108 - 7s - loss: 1.9011 - val_loss: 1.6222\n",
            "108/108 - 7s - loss: 1.8437 - val_loss: 1.6298\n",
            "108/108 - 7s - loss: 1.8976 - val_loss: 1.6656\n",
            "108/108 - 7s - loss: 1.8873 - val_loss: 1.6356\n",
            "108/108 - 7s - loss: 1.8983 - val_loss: 1.6597\n",
            "108/108 - 7s - loss: 1.8738 - val_loss: 1.5872\n",
            "108/108 - 7s - loss: 1.9114 - val_loss: 1.6840\n",
            "108/108 - 7s - loss: 1.8887 - val_loss: 1.6577\n",
            "108/108 - 7s - loss: 1.8999 - val_loss: 1.6126\n",
            "108/108 - 7s - loss: 1.8675 - val_loss: 1.6067\n",
            "108/108 - 7s - loss: 1.8267 - val_loss: 1.6233\n",
            "108/108 - 7s - loss: 1.8877 - val_loss: 1.5858\n",
            "108/108 - 7s - loss: 1.8533 - val_loss: 1.6021\n",
            "108/108 - 7s - loss: 1.8285 - val_loss: 1.5459\n",
            "108/108 - 7s - loss: 1.8755 - val_loss: 1.6344\n",
            "108/108 - 7s - loss: 1.8623 - val_loss: 1.6156\n",
            "108/108 - 7s - loss: 1.8728 - val_loss: 1.5874\n",
            "108/108 - 7s - loss: 1.8365 - val_loss: 1.5939\n",
            "108/108 - 7s - loss: 1.8654 - val_loss: 1.6498\n",
            "108/108 - 7s - loss: 1.9238 - val_loss: 1.6943\n",
            "108/108 - 7s - loss: 1.8841 - val_loss: 1.6977\n",
            "108/108 - 7s - loss: 1.8773 - val_loss: 1.7743\n",
            "108/108 - 7s - loss: 1.8928 - val_loss: 1.7165\n",
            "108/108 - 7s - loss: 1.8644 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8789 - val_loss: 1.6376\n",
            "108/108 - 7s - loss: 1.8963 - val_loss: 1.5840\n",
            "108/108 - 7s - loss: 1.8665 - val_loss: 1.5496\n",
            "108/108 - 7s - loss: 1.8583 - val_loss: 1.6902\n",
            "0.9\n",
            "108/108 - 14s - loss: 15.5697 - val_loss: 15.6274\n",
            "108/108 - 7s - loss: 14.7348 - val_loss: 14.8762\n",
            "108/108 - 7s - loss: 14.0123 - val_loss: 14.1892\n",
            "108/108 - 7s - loss: 13.3674 - val_loss: 13.5478\n",
            "108/108 - 7s - loss: 12.7490 - val_loss: 12.9391\n",
            "108/108 - 7s - loss: 12.1387 - val_loss: 12.3594\n",
            "108/108 - 7s - loss: 11.6073 - val_loss: 11.8084\n",
            "108/108 - 7s - loss: 11.0351 - val_loss: 11.2786\n",
            "108/108 - 7s - loss: 10.5981 - val_loss: 10.7767\n",
            "108/108 - 7s - loss: 10.0936 - val_loss: 10.2950\n",
            "108/108 - 7s - loss: 9.5991 - val_loss: 9.8286\n",
            "108/108 - 7s - loss: 9.1615 - val_loss: 9.3835\n",
            "108/108 - 7s - loss: 8.7869 - val_loss: 8.9567\n",
            "108/108 - 7s - loss: 8.4028 - val_loss: 8.5496\n",
            "108/108 - 7s - loss: 7.9972 - val_loss: 8.1573\n",
            "108/108 - 7s - loss: 7.6392 - val_loss: 7.7829\n",
            "108/108 - 7s - loss: 7.2613 - val_loss: 7.4225\n",
            "108/108 - 7s - loss: 6.8860 - val_loss: 7.0758\n",
            "108/108 - 7s - loss: 6.6293 - val_loss: 6.7461\n",
            "108/108 - 7s - loss: 6.3466 - val_loss: 6.4301\n",
            "108/108 - 7s - loss: 6.0149 - val_loss: 6.1274\n",
            "108/108 - 7s - loss: 5.7762 - val_loss: 5.8431\n",
            "108/108 - 7s - loss: 5.4377 - val_loss: 5.5681\n",
            "108/108 - 7s - loss: 5.2011 - val_loss: 5.3041\n",
            "108/108 - 7s - loss: 5.0006 - val_loss: 5.0564\n",
            "108/108 - 7s - loss: 4.8206 - val_loss: 4.8239\n",
            "108/108 - 7s - loss: 4.6189 - val_loss: 4.6068\n",
            "108/108 - 7s - loss: 4.4058 - val_loss: 4.4007\n",
            "108/108 - 7s - loss: 4.1772 - val_loss: 4.2067\n",
            "108/108 - 7s - loss: 4.0210 - val_loss: 4.0245\n",
            "108/108 - 7s - loss: 3.9369 - val_loss: 3.8546\n",
            "108/108 - 7s - loss: 3.7583 - val_loss: 3.6939\n",
            "108/108 - 7s - loss: 3.5727 - val_loss: 3.5403\n",
            "108/108 - 7s - loss: 3.5328 - val_loss: 3.3985\n",
            "108/108 - 7s - loss: 3.3965 - val_loss: 3.2671\n",
            "108/108 - 7s - loss: 3.1590 - val_loss: 3.1435\n",
            "108/108 - 7s - loss: 3.1172 - val_loss: 3.0240\n",
            "108/108 - 7s - loss: 3.1292 - val_loss: 2.9139\n",
            "108/108 - 7s - loss: 2.8737 - val_loss: 2.8101\n",
            "108/108 - 7s - loss: 2.7944 - val_loss: 2.7097\n",
            "108/108 - 7s - loss: 2.7857 - val_loss: 2.6154\n",
            "108/108 - 7s - loss: 2.6689 - val_loss: 2.5272\n",
            "108/108 - 7s - loss: 2.6040 - val_loss: 2.4451\n",
            "108/108 - 7s - loss: 2.5457 - val_loss: 2.3666\n",
            "108/108 - 7s - loss: 2.5379 - val_loss: 2.2940\n",
            "108/108 - 7s - loss: 2.4253 - val_loss: 2.2257\n",
            "108/108 - 7s - loss: 2.3286 - val_loss: 2.1632\n",
            "108/108 - 7s - loss: 2.3264 - val_loss: 2.1113\n",
            "108/108 - 7s - loss: 2.2669 - val_loss: 2.0578\n",
            "108/108 - 7s - loss: 2.1786 - val_loss: 2.0085\n",
            "108/108 - 7s - loss: 2.1894 - val_loss: 1.9639\n",
            "108/108 - 7s - loss: 2.1665 - val_loss: 1.9215\n",
            "108/108 - 7s - loss: 2.0514 - val_loss: 1.8833\n",
            "108/108 - 7s - loss: 2.1027 - val_loss: 1.8507\n",
            "108/108 - 7s - loss: 2.1244 - val_loss: 1.8194\n",
            "108/108 - 7s - loss: 2.0661 - val_loss: 1.7941\n",
            "108/108 - 7s - loss: 2.0975 - val_loss: 1.7684\n",
            "108/108 - 7s - loss: 2.0427 - val_loss: 1.7446\n",
            "108/108 - 7s - loss: 2.0416 - val_loss: 1.7228\n",
            "108/108 - 7s - loss: 2.0360 - val_loss: 1.7035\n",
            "108/108 - 7s - loss: 2.0032 - val_loss: 1.6891\n",
            "108/108 - 7s - loss: 1.9638 - val_loss: 1.6767\n",
            "108/108 - 7s - loss: 1.9989 - val_loss: 1.6638\n",
            "108/108 - 7s - loss: 1.9743 - val_loss: 1.6525\n",
            "108/108 - 7s - loss: 2.0092 - val_loss: 1.6412\n",
            "108/108 - 7s - loss: 1.9935 - val_loss: 1.6353\n",
            "108/108 - 7s - loss: 1.9010 - val_loss: 1.6283\n",
            "108/108 - 7s - loss: 1.9412 - val_loss: 1.6226\n",
            "108/108 - 7s - loss: 1.8779 - val_loss: 1.6179\n",
            "108/108 - 7s - loss: 1.9396 - val_loss: 1.6141\n",
            "108/108 - 7s - loss: 1.8571 - val_loss: 1.6116\n",
            "108/108 - 7s - loss: 1.9435 - val_loss: 1.6093\n",
            "108/108 - 7s - loss: 1.9295 - val_loss: 1.6059\n",
            "108/108 - 7s - loss: 1.9459 - val_loss: 1.6028\n",
            "108/108 - 7s - loss: 1.9962 - val_loss: 1.6002\n",
            "108/108 - 7s - loss: 1.8747 - val_loss: 1.5998\n",
            "108/108 - 7s - loss: 1.8910 - val_loss: 1.5985\n",
            "108/108 - 7s - loss: 1.9178 - val_loss: 1.5973\n",
            "108/108 - 7s - loss: 1.8701 - val_loss: 1.5972\n",
            "108/108 - 7s - loss: 1.9233 - val_loss: 1.5966\n",
            "108/108 - 7s - loss: 1.9840 - val_loss: 1.5956\n",
            "108/108 - 7s - loss: 1.9284 - val_loss: 1.5951\n",
            "108/108 - 7s - loss: 1.9000 - val_loss: 1.5945\n",
            "108/108 - 7s - loss: 1.8782 - val_loss: 1.5941\n",
            "108/108 - 7s - loss: 1.8585 - val_loss: 1.5945\n",
            "108/108 - 7s - loss: 1.9381 - val_loss: 1.5944\n",
            "108/108 - 7s - loss: 1.9074 - val_loss: 1.5941\n",
            "108/108 - 7s - loss: 1.9286 - val_loss: 1.5942\n",
            "108/108 - 7s - loss: 1.8936 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.8508 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9190 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9153 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.9240 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9037 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9347 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.9324 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.8995 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 1.8830 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.9383 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.9041 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9039 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.8783 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.8580 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8938 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.8934 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.9161 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.9341 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8793 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.9093 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.9091 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.9275 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8832 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.8921 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.9279 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.9233 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9402 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9717 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8479 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.9083 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.8961 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.8949 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.8888 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8806 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9309 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.8902 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.9366 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.8681 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9236 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.8996 - val_loss: 1.5929\n",
            "108/108 - 7s - loss: 1.9012 - val_loss: 1.5930\n",
            "108/108 - 7s - loss: 1.9272 - val_loss: 1.5930\n",
            "108/108 - 7s - loss: 1.8853 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9013 - val_loss: 1.5929\n",
            "108/108 - 7s - loss: 1.9308 - val_loss: 1.5929\n",
            "108/108 - 7s - loss: 1.9236 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.8995 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.8839 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.8693 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9201 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.9239 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9054 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 2.0001 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 1.8545 - val_loss: 1.5941\n",
            "108/108 - 7s - loss: 1.8789 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9346 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.8796 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.8993 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9080 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.8964 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9132 - val_loss: 1.5939\n",
            "108/108 - 7s - loss: 1.8445 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.8815 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.8810 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.8650 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8915 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.8951 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 1.8912 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.8689 - val_loss: 1.5939\n",
            "108/108 - 7s - loss: 1.8433 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 1.9133 - val_loss: 1.5943\n",
            "108/108 - 7s - loss: 1.8855 - val_loss: 1.5942\n",
            "108/108 - 7s - loss: 1.8930 - val_loss: 1.5946\n",
            "108/108 - 7s - loss: 1.8744 - val_loss: 1.5942\n",
            "108/108 - 7s - loss: 1.8911 - val_loss: 1.5941\n",
            "108/108 - 7s - loss: 1.9487 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.8784 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.9782 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9294 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.8726 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9556 - val_loss: 1.6288\n",
            "108/108 - 7s - loss: 1.9092 - val_loss: 1.6193\n",
            "108/108 - 7s - loss: 1.9664 - val_loss: 1.6119\n",
            "108/108 - 7s - loss: 1.9331 - val_loss: 1.6067\n",
            "108/108 - 7s - loss: 1.9234 - val_loss: 1.6024\n",
            "108/108 - 7s - loss: 1.9427 - val_loss: 1.5982\n",
            "108/108 - 7s - loss: 1.9211 - val_loss: 1.5924\n",
            "108/108 - 7s - loss: 1.9325 - val_loss: 1.6032\n",
            "108/108 - 7s - loss: 1.8843 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.9022 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 1.9205 - val_loss: 1.5952\n",
            "108/108 - 7s - loss: 1.8570 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 1.8942 - val_loss: 1.5945\n",
            "108/108 - 7s - loss: 1.9485 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 1.9536 - val_loss: 1.5948\n",
            "108/108 - 7s - loss: 1.8935 - val_loss: 1.5962\n",
            "108/108 - 7s - loss: 1.8813 - val_loss: 1.5981\n",
            "108/108 - 7s - loss: 1.9298 - val_loss: 1.5944\n",
            "108/108 - 7s - loss: 1.8551 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.8954 - val_loss: 1.5940\n",
            "108/108 - 7s - loss: 1.8681 - val_loss: 1.5943\n",
            "108/108 - 7s - loss: 1.9642 - val_loss: 1.5939\n",
            "108/108 - 7s - loss: 1.9371 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9102 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.8881 - val_loss: 1.5948\n",
            "108/108 - 7s - loss: 1.8932 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.8709 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.8716 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9096 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8796 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8622 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8803 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.8739 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.8696 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.8846 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.9799 - val_loss: 1.7328\n",
            "108/108 - 7s - loss: 2.1286 - val_loss: 1.6982\n",
            "108/108 - 7s - loss: 1.9592 - val_loss: 1.6368\n",
            "108/108 - 7s - loss: 1.9335 - val_loss: 1.5948\n",
            "108/108 - 7s - loss: 1.9061 - val_loss: 1.5942\n",
            "108/108 - 7s - loss: 1.9087 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9009 - val_loss: 1.5929\n",
            "108/108 - 7s - loss: 1.8728 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.9453 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.9130 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8743 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9155 - val_loss: 1.5937\n",
            "108/108 - 7s - loss: 1.9096 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.8430 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9115 - val_loss: 1.5938\n",
            "108/108 - 7s - loss: 1.8623 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9027 - val_loss: 1.5935\n",
            "108/108 - 7s - loss: 1.9100 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.9217 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.9248 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.9263 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.8756 - val_loss: 1.5930\n",
            "108/108 - 7s - loss: 1.9021 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.8913 - val_loss: 1.5936\n",
            "108/108 - 7s - loss: 1.9433 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.9504 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.8622 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.8591 - val_loss: 1.5930\n",
            "108/108 - 7s - loss: 1.9071 - val_loss: 1.5930\n",
            "108/108 - 7s - loss: 1.8922 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.8988 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.9373 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.8960 - val_loss: 1.5931\n",
            "108/108 - 7s - loss: 1.8817 - val_loss: 1.5932\n",
            "108/108 - 7s - loss: 1.8944 - val_loss: 1.5934\n",
            "108/108 - 7s - loss: 1.8896 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.9119 - val_loss: 1.5933\n",
            "108/108 - 7s - loss: 1.9534 - val_loss: 1.5929\n",
            "108/108 - 7s - loss: 1.8712 - val_loss: 1.5928\n",
            "108/108 - 7s - loss: 1.8855 - val_loss: 1.5905\n",
            "108/108 - 7s - loss: 1.7692 - val_loss: 1.3983\n",
            "108/108 - 7s - loss: 1.7036 - val_loss: 1.3658\n",
            "108/108 - 7s - loss: 1.6653 - val_loss: 1.3408\n",
            "108/108 - 7s - loss: 1.7141 - val_loss: 1.2365\n",
            "108/108 - 7s - loss: 1.5853 - val_loss: 1.1809\n",
            "108/108 - 7s - loss: 1.5284 - val_loss: 1.1668\n",
            "108/108 - 7s - loss: 1.6011 - val_loss: 1.1956\n",
            "108/108 - 7s - loss: 1.5503 - val_loss: 1.1013\n",
            "108/108 - 7s - loss: 1.5826 - val_loss: 1.1056\n",
            "108/108 - 7s - loss: 1.5365 - val_loss: 1.0936\n",
            "108/108 - 7s - loss: 1.5197 - val_loss: 1.1260\n",
            "108/108 - 7s - loss: 1.5197 - val_loss: 1.1058\n",
            "108/108 - 7s - loss: 1.4871 - val_loss: 1.0965\n",
            "108/108 - 7s - loss: 1.5086 - val_loss: 1.1052\n",
            "108/108 - 7s - loss: 1.4776 - val_loss: 1.1489\n",
            "108/108 - 7s - loss: 1.4969 - val_loss: 1.1149\n",
            "108/108 - 7s - loss: 1.4538 - val_loss: 1.1545\n",
            "108/108 - 7s - loss: 1.4925 - val_loss: 1.1281\n",
            "108/108 - 7s - loss: 1.4949 - val_loss: 1.0954\n",
            "108/108 - 7s - loss: 1.4791 - val_loss: 1.1199\n",
            "108/108 - 7s - loss: 1.5010 - val_loss: 1.1424\n",
            "108/108 - 7s - loss: 1.5356 - val_loss: 1.1217\n",
            "108/108 - 7s - loss: 1.4460 - val_loss: 1.0938\n",
            "108/108 - 7s - loss: 1.4993 - val_loss: 1.1084\n",
            "108/108 - 7s - loss: 1.5222 - val_loss: 1.0816\n",
            "108/108 - 7s - loss: 1.4631 - val_loss: 1.0972\n",
            "108/108 - 7s - loss: 1.5003 - val_loss: 1.0724\n",
            "108/108 - 7s - loss: 1.4237 - val_loss: 1.1082\n",
            "108/108 - 7s - loss: 1.4445 - val_loss: 1.1236\n",
            "108/108 - 7s - loss: 1.4308 - val_loss: 1.1608\n",
            "108/108 - 7s - loss: 1.4425 - val_loss: 1.0884\n",
            "108/108 - 7s - loss: 1.5131 - val_loss: 1.0995\n",
            "108/108 - 7s - loss: 1.4872 - val_loss: 1.1531\n",
            "108/108 - 7s - loss: 1.4218 - val_loss: 1.0732\n",
            "108/108 - 7s - loss: 1.4514 - val_loss: 1.1417\n",
            "108/108 - 7s - loss: 1.4588 - val_loss: 1.0760\n",
            "108/108 - 7s - loss: 1.4100 - val_loss: 1.0940\n",
            "108/108 - 7s - loss: 1.4722 - val_loss: 1.1511\n",
            "108/108 - 7s - loss: 1.4750 - val_loss: 1.1329\n",
            "108/108 - 7s - loss: 1.4080 - val_loss: 1.0564\n",
            "108/108 - 7s - loss: 1.4266 - val_loss: 1.0951\n",
            "108/108 - 7s - loss: 1.3807 - val_loss: 1.1141\n",
            "108/108 - 7s - loss: 1.4669 - val_loss: 1.0966\n",
            "108/108 - 7s - loss: 1.3985 - val_loss: 1.0449\n",
            "108/108 - 7s - loss: 1.3964 - val_loss: 1.0546\n",
            "108/108 - 7s - loss: 1.4554 - val_loss: 1.0762\n",
            "108/108 - 7s - loss: 1.4263 - val_loss: 1.0970\n",
            "108/108 - 7s - loss: 1.3723 - val_loss: 1.0460\n",
            "108/108 - 7s - loss: 1.4210 - val_loss: 1.1313\n",
            "108/108 - 7s - loss: 1.4014 - val_loss: 1.0924\n",
            "108/108 - 7s - loss: 1.3662 - val_loss: 1.0750\n",
            "108/108 - 7s - loss: 1.3850 - val_loss: 1.0760\n",
            "108/108 - 7s - loss: 1.3808 - val_loss: 1.1006\n",
            "108/108 - 7s - loss: 1.4462 - val_loss: 1.0480\n",
            "108/108 - 7s - loss: 1.3753 - val_loss: 1.1128\n",
            "108/108 - 7s - loss: 1.3899 - val_loss: 1.0896\n",
            "108/108 - 7s - loss: 1.4224 - val_loss: 1.0539\n",
            "108/108 - 7s - loss: 1.3538 - val_loss: 1.0513\n",
            "108/108 - 7s - loss: 1.3726 - val_loss: 1.0409\n",
            "108/108 - 7s - loss: 1.4268 - val_loss: 1.0456\n",
            "108/108 - 7s - loss: 1.3682 - val_loss: 1.0345\n",
            "108/108 - 7s - loss: 1.3792 - val_loss: 1.0467\n",
            "108/108 - 7s - loss: 1.4237 - val_loss: 1.0655\n",
            "108/108 - 7s - loss: 1.3846 - val_loss: 1.0258\n",
            "108/108 - 7s - loss: 1.3927 - val_loss: 1.0335\n",
            "108/108 - 7s - loss: 1.4091 - val_loss: 1.0564\n",
            "108/108 - 7s - loss: 1.3582 - val_loss: 1.0905\n",
            "108/108 - 7s - loss: 1.3884 - val_loss: 1.0371\n",
            "108/108 - 7s - loss: 1.3705 - val_loss: 1.0263\n",
            "108/108 - 7s - loss: 1.3882 - val_loss: 1.1024\n",
            "108/108 - 7s - loss: 1.3898 - val_loss: 1.1004\n",
            "108/108 - 7s - loss: 1.4057 - val_loss: 1.0361\n",
            "108/108 - 7s - loss: 1.3528 - val_loss: 1.0520\n",
            "108/108 - 7s - loss: 1.3594 - val_loss: 1.0256\n",
            "108/108 - 7s - loss: 1.3877 - val_loss: 1.0180\n",
            "108/108 - 7s - loss: 1.3736 - val_loss: 1.0095\n",
            "108/108 - 7s - loss: 1.3535 - val_loss: 1.0789\n",
            "108/108 - 7s - loss: 1.3490 - val_loss: 1.0670\n",
            "108/108 - 7s - loss: 1.3185 - val_loss: 1.0067\n",
            "108/108 - 7s - loss: 1.3463 - val_loss: 0.9880\n",
            "108/108 - 7s - loss: 1.3664 - val_loss: 0.9862\n",
            "108/108 - 7s - loss: 1.3578 - val_loss: 1.0007\n",
            "108/108 - 7s - loss: 1.4224 - val_loss: 1.0397\n",
            "108/108 - 7s - loss: 1.3868 - val_loss: 1.0305\n",
            "108/108 - 7s - loss: 1.3559 - val_loss: 1.0403\n",
            "108/108 - 7s - loss: 1.3578 - val_loss: 1.0575\n",
            "108/108 - 7s - loss: 1.3916 - val_loss: 0.9903\n",
            "108/108 - 7s - loss: 1.3886 - val_loss: 1.0258\n",
            "108/108 - 7s - loss: 1.3522 - val_loss: 1.0440\n",
            "108/108 - 7s - loss: 1.3439 - val_loss: 1.0010\n",
            "108/108 - 7s - loss: 1.3218 - val_loss: 0.9985\n",
            "108/108 - 7s - loss: 1.3944 - val_loss: 1.0380\n",
            "108/108 - 7s - loss: 1.3474 - val_loss: 0.9960\n",
            "108/108 - 7s - loss: 1.3113 - val_loss: 1.0127\n",
            "108/108 - 7s - loss: 1.4075 - val_loss: 1.0514\n",
            "108/108 - 7s - loss: 1.3152 - val_loss: 0.9865\n",
            "108/108 - 7s - loss: 1.3636 - val_loss: 1.0121\n",
            "108/108 - 7s - loss: 1.3656 - val_loss: 1.0511\n",
            "108/108 - 7s - loss: 1.3583 - val_loss: 1.0730\n",
            "108/108 - 7s - loss: 1.3350 - val_loss: 1.0179\n",
            "108/108 - 7s - loss: 1.3573 - val_loss: 1.0449\n",
            "108/108 - 7s - loss: 1.3797 - val_loss: 1.0791\n",
            "108/108 - 7s - loss: 1.3380 - val_loss: 1.0136\n",
            "108/108 - 7s - loss: 1.3366 - val_loss: 1.0812\n",
            "108/108 - 7s - loss: 1.3793 - val_loss: 1.0199\n",
            "108/108 - 7s - loss: 1.2678 - val_loss: 0.9961\n",
            "108/108 - 7s - loss: 1.3948 - val_loss: 1.0512\n",
            "108/108 - 7s - loss: 1.3567 - val_loss: 1.0089\n",
            "108/108 - 7s - loss: 1.3227 - val_loss: 0.9999\n",
            "108/108 - 7s - loss: 1.3374 - val_loss: 0.9933\n",
            "108/108 - 7s - loss: 1.3446 - val_loss: 1.0099\n",
            "108/108 - 7s - loss: 1.3370 - val_loss: 1.0136\n",
            "108/108 - 7s - loss: 1.3457 - val_loss: 1.0154\n",
            "108/108 - 7s - loss: 1.3216 - val_loss: 1.0361\n",
            "108/108 - 7s - loss: 1.3731 - val_loss: 1.0757\n",
            "108/108 - 7s - loss: 1.3414 - val_loss: 1.1098\n",
            "108/108 - 7s - loss: 1.3508 - val_loss: 1.1043\n",
            "108/108 - 7s - loss: 1.2906 - val_loss: 1.0511\n",
            "108/108 - 7s - loss: 1.3306 - val_loss: 1.0258\n",
            "108/108 - 7s - loss: 1.3195 - val_loss: 1.0609\n",
            "108/108 - 7s - loss: 1.3285 - val_loss: 1.0356\n",
            "108/108 - 7s - loss: 1.3022 - val_loss: 1.0281\n",
            "108/108 - 7s - loss: 1.3198 - val_loss: 1.0831\n",
            "108/108 - 7s - loss: 1.2661 - val_loss: 1.0136\n",
            "108/108 - 7s - loss: 1.3068 - val_loss: 0.9815\n",
            "108/108 - 7s - loss: 1.2719 - val_loss: 1.0359\n",
            "108/108 - 7s - loss: 1.3265 - val_loss: 1.0497\n",
            "108/108 - 7s - loss: 1.2782 - val_loss: 1.0726\n",
            "108/108 - 7s - loss: 1.3232 - val_loss: 1.0230\n",
            "108/108 - 7s - loss: 1.3285 - val_loss: 1.0025\n",
            "108/108 - 7s - loss: 1.2846 - val_loss: 1.0284\n",
            "108/108 - 7s - loss: 1.2984 - val_loss: 0.9561\n",
            "108/108 - 7s - loss: 1.3061 - val_loss: 1.0129\n",
            "108/108 - 7s - loss: 1.3132 - val_loss: 0.9771\n",
            "108/108 - 7s - loss: 1.2809 - val_loss: 1.0079\n",
            "108/108 - 7s - loss: 1.2989 - val_loss: 1.0240\n",
            "108/108 - 7s - loss: 1.2823 - val_loss: 0.9956\n",
            "108/108 - 7s - loss: 1.2879 - val_loss: 0.9828\n",
            "108/108 - 7s - loss: 1.3319 - val_loss: 0.9925\n",
            "108/108 - 7s - loss: 1.3368 - val_loss: 1.0174\n",
            "108/108 - 7s - loss: 1.3139 - val_loss: 1.0152\n",
            "108/108 - 7s - loss: 1.2986 - val_loss: 0.9951\n",
            "108/108 - 7s - loss: 1.2587 - val_loss: 0.9582\n",
            "108/108 - 7s - loss: 1.2744 - val_loss: 0.9831\n",
            "108/108 - 7s - loss: 1.2961 - val_loss: 1.0149\n",
            "108/108 - 7s - loss: 1.2770 - val_loss: 1.0213\n",
            "108/108 - 7s - loss: 1.3238 - val_loss: 0.9845\n",
            "108/108 - 7s - loss: 1.2675 - val_loss: 1.0483\n",
            "108/108 - 7s - loss: 1.3032 - val_loss: 1.0474\n",
            "108/108 - 7s - loss: 1.3004 - val_loss: 0.9740\n",
            "108/108 - 7s - loss: 1.2721 - val_loss: 0.9904\n",
            "108/108 - 7s - loss: 1.2959 - val_loss: 0.9716\n",
            "108/108 - 7s - loss: 1.3163 - val_loss: 1.0558\n",
            "108/108 - 7s - loss: 1.3182 - val_loss: 0.9701\n",
            "108/108 - 7s - loss: 1.2656 - val_loss: 0.9868\n",
            "108/108 - 7s - loss: 1.2298 - val_loss: 0.9490\n",
            "108/108 - 7s - loss: 1.2580 - val_loss: 0.9557\n",
            "108/108 - 7s - loss: 1.2390 - val_loss: 0.9203\n",
            "108/108 - 7s - loss: 1.2401 - val_loss: 0.9148\n",
            "108/108 - 7s - loss: 1.3231 - val_loss: 1.0090\n",
            "108/108 - 7s - loss: 1.2570 - val_loss: 0.9648\n",
            "108/108 - 7s - loss: 1.3179 - val_loss: 1.0139\n",
            "108/108 - 7s - loss: 1.2694 - val_loss: 0.9758\n",
            "108/108 - 7s - loss: 1.2767 - val_loss: 0.9764\n",
            "108/108 - 7s - loss: 1.2479 - val_loss: 0.9414\n",
            "108/108 - 7s - loss: 1.2797 - val_loss: 0.9526\n",
            "108/108 - 7s - loss: 1.2844 - val_loss: 1.0201\n",
            "108/108 - 7s - loss: 1.3143 - val_loss: 0.9901\n",
            "108/108 - 7s - loss: 1.2591 - val_loss: 0.9649\n",
            "108/108 - 7s - loss: 1.2844 - val_loss: 1.0367\n",
            "108/108 - 7s - loss: 1.2760 - val_loss: 1.0728\n",
            "108/108 - 7s - loss: 1.2473 - val_loss: 0.9799\n",
            "108/108 - 7s - loss: 1.2542 - val_loss: 0.9744\n",
            "108/108 - 7s - loss: 1.2835 - val_loss: 1.0013\n",
            "108/108 - 7s - loss: 1.2865 - val_loss: 0.9604\n",
            "108/108 - 7s - loss: 1.2402 - val_loss: 0.9450\n",
            "108/108 - 7s - loss: 1.2760 - val_loss: 0.9870\n",
            "108/108 - 7s - loss: 1.2390 - val_loss: 1.0309\n",
            "108/108 - 7s - loss: 1.2554 - val_loss: 0.9929\n",
            "108/108 - 7s - loss: 1.2292 - val_loss: 0.9379\n",
            "108/108 - 7s - loss: 1.2936 - val_loss: 0.9993\n",
            "108/108 - 7s - loss: 1.2505 - val_loss: 0.9507\n",
            "108/108 - 7s - loss: 1.2379 - val_loss: 0.9928\n",
            "108/108 - 7s - loss: 1.2712 - val_loss: 0.9647\n",
            "108/108 - 7s - loss: 1.3038 - val_loss: 1.0350\n",
            "108/108 - 7s - loss: 1.2578 - val_loss: 0.9485\n",
            "108/108 - 7s - loss: 1.2052 - val_loss: 0.9378\n",
            "108/108 - 7s - loss: 1.2618 - val_loss: 0.9996\n",
            "108/108 - 7s - loss: 1.2658 - val_loss: 0.9958\n",
            "108/108 - 7s - loss: 1.2773 - val_loss: 0.9736\n",
            "108/108 - 7s - loss: 1.3198 - val_loss: 0.9812\n",
            "108/108 - 7s - loss: 1.2852 - val_loss: 0.9788\n",
            "108/108 - 7s - loss: 1.2528 - val_loss: 0.9668\n",
            "108/108 - 7s - loss: 1.2556 - val_loss: 0.9971\n",
            "108/108 - 7s - loss: 1.3072 - val_loss: 1.0102\n",
            "108/108 - 7s - loss: 1.2601 - val_loss: 0.9759\n",
            "108/108 - 7s - loss: 1.2842 - val_loss: 1.0124\n",
            "108/108 - 7s - loss: 1.2639 - val_loss: 0.9605\n",
            "108/108 - 7s - loss: 1.2385 - val_loss: 0.9949\n",
            "108/108 - 7s - loss: 1.2097 - val_loss: 0.9587\n",
            "108/108 - 7s - loss: 1.2506 - val_loss: 0.9381\n",
            "108/108 - 7s - loss: 1.2663 - val_loss: 0.9546\n",
            "108/108 - 7s - loss: 1.2521 - val_loss: 0.9735\n",
            "108/108 - 7s - loss: 1.2568 - val_loss: 0.9634\n",
            "108/108 - 7s - loss: 1.2402 - val_loss: 0.9716\n",
            "108/108 - 7s - loss: 1.2948 - val_loss: 0.9751\n",
            "108/108 - 7s - loss: 1.2215 - val_loss: 0.9478\n",
            "108/108 - 7s - loss: 1.2445 - val_loss: 0.9364\n",
            "108/108 - 7s - loss: 1.2301 - val_loss: 1.0285\n",
            "108/108 - 7s - loss: 1.2836 - val_loss: 0.9883\n",
            "108/108 - 7s - loss: 1.2628 - val_loss: 0.9155\n",
            "108/108 - 7s - loss: 1.2557 - val_loss: 0.9381\n",
            "108/108 - 7s - loss: 1.2436 - val_loss: 0.9379\n",
            "108/108 - 7s - loss: 1.2789 - val_loss: 0.9557\n",
            "108/108 - 7s - loss: 1.2377 - val_loss: 0.9552\n",
            "108/108 - 7s - loss: 1.2512 - val_loss: 0.9625\n",
            "108/108 - 7s - loss: 1.2255 - val_loss: 0.9460\n",
            "108/108 - 7s - loss: 1.2209 - val_loss: 1.0277\n",
            "108/108 - 7s - loss: 1.2249 - val_loss: 0.9529\n",
            "108/108 - 7s - loss: 1.2755 - val_loss: 0.9779\n",
            "108/108 - 7s - loss: 1.2612 - val_loss: 0.9948\n",
            "108/108 - 7s - loss: 1.2518 - val_loss: 1.0153\n",
            "108/108 - 7s - loss: 1.2492 - val_loss: 1.0854\n",
            "108/108 - 7s - loss: 1.2188 - val_loss: 0.9710\n",
            "108/108 - 7s - loss: 1.2501 - val_loss: 0.9557\n",
            "108/108 - 7s - loss: 1.2614 - val_loss: 0.9678\n",
            "108/108 - 7s - loss: 1.2266 - val_loss: 1.0417\n",
            "108/108 - 7s - loss: 1.2742 - val_loss: 0.9909\n",
            "108/108 - 7s - loss: 1.2652 - val_loss: 0.9336\n",
            "108/108 - 7s - loss: 1.2298 - val_loss: 0.9917\n",
            "108/108 - 7s - loss: 1.2562 - val_loss: 0.9597\n",
            "108/108 - 7s - loss: 1.2579 - val_loss: 1.0358\n",
            "108/108 - 7s - loss: 1.2507 - val_loss: 0.9903\n",
            "108/108 - 7s - loss: 1.2704 - val_loss: 0.9158\n",
            "108/108 - 7s - loss: 1.2194 - val_loss: 0.9609\n",
            "108/108 - 7s - loss: 1.2723 - val_loss: 0.9602\n",
            "108/108 - 7s - loss: 1.2265 - val_loss: 0.9424\n",
            "108/108 - 7s - loss: 1.2750 - val_loss: 0.9530\n",
            "108/108 - 7s - loss: 1.2736 - val_loss: 0.9513\n",
            "108/108 - 7s - loss: 1.2475 - val_loss: 0.9987\n",
            "108/108 - 7s - loss: 1.2407 - val_loss: 1.0009\n",
            "108/108 - 7s - loss: 1.2162 - val_loss: 0.9049\n",
            "108/108 - 7s - loss: 1.2284 - val_loss: 0.9786\n",
            "108/108 - 7s - loss: 1.1995 - val_loss: 0.9527\n",
            "108/108 - 7s - loss: 1.2387 - val_loss: 0.9485\n",
            "108/108 - 7s - loss: 1.2211 - val_loss: 0.9790\n",
            "108/108 - 7s - loss: 1.2285 - val_loss: 0.9683\n",
            "108/108 - 7s - loss: 1.2442 - val_loss: 0.9675\n",
            "108/108 - 7s - loss: 1.2146 - val_loss: 0.9417\n",
            "108/108 - 7s - loss: 1.2350 - val_loss: 0.9307\n",
            "108/108 - 7s - loss: 1.2339 - val_loss: 0.9649\n",
            "108/108 - 7s - loss: 1.2405 - val_loss: 0.9342\n",
            "108/108 - 7s - loss: 1.2244 - val_loss: 0.9458\n",
            "108/108 - 7s - loss: 1.2275 - val_loss: 0.9384\n",
            "108/108 - 7s - loss: 1.1742 - val_loss: 0.9465\n",
            "108/108 - 7s - loss: 1.2407 - val_loss: 1.0190\n",
            "finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-A1v-rHSAjr",
        "outputId": "2600c38b-2b9f-4ea8-831f-3d430c9d3a0c"
      },
      "source": [
        "n_batch = 8\r\n",
        "\r\n",
        "for l, q in enumerate(np.arange(0.1, 1, 0.1)):\r\n",
        "  print(q)\r\n",
        "  model = Sequential()\r\n",
        "  for i in range(2):\r\n",
        "      model.add(LSTM(64, batch_input_shape=(8, 336, 8), stateful=True, return_sequences=True))\r\n",
        "      model.add(Dropout(0.3))\r\n",
        "  model.add(LSTM(64, batch_input_shape=(8, 336, 8), stateful=True, return_sequences=True))\r\n",
        "  model.add(Dropout(0.3))\r\n",
        "  model.add(LSTM(64, return_sequences=True))\r\n",
        "  model.add(Dropout(0.3))\r\n",
        "  model.add(LSTM(16))\r\n",
        "  model.add(Dropout(0.3))\r\n",
        "  model.add(Dense(96))\r\n",
        "  model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "  for j in range(500):\r\n",
        "    # model.fit(train_x, train_y, epochs=1, batch_size=1, shuffle=False, callbacks=[custom_hist], validation_data=(val_x, val_y))\r\n",
        "    model.fit(train_x, train_y, epochs=1, batch_size=n_batch, shuffle=False, validation_data=(val_x, val_y),  verbose=2)\r\n",
        "    model.reset_states()\r\n",
        "  weights = model.get_weights()\r\n",
        "  \r\n",
        "  single_item_model = Sequential()\r\n",
        "  for i in range(2):\r\n",
        "      single_item_model.add(LSTM(64, batch_input_shape=(1, 336, 8), stateful=True, return_sequences=True))\r\n",
        "      single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(LSTM(64, batch_input_shape=(1, 336, 8), stateful=True, return_sequences=True))\r\n",
        "  single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(LSTM(64, return_sequences=True))\r\n",
        "  single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(LSTM(16))\r\n",
        "  single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(Dense(96))\r\n",
        "  single_item_model.set_weights(weights)\r\n",
        "  single_item_model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "  \r\n",
        "  predictions = []\r\n",
        "  for k in range(81):\r\n",
        "    prediction = single_item_model.predict(np.array([test[k]]), batch_size=1)\r\n",
        "    predictions.append(sun_rise(np.array([test[k]]),prediction))\r\n",
        "  predictions = np.reshape(np.concatenate(np.array(predictions), axis=0),(81*96))\r\n",
        "  submission.iloc[:,l+1] = predictions\r\n",
        "submission.to_csv(f'submission.csv_1', index=False)\r\n",
        "print('finish')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1\n",
            "108/108 - 13s - loss: 1.7781 - val_loss: 1.8346\n",
            "108/108 - 6s - loss: 1.7315 - val_loss: 1.7264\n",
            "108/108 - 6s - loss: 1.6403 - val_loss: 1.6696\n",
            "108/108 - 6s - loss: 1.5950 - val_loss: 1.6355\n",
            "108/108 - 6s - loss: 1.5657 - val_loss: 1.6120\n",
            "108/108 - 6s - loss: 1.5459 - val_loss: 1.5965\n",
            "108/108 - 6s - loss: 1.5340 - val_loss: 1.5850\n",
            "108/108 - 6s - loss: 1.5262 - val_loss: 1.5768\n",
            "108/108 - 6s - loss: 1.5180 - val_loss: 1.5706\n",
            "108/108 - 6s - loss: 1.5151 - val_loss: 1.5661\n",
            "108/108 - 6s - loss: 1.5080 - val_loss: 1.5627\n",
            "108/108 - 6s - loss: 1.5055 - val_loss: 1.5601\n",
            "108/108 - 6s - loss: 1.5019 - val_loss: 1.5581\n",
            "108/108 - 6s - loss: 1.5036 - val_loss: 1.5564\n",
            "108/108 - 6s - loss: 1.4988 - val_loss: 1.5551\n",
            "108/108 - 6s - loss: 1.5010 - val_loss: 1.5541\n",
            "108/108 - 6s - loss: 1.5009 - val_loss: 1.5534\n",
            "108/108 - 6s - loss: 1.5024 - val_loss: 1.5527\n",
            "108/108 - 6s - loss: 1.4968 - val_loss: 1.5519\n",
            "108/108 - 6s - loss: 1.4992 - val_loss: 1.5514\n",
            "108/108 - 6s - loss: 1.4960 - val_loss: 1.5507\n",
            "108/108 - 6s - loss: 1.4954 - val_loss: 1.5506\n",
            "108/108 - 6s - loss: 1.4925 - val_loss: 1.5503\n",
            "108/108 - 6s - loss: 1.4962 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4972 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4921 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.5006 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4965 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.5001 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4962 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4934 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4976 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4953 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4953 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4957 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.5040 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4947 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4956 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4955 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4947 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4887 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.5041 - val_loss: 1.5490\n",
            "108/108 - 6s - loss: 1.5025 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4939 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4972 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4995 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4903 - val_loss: 1.5491\n",
            "108/108 - 6s - loss: 1.4990 - val_loss: 1.5491\n",
            "108/108 - 6s - loss: 1.4928 - val_loss: 1.5490\n",
            "108/108 - 6s - loss: 1.4985 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4948 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4966 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4969 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4969 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4888 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4894 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4937 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4951 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4941 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4951 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4955 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.5012 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4981 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4909 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4877 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.5012 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4948 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4961 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4927 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.5019 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.5002 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4980 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4877 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.5025 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4940 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4983 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4944 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4921 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4939 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4926 - val_loss: 1.5491\n",
            "108/108 - 6s - loss: 1.4937 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4952 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4966 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4967 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4922 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.5002 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4941 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4917 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.5001 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4963 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4980 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4983 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4884 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4962 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4985 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4929 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.5007 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4987 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4919 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4904 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4984 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.5012 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4889 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4907 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.5004 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4971 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4965 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.5033 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4950 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.5008 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4905 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.5009 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4976 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4955 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4944 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4963 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4978 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4976 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4984 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4938 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4984 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.5023 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4965 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4960 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.5007 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4918 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.5002 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4946 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.5014 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4964 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4969 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4932 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4968 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.5014 - val_loss: 1.5504\n",
            "108/108 - 6s - loss: 1.5029 - val_loss: 1.5506\n",
            "108/108 - 6s - loss: 1.4982 - val_loss: 1.5503\n",
            "108/108 - 6s - loss: 1.4956 - val_loss: 1.5502\n",
            "108/108 - 6s - loss: 1.4923 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4897 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4951 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.5049 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4909 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.5023 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4986 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.5047 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4998 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4918 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4939 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4947 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4966 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4967 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4940 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.5024 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4971 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4975 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4958 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4952 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4970 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4925 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4904 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4968 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4969 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4971 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.5003 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4954 - val_loss: 1.5502\n",
            "108/108 - 6s - loss: 1.4910 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4895 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.5008 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4957 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4979 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4984 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4992 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4948 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4991 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4953 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4944 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.5003 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4932 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4907 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4921 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4994 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4965 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4951 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.5002 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4970 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4920 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4919 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4954 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4999 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4876 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4977 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4962 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4969 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4952 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4995 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4949 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4920 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4985 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4946 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.5001 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4952 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4991 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4946 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.5012 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4901 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.5039 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4878 - val_loss: 1.5482\n",
            "108/108 - 6s - loss: 1.4887 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4962 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4957 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4957 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4988 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4950 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4928 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4951 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.5008 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.5060 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.5034 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4990 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4983 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4909 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4973 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.5013 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4981 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4959 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4919 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4996 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.5005 - val_loss: 1.5500\n",
            "108/108 - 6s - loss: 1.4908 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4882 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4996 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4945 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4995 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4959 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4888 - val_loss: 1.5564\n",
            "108/108 - 6s - loss: 1.4993 - val_loss: 1.5550\n",
            "108/108 - 6s - loss: 1.5004 - val_loss: 1.5542\n",
            "108/108 - 6s - loss: 1.5095 - val_loss: 1.5538\n",
            "108/108 - 6s - loss: 1.5005 - val_loss: 1.5533\n",
            "108/108 - 6s - loss: 1.4958 - val_loss: 1.5528\n",
            "108/108 - 6s - loss: 1.4953 - val_loss: 1.5522\n",
            "108/108 - 6s - loss: 1.4983 - val_loss: 1.5517\n",
            "108/108 - 6s - loss: 1.4939 - val_loss: 1.5513\n",
            "108/108 - 6s - loss: 1.4944 - val_loss: 1.5510\n",
            "108/108 - 6s - loss: 1.4993 - val_loss: 1.5507\n",
            "108/108 - 6s - loss: 1.4957 - val_loss: 1.5504\n",
            "108/108 - 6s - loss: 1.4998 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4998 - val_loss: 1.5503\n",
            "108/108 - 6s - loss: 1.4949 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4976 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4935 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4986 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4994 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4946 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4962 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4945 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4996 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4957 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.5034 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4960 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4926 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4938 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4962 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.5017 - val_loss: 1.5445\n",
            "108/108 - 6s - loss: 1.4971 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4963 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4932 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4974 - val_loss: 1.5498\n",
            "108/108 - 6s - loss: 1.4971 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4955 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4965 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4959 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4967 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.5022 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4965 - val_loss: 1.5501\n",
            "108/108 - 6s - loss: 1.4944 - val_loss: 1.5499\n",
            "108/108 - 6s - loss: 1.4883 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4920 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.5004 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4913 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4906 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4965 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4934 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4913 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.5016 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4992 - val_loss: 1.5491\n",
            "108/108 - 6s - loss: 1.4961 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.5047 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4941 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4977 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4940 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4912 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4941 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4947 - val_loss: 1.5495\n",
            "108/108 - 6s - loss: 1.4992 - val_loss: 1.5494\n",
            "108/108 - 6s - loss: 1.4958 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4938 - val_loss: 1.5493\n",
            "108/108 - 6s - loss: 1.4881 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4991 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4976 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4920 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4960 - val_loss: 1.5491\n",
            "108/108 - 6s - loss: 1.4980 - val_loss: 1.5492\n",
            "108/108 - 6s - loss: 1.4960 - val_loss: 1.5496\n",
            "108/108 - 6s - loss: 1.4981 - val_loss: 1.5497\n",
            "108/108 - 6s - loss: 1.4945 - val_loss: 1.5483\n",
            "108/108 - 6s - loss: 1.4928 - val_loss: 1.5448\n",
            "108/108 - 6s - loss: 1.4897 - val_loss: 1.5484\n",
            "108/108 - 6s - loss: 1.4954 - val_loss: 1.5401\n",
            "108/108 - 6s - loss: 1.4861 - val_loss: 1.5402\n",
            "108/108 - 6s - loss: 1.4847 - val_loss: 1.5378\n",
            "108/108 - 6s - loss: 1.4883 - val_loss: 1.5405\n",
            "108/108 - 6s - loss: 1.4847 - val_loss: 1.5417\n",
            "108/108 - 6s - loss: 1.4832 - val_loss: 1.5406\n",
            "108/108 - 6s - loss: 1.4860 - val_loss: 1.5379\n",
            "108/108 - 6s - loss: 1.4796 - val_loss: 1.5378\n",
            "108/108 - 6s - loss: 1.4839 - val_loss: 1.5400\n",
            "108/108 - 6s - loss: 1.4805 - val_loss: 1.5404\n",
            "108/108 - 6s - loss: 1.4787 - val_loss: 1.5374\n",
            "108/108 - 6s - loss: 1.4748 - val_loss: 1.5365\n",
            "108/108 - 6s - loss: 1.4706 - val_loss: 1.5335\n",
            "108/108 - 6s - loss: 1.4752 - val_loss: 1.5342\n",
            "108/108 - 6s - loss: 1.4769 - val_loss: 1.5370\n",
            "108/108 - 6s - loss: 1.4782 - val_loss: 1.5373\n",
            "108/108 - 6s - loss: 1.4706 - val_loss: 1.5300\n",
            "108/108 - 6s - loss: 1.4693 - val_loss: 1.5306\n",
            "108/108 - 6s - loss: 1.4703 - val_loss: 1.5315\n",
            "108/108 - 6s - loss: 1.4697 - val_loss: 1.5328\n",
            "108/108 - 6s - loss: 1.4729 - val_loss: 1.5292\n",
            "108/108 - 6s - loss: 1.4736 - val_loss: 1.5307\n",
            "108/108 - 6s - loss: 1.4697 - val_loss: 1.5295\n",
            "108/108 - 6s - loss: 1.4680 - val_loss: 1.5280\n",
            "108/108 - 6s - loss: 1.4615 - val_loss: 1.5259\n",
            "108/108 - 6s - loss: 1.4625 - val_loss: 1.5248\n",
            "108/108 - 6s - loss: 1.4641 - val_loss: 1.5236\n",
            "108/108 - 6s - loss: 1.4675 - val_loss: 1.5249\n",
            "108/108 - 6s - loss: 1.4638 - val_loss: 1.5259\n",
            "108/108 - 6s - loss: 1.4664 - val_loss: 1.5261\n",
            "108/108 - 6s - loss: 1.4646 - val_loss: 1.5306\n",
            "108/108 - 6s - loss: 1.4610 - val_loss: 1.5260\n",
            "108/108 - 6s - loss: 1.4699 - val_loss: 1.5250\n",
            "108/108 - 6s - loss: 1.4654 - val_loss: 1.5190\n",
            "108/108 - 6s - loss: 1.4579 - val_loss: 1.5333\n",
            "108/108 - 6s - loss: 1.4667 - val_loss: 1.5343\n",
            "108/108 - 6s - loss: 1.4618 - val_loss: 1.5291\n",
            "108/108 - 6s - loss: 1.4613 - val_loss: 1.5238\n",
            "108/108 - 6s - loss: 1.4559 - val_loss: 1.5239\n",
            "108/108 - 6s - loss: 1.4621 - val_loss: 1.5193\n",
            "108/108 - 6s - loss: 1.4587 - val_loss: 1.5231\n",
            "108/108 - 6s - loss: 1.4582 - val_loss: 1.5223\n",
            "108/108 - 6s - loss: 1.4556 - val_loss: 1.5185\n",
            "108/108 - 6s - loss: 1.4603 - val_loss: 1.5179\n",
            "108/108 - 6s - loss: 1.4565 - val_loss: 1.5173\n",
            "108/108 - 6s - loss: 1.4505 - val_loss: 1.5249\n",
            "108/108 - 6s - loss: 1.4558 - val_loss: 1.5258\n",
            "108/108 - 6s - loss: 1.4596 - val_loss: 1.5228\n",
            "108/108 - 6s - loss: 1.4471 - val_loss: 1.5191\n",
            "108/108 - 6s - loss: 1.4507 - val_loss: 1.5204\n",
            "108/108 - 6s - loss: 1.4572 - val_loss: 1.5168\n",
            "108/108 - 6s - loss: 1.4495 - val_loss: 1.5196\n",
            "108/108 - 6s - loss: 1.4453 - val_loss: 1.5254\n",
            "108/108 - 6s - loss: 1.4508 - val_loss: 1.5151\n",
            "108/108 - 6s - loss: 1.4520 - val_loss: 1.5170\n",
            "108/108 - 6s - loss: 1.4505 - val_loss: 1.5180\n",
            "108/108 - 6s - loss: 1.4373 - val_loss: 1.5185\n",
            "108/108 - 6s - loss: 1.4493 - val_loss: 1.5239\n",
            "108/108 - 6s - loss: 1.4445 - val_loss: 1.5149\n",
            "108/108 - 6s - loss: 1.4406 - val_loss: 1.5116\n",
            "108/108 - 6s - loss: 1.4415 - val_loss: 1.5194\n",
            "108/108 - 6s - loss: 1.4395 - val_loss: 1.5097\n",
            "108/108 - 6s - loss: 1.4433 - val_loss: 1.5183\n",
            "108/108 - 6s - loss: 1.4312 - val_loss: 1.5175\n",
            "108/108 - 6s - loss: 1.4353 - val_loss: 1.5164\n",
            "108/108 - 6s - loss: 1.4341 - val_loss: 1.5112\n",
            "108/108 - 6s - loss: 1.4310 - val_loss: 1.5047\n",
            "108/108 - 6s - loss: 1.4246 - val_loss: 1.5273\n",
            "108/108 - 6s - loss: 1.4378 - val_loss: 1.5167\n",
            "108/108 - 6s - loss: 1.4246 - val_loss: 1.5183\n",
            "108/108 - 6s - loss: 1.4307 - val_loss: 1.5176\n",
            "108/108 - 6s - loss: 1.4196 - val_loss: 1.5136\n",
            "108/108 - 6s - loss: 1.4316 - val_loss: 1.5114\n",
            "108/108 - 6s - loss: 1.4301 - val_loss: 1.5168\n",
            "108/108 - 6s - loss: 1.4247 - val_loss: 1.5083\n",
            "108/108 - 6s - loss: 1.4340 - val_loss: 1.5077\n",
            "108/108 - 6s - loss: 1.4239 - val_loss: 1.5156\n",
            "108/108 - 6s - loss: 1.4196 - val_loss: 1.5148\n",
            "108/108 - 6s - loss: 1.4190 - val_loss: 1.5173\n",
            "108/108 - 6s - loss: 1.4227 - val_loss: 1.5146\n",
            "108/108 - 6s - loss: 1.4254 - val_loss: 1.5147\n",
            "108/108 - 6s - loss: 1.4260 - val_loss: 1.5178\n",
            "108/108 - 6s - loss: 1.4200 - val_loss: 1.5139\n",
            "108/108 - 6s - loss: 1.4165 - val_loss: 1.5022\n",
            "108/108 - 6s - loss: 1.4194 - val_loss: 1.5128\n",
            "108/108 - 6s - loss: 1.4233 - val_loss: 1.5159\n",
            "108/108 - 6s - loss: 1.4228 - val_loss: 1.5162\n",
            "108/108 - 6s - loss: 1.4210 - val_loss: 1.5032\n",
            "108/108 - 6s - loss: 1.4195 - val_loss: 1.5099\n",
            "108/108 - 6s - loss: 1.4115 - val_loss: 1.5098\n",
            "108/108 - 6s - loss: 1.4127 - val_loss: 1.4982\n",
            "108/108 - 6s - loss: 1.4187 - val_loss: 1.5010\n",
            "108/108 - 6s - loss: 1.4193 - val_loss: 1.5026\n",
            "108/108 - 6s - loss: 1.4222 - val_loss: 1.5025\n",
            "108/108 - 6s - loss: 1.4131 - val_loss: 1.5081\n",
            "108/108 - 6s - loss: 1.4051 - val_loss: 1.5101\n",
            "108/108 - 6s - loss: 1.4015 - val_loss: 1.5117\n",
            "108/108 - 6s - loss: 1.4142 - val_loss: 1.5063\n",
            "108/108 - 6s - loss: 1.4133 - val_loss: 1.5149\n",
            "108/108 - 6s - loss: 1.4028 - val_loss: 1.4993\n",
            "108/108 - 6s - loss: 1.4039 - val_loss: 1.5091\n",
            "108/108 - 6s - loss: 1.4162 - val_loss: 1.5095\n",
            "108/108 - 6s - loss: 1.4125 - val_loss: 1.5051\n",
            "108/108 - 6s - loss: 1.4051 - val_loss: 1.5023\n",
            "108/108 - 6s - loss: 1.3984 - val_loss: 1.4946\n",
            "108/108 - 6s - loss: 1.3928 - val_loss: 1.5022\n",
            "108/108 - 6s - loss: 1.4188 - val_loss: 1.5019\n",
            "108/108 - 6s - loss: 1.3999 - val_loss: 1.5027\n",
            "108/108 - 6s - loss: 1.3930 - val_loss: 1.5023\n",
            "108/108 - 6s - loss: 1.4100 - val_loss: 1.4943\n",
            "108/108 - 6s - loss: 1.4004 - val_loss: 1.4974\n",
            "108/108 - 6s - loss: 1.4024 - val_loss: 1.5105\n",
            "108/108 - 6s - loss: 1.3999 - val_loss: 1.5084\n",
            "108/108 - 6s - loss: 1.3985 - val_loss: 1.5066\n",
            "108/108 - 6s - loss: 1.4060 - val_loss: 1.5083\n",
            "108/108 - 6s - loss: 1.4101 - val_loss: 1.5015\n",
            "108/108 - 6s - loss: 1.4018 - val_loss: 1.5138\n",
            "108/108 - 6s - loss: 1.4039 - val_loss: 1.5127\n",
            "108/108 - 6s - loss: 1.3993 - val_loss: 1.4989\n",
            "108/108 - 6s - loss: 1.3967 - val_loss: 1.5041\n",
            "108/108 - 6s - loss: 1.3911 - val_loss: 1.5041\n",
            "108/108 - 6s - loss: 1.3982 - val_loss: 1.4998\n",
            "108/108 - 6s - loss: 1.3951 - val_loss: 1.5010\n",
            "108/108 - 6s - loss: 1.3897 - val_loss: 1.5120\n",
            "108/108 - 6s - loss: 1.3894 - val_loss: 1.5137\n",
            "108/108 - 6s - loss: 1.3903 - val_loss: 1.5083\n",
            "108/108 - 6s - loss: 1.3852 - val_loss: 1.5209\n",
            "108/108 - 6s - loss: 1.3911 - val_loss: 1.5121\n",
            "108/108 - 6s - loss: 1.3879 - val_loss: 1.5210\n",
            "108/108 - 6s - loss: 1.4025 - val_loss: 1.4982\n",
            "108/108 - 6s - loss: 1.3927 - val_loss: 1.4958\n",
            "108/108 - 6s - loss: 1.3888 - val_loss: 1.5005\n",
            "108/108 - 6s - loss: 1.3978 - val_loss: 1.4978\n",
            "108/108 - 6s - loss: 1.3824 - val_loss: 1.4958\n",
            "108/108 - 6s - loss: 1.3964 - val_loss: 1.5017\n",
            "108/108 - 6s - loss: 1.4007 - val_loss: 1.4880\n",
            "108/108 - 6s - loss: 1.3864 - val_loss: 1.4974\n",
            "108/108 - 6s - loss: 1.3973 - val_loss: 1.4963\n",
            "108/108 - 6s - loss: 1.3780 - val_loss: 1.5009\n",
            "108/108 - 6s - loss: 1.3800 - val_loss: 1.5117\n",
            "108/108 - 6s - loss: 1.3895 - val_loss: 1.5006\n",
            "108/108 - 6s - loss: 1.3904 - val_loss: 1.5126\n",
            "108/108 - 6s - loss: 1.3826 - val_loss: 1.5133\n",
            "108/108 - 6s - loss: 1.3852 - val_loss: 1.5120\n",
            "108/108 - 6s - loss: 1.3889 - val_loss: 1.5019\n",
            "108/108 - 6s - loss: 1.3837 - val_loss: 1.5007\n",
            "108/108 - 6s - loss: 1.3809 - val_loss: 1.5128\n",
            "108/108 - 6s - loss: 1.3907 - val_loss: 1.4950\n",
            "108/108 - 6s - loss: 1.3807 - val_loss: 1.4933\n",
            "108/108 - 6s - loss: 1.3850 - val_loss: 1.5084\n",
            "108/108 - 6s - loss: 1.3814 - val_loss: 1.5071\n",
            "108/108 - 6s - loss: 1.3815 - val_loss: 1.5010\n",
            "108/108 - 6s - loss: 1.3884 - val_loss: 1.4944\n",
            "108/108 - 6s - loss: 1.3854 - val_loss: 1.4914\n",
            "108/108 - 6s - loss: 1.3806 - val_loss: 1.5115\n",
            "108/108 - 6s - loss: 1.3789 - val_loss: 1.4976\n",
            "108/108 - 6s - loss: 1.3846 - val_loss: 1.4897\n",
            "108/108 - 6s - loss: 1.3679 - val_loss: 1.4941\n",
            "108/108 - 6s - loss: 1.3727 - val_loss: 1.4927\n",
            "108/108 - 6s - loss: 1.3820 - val_loss: 1.5008\n",
            "108/108 - 6s - loss: 1.3805 - val_loss: 1.4941\n",
            "108/108 - 6s - loss: 1.3673 - val_loss: 1.4902\n",
            "108/108 - 6s - loss: 1.3678 - val_loss: 1.4837\n",
            "108/108 - 6s - loss: 1.3791 - val_loss: 1.4853\n",
            "108/108 - 6s - loss: 1.3710 - val_loss: 1.4842\n",
            "108/108 - 6s - loss: 1.3915 - val_loss: 1.4920\n",
            "108/108 - 6s - loss: 1.3712 - val_loss: 1.4885\n",
            "108/108 - 6s - loss: 1.3813 - val_loss: 1.4947\n",
            "108/108 - 6s - loss: 1.3745 - val_loss: 1.4936\n",
            "108/108 - 6s - loss: 1.3580 - val_loss: 1.4965\n",
            "108/108 - 6s - loss: 1.3664 - val_loss: 1.4973\n",
            "108/108 - 6s - loss: 1.3743 - val_loss: 1.4929\n",
            "108/108 - 6s - loss: 1.3602 - val_loss: 1.4922\n",
            "108/108 - 6s - loss: 1.3744 - val_loss: 1.4887\n",
            "108/108 - 6s - loss: 1.3670 - val_loss: 1.4732\n",
            "108/108 - 6s - loss: 1.3669 - val_loss: 1.4970\n",
            "108/108 - 6s - loss: 1.3896 - val_loss: 1.5240\n",
            "108/108 - 6s - loss: 1.3761 - val_loss: 1.4940\n",
            "108/108 - 6s - loss: 1.3621 - val_loss: 1.4877\n",
            "108/108 - 6s - loss: 1.3512 - val_loss: 1.4824\n",
            "108/108 - 6s - loss: 1.3744 - val_loss: 1.4741\n",
            "108/108 - 6s - loss: 1.3664 - val_loss: 1.4881\n",
            "108/108 - 6s - loss: 1.3666 - val_loss: 1.5026\n",
            "108/108 - 6s - loss: 1.3584 - val_loss: 1.4846\n",
            "108/108 - 6s - loss: 1.3756 - val_loss: 1.4813\n",
            "108/108 - 6s - loss: 1.3629 - val_loss: 1.4849\n",
            "108/108 - 6s - loss: 1.3705 - val_loss: 1.4933\n",
            "108/108 - 6s - loss: 1.3566 - val_loss: 1.4950\n",
            "108/108 - 6s - loss: 1.3581 - val_loss: 1.4847\n",
            "108/108 - 6s - loss: 1.3560 - val_loss: 1.4892\n",
            "108/108 - 6s - loss: 1.3554 - val_loss: 1.4964\n",
            "108/108 - 6s - loss: 1.3561 - val_loss: 1.4807\n",
            "0.2\n",
            "108/108 - 13s - loss: 3.5541 - val_loss: 3.6532\n",
            "108/108 - 6s - loss: 3.4017 - val_loss: 3.4002\n",
            "108/108 - 6s - loss: 3.2165 - val_loss: 3.2642\n",
            "108/108 - 6s - loss: 3.1010 - val_loss: 3.1675\n",
            "108/108 - 6s - loss: 3.0226 - val_loss: 3.0924\n",
            "108/108 - 6s - loss: 2.9512 - val_loss: 3.0304\n",
            "108/108 - 6s - loss: 2.8928 - val_loss: 2.9776\n",
            "108/108 - 6s - loss: 2.8507 - val_loss: 2.9338\n",
            "108/108 - 6s - loss: 2.8145 - val_loss: 2.8961\n",
            "108/108 - 6s - loss: 2.7749 - val_loss: 2.8626\n",
            "108/108 - 6s - loss: 2.7569 - val_loss: 2.8336\n",
            "108/108 - 6s - loss: 2.7236 - val_loss: 2.8086\n",
            "108/108 - 6s - loss: 2.7073 - val_loss: 2.7874\n",
            "108/108 - 6s - loss: 2.6801 - val_loss: 2.7688\n",
            "108/108 - 6s - loss: 2.6785 - val_loss: 2.7533\n",
            "108/108 - 6s - loss: 2.6429 - val_loss: 2.7394\n",
            "108/108 - 6s - loss: 2.6484 - val_loss: 2.7276\n",
            "108/108 - 6s - loss: 2.6422 - val_loss: 2.7171\n",
            "108/108 - 6s - loss: 2.6346 - val_loss: 2.7086\n",
            "108/108 - 6s - loss: 2.6253 - val_loss: 2.7011\n",
            "108/108 - 6s - loss: 2.6296 - val_loss: 2.6952\n",
            "108/108 - 6s - loss: 2.6250 - val_loss: 2.6903\n",
            "108/108 - 6s - loss: 2.6116 - val_loss: 2.6853\n",
            "108/108 - 6s - loss: 2.6197 - val_loss: 2.6809\n",
            "108/108 - 6s - loss: 2.6169 - val_loss: 2.6775\n",
            "108/108 - 6s - loss: 2.6143 - val_loss: 2.6741\n",
            "108/108 - 6s - loss: 2.5882 - val_loss: 2.6705\n",
            "108/108 - 6s - loss: 2.6011 - val_loss: 2.6678\n",
            "108/108 - 6s - loss: 2.6016 - val_loss: 2.6656\n",
            "108/108 - 6s - loss: 2.6135 - val_loss: 2.6641\n",
            "108/108 - 6s - loss: 2.5893 - val_loss: 2.6625\n",
            "108/108 - 6s - loss: 2.6023 - val_loss: 2.6612\n",
            "108/108 - 6s - loss: 2.6035 - val_loss: 2.6600\n",
            "108/108 - 6s - loss: 2.6085 - val_loss: 2.6590\n",
            "108/108 - 6s - loss: 2.6052 - val_loss: 2.6577\n",
            "108/108 - 6s - loss: 2.6106 - val_loss: 2.6572\n",
            "108/108 - 6s - loss: 2.5987 - val_loss: 2.6563\n",
            "108/108 - 6s - loss: 2.5931 - val_loss: 2.6550\n",
            "108/108 - 6s - loss: 2.6013 - val_loss: 2.6548\n",
            "108/108 - 6s - loss: 2.6170 - val_loss: 2.6548\n",
            "108/108 - 6s - loss: 2.6072 - val_loss: 2.6544\n",
            "108/108 - 6s - loss: 2.6082 - val_loss: 2.6546\n",
            "108/108 - 6s - loss: 2.6122 - val_loss: 2.6545\n",
            "108/108 - 6s - loss: 2.6155 - val_loss: 2.6538\n",
            "108/108 - 6s - loss: 2.6028 - val_loss: 2.6533\n",
            "108/108 - 6s - loss: 2.6089 - val_loss: 2.6534\n",
            "108/108 - 6s - loss: 2.6099 - val_loss: 2.6525\n",
            "108/108 - 6s - loss: 2.6041 - val_loss: 2.6522\n",
            "108/108 - 6s - loss: 2.6097 - val_loss: 2.6522\n",
            "108/108 - 6s - loss: 2.5952 - val_loss: 2.6523\n",
            "108/108 - 6s - loss: 2.5995 - val_loss: 2.6521\n",
            "108/108 - 6s - loss: 2.5997 - val_loss: 2.6519\n",
            "108/108 - 6s - loss: 2.6021 - val_loss: 2.6521\n",
            "108/108 - 6s - loss: 2.5981 - val_loss: 2.6516\n",
            "108/108 - 6s - loss: 2.5952 - val_loss: 2.6517\n",
            "108/108 - 6s - loss: 2.5902 - val_loss: 2.6511\n",
            "108/108 - 6s - loss: 2.5940 - val_loss: 2.6512\n",
            "108/108 - 6s - loss: 2.6031 - val_loss: 2.6511\n",
            "108/108 - 6s - loss: 2.5917 - val_loss: 2.6509\n",
            "108/108 - 6s - loss: 2.5852 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.6049 - val_loss: 2.6506\n",
            "108/108 - 6s - loss: 2.6057 - val_loss: 2.6508\n",
            "108/108 - 6s - loss: 2.5929 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.5886 - val_loss: 2.6506\n",
            "108/108 - 6s - loss: 2.5937 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.6082 - val_loss: 2.6509\n",
            "108/108 - 6s - loss: 2.5954 - val_loss: 2.6506\n",
            "108/108 - 6s - loss: 2.5910 - val_loss: 2.6498\n",
            "108/108 - 6s - loss: 2.6002 - val_loss: 2.6494\n",
            "108/108 - 6s - loss: 2.6065 - val_loss: 2.6494\n",
            "108/108 - 6s - loss: 2.5902 - val_loss: 2.6488\n",
            "108/108 - 6s - loss: 2.6046 - val_loss: 2.6489\n",
            "108/108 - 6s - loss: 2.5845 - val_loss: 2.6486\n",
            "108/108 - 6s - loss: 2.6019 - val_loss: 2.6489\n",
            "108/108 - 6s - loss: 2.6038 - val_loss: 2.6487\n",
            "108/108 - 6s - loss: 2.6097 - val_loss: 2.6492\n",
            "108/108 - 6s - loss: 2.5759 - val_loss: 2.6487\n",
            "108/108 - 6s - loss: 2.5835 - val_loss: 2.6491\n",
            "108/108 - 6s - loss: 2.5926 - val_loss: 2.6489\n",
            "108/108 - 6s - loss: 2.5973 - val_loss: 2.6494\n",
            "108/108 - 6s - loss: 2.6029 - val_loss: 2.6492\n",
            "108/108 - 6s - loss: 2.5890 - val_loss: 2.6487\n",
            "108/108 - 6s - loss: 2.6049 - val_loss: 2.6488\n",
            "108/108 - 6s - loss: 2.6131 - val_loss: 2.6491\n",
            "108/108 - 6s - loss: 2.6118 - val_loss: 2.6495\n",
            "108/108 - 6s - loss: 2.5881 - val_loss: 2.6489\n",
            "108/108 - 6s - loss: 2.6038 - val_loss: 2.6490\n",
            "108/108 - 6s - loss: 2.6173 - val_loss: 2.6492\n",
            "108/108 - 6s - loss: 2.6080 - val_loss: 2.6493\n",
            "108/108 - 6s - loss: 2.6169 - val_loss: 2.6498\n",
            "108/108 - 6s - loss: 2.5938 - val_loss: 2.6499\n",
            "108/108 - 6s - loss: 2.5868 - val_loss: 2.6497\n",
            "108/108 - 6s - loss: 2.6202 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.6037 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.6062 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.6001 - val_loss: 2.6505\n",
            "108/108 - 6s - loss: 2.6034 - val_loss: 2.6505\n",
            "108/108 - 6s - loss: 2.6127 - val_loss: 2.6507\n",
            "108/108 - 6s - loss: 2.5880 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.6025 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.6184 - val_loss: 2.6505\n",
            "108/108 - 6s - loss: 2.5846 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.6062 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.6003 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.6033 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.6090 - val_loss: 2.6505\n",
            "108/108 - 6s - loss: 2.6135 - val_loss: 2.6507\n",
            "108/108 - 6s - loss: 2.6035 - val_loss: 2.6509\n",
            "108/108 - 6s - loss: 2.5842 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.6031 - val_loss: 2.6501\n",
            "108/108 - 6s - loss: 2.5974 - val_loss: 2.6505\n",
            "108/108 - 6s - loss: 2.5972 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.5882 - val_loss: 2.6497\n",
            "108/108 - 6s - loss: 2.6026 - val_loss: 2.6498\n",
            "108/108 - 6s - loss: 2.6111 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.6051 - val_loss: 2.6501\n",
            "108/108 - 6s - loss: 2.6073 - val_loss: 2.6505\n",
            "108/108 - 6s - loss: 2.5913 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.5863 - val_loss: 2.6498\n",
            "108/108 - 6s - loss: 2.5911 - val_loss: 2.6496\n",
            "108/108 - 6s - loss: 2.5946 - val_loss: 2.6493\n",
            "108/108 - 6s - loss: 2.6014 - val_loss: 2.6490\n",
            "108/108 - 6s - loss: 2.5874 - val_loss: 2.6490\n",
            "108/108 - 6s - loss: 2.5931 - val_loss: 2.6491\n",
            "108/108 - 6s - loss: 2.5943 - val_loss: 2.6494\n",
            "108/108 - 6s - loss: 2.5989 - val_loss: 2.6491\n",
            "108/108 - 6s - loss: 2.6172 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.5976 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.5984 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.5845 - val_loss: 2.6497\n",
            "108/108 - 6s - loss: 2.6142 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.6084 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.5888 - val_loss: 2.6499\n",
            "108/108 - 6s - loss: 2.5853 - val_loss: 2.6493\n",
            "108/108 - 6s - loss: 2.6126 - val_loss: 2.6496\n",
            "108/108 - 6s - loss: 2.6070 - val_loss: 2.6499\n",
            "108/108 - 6s - loss: 2.6018 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.6012 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.6025 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.6006 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.5820 - val_loss: 2.6498\n",
            "108/108 - 6s - loss: 2.6024 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.5853 - val_loss: 2.6493\n",
            "108/108 - 6s - loss: 2.6098 - val_loss: 2.6496\n",
            "108/108 - 6s - loss: 2.5950 - val_loss: 2.6499\n",
            "108/108 - 6s - loss: 2.5966 - val_loss: 2.6498\n",
            "108/108 - 6s - loss: 2.5990 - val_loss: 2.6497\n",
            "108/108 - 6s - loss: 2.6084 - val_loss: 2.6497\n",
            "108/108 - 6s - loss: 2.6060 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.5922 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.6049 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.5935 - val_loss: 2.6500\n",
            "108/108 - 6s - loss: 2.5989 - val_loss: 2.6498\n",
            "108/108 - 6s - loss: 2.6027 - val_loss: 2.6499\n",
            "108/108 - 6s - loss: 2.6059 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.6014 - val_loss: 2.6504\n",
            "108/108 - 6s - loss: 2.6029 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.5938 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.6091 - val_loss: 2.6508\n",
            "108/108 - 6s - loss: 2.5920 - val_loss: 2.6501\n",
            "108/108 - 6s - loss: 2.6137 - val_loss: 2.6505\n",
            "108/108 - 6s - loss: 2.6001 - val_loss: 2.6506\n",
            "108/108 - 6s - loss: 2.6135 - val_loss: 2.6510\n",
            "108/108 - 6s - loss: 2.5909 - val_loss: 2.6507\n",
            "108/108 - 6s - loss: 2.6517 - val_loss: 2.9345\n",
            "108/108 - 6s - loss: 2.6570 - val_loss: 2.7075\n",
            "108/108 - 6s - loss: 2.6326 - val_loss: 2.7007\n",
            "108/108 - 6s - loss: 2.6299 - val_loss: 2.6952\n",
            "108/108 - 6s - loss: 2.6206 - val_loss: 2.6899\n",
            "108/108 - 6s - loss: 2.6143 - val_loss: 2.6857\n",
            "108/108 - 6s - loss: 2.6142 - val_loss: 2.6817\n",
            "108/108 - 6s - loss: 2.6111 - val_loss: 2.6786\n",
            "108/108 - 6s - loss: 2.6206 - val_loss: 2.6755\n",
            "108/108 - 6s - loss: 2.6176 - val_loss: 2.6729\n",
            "108/108 - 6s - loss: 2.6035 - val_loss: 2.6657\n",
            "108/108 - 6s - loss: 2.6160 - val_loss: 2.6563\n",
            "108/108 - 6s - loss: 2.6056 - val_loss: 2.6539\n",
            "108/108 - 6s - loss: 2.5877 - val_loss: 2.6343\n",
            "108/108 - 6s - loss: 2.5809 - val_loss: 2.6223\n",
            "108/108 - 6s - loss: 2.5496 - val_loss: 2.6122\n",
            "108/108 - 6s - loss: 2.5603 - val_loss: 2.6197\n",
            "108/108 - 6s - loss: 2.5760 - val_loss: 2.6257\n",
            "108/108 - 6s - loss: 2.5404 - val_loss: 2.6019\n",
            "108/108 - 6s - loss: 2.5126 - val_loss: 2.6110\n",
            "108/108 - 6s - loss: 2.5296 - val_loss: 2.6038\n",
            "108/108 - 6s - loss: 2.5171 - val_loss: 2.5770\n",
            "108/108 - 6s - loss: 2.5175 - val_loss: 2.5814\n",
            "108/108 - 6s - loss: 2.5054 - val_loss: 2.6012\n",
            "108/108 - 6s - loss: 2.5155 - val_loss: 2.5634\n",
            "108/108 - 6s - loss: 2.4982 - val_loss: 2.5576\n",
            "108/108 - 6s - loss: 2.4877 - val_loss: 2.5514\n",
            "108/108 - 6s - loss: 2.4928 - val_loss: 2.5484\n",
            "108/108 - 6s - loss: 2.4939 - val_loss: 2.5517\n",
            "108/108 - 6s - loss: 2.4775 - val_loss: 2.5359\n",
            "108/108 - 6s - loss: 2.4633 - val_loss: 2.5453\n",
            "108/108 - 6s - loss: 2.4597 - val_loss: 2.5342\n",
            "108/108 - 6s - loss: 2.4546 - val_loss: 2.5533\n",
            "108/108 - 6s - loss: 2.4485 - val_loss: 2.5554\n",
            "108/108 - 6s - loss: 2.4374 - val_loss: 2.5423\n",
            "108/108 - 6s - loss: 2.4500 - val_loss: 2.5228\n",
            "108/108 - 6s - loss: 2.4496 - val_loss: 2.5132\n",
            "108/108 - 6s - loss: 2.4308 - val_loss: 2.5128\n",
            "108/108 - 6s - loss: 2.4169 - val_loss: 2.5256\n",
            "108/108 - 6s - loss: 2.4181 - val_loss: 2.5297\n",
            "108/108 - 6s - loss: 2.4305 - val_loss: 2.5250\n",
            "108/108 - 6s - loss: 2.3977 - val_loss: 2.5119\n",
            "108/108 - 6s - loss: 2.4182 - val_loss: 2.5139\n",
            "108/108 - 6s - loss: 2.4130 - val_loss: 2.5169\n",
            "108/108 - 6s - loss: 2.3970 - val_loss: 2.5211\n",
            "108/108 - 6s - loss: 2.4129 - val_loss: 2.4993\n",
            "108/108 - 6s - loss: 2.3817 - val_loss: 2.5231\n",
            "108/108 - 6s - loss: 2.4122 - val_loss: 2.5060\n",
            "108/108 - 6s - loss: 2.3951 - val_loss: 2.5069\n",
            "108/108 - 6s - loss: 2.3988 - val_loss: 2.5032\n",
            "108/108 - 6s - loss: 2.3971 - val_loss: 2.5087\n",
            "108/108 - 6s - loss: 2.3752 - val_loss: 2.5172\n",
            "108/108 - 6s - loss: 2.3816 - val_loss: 2.5119\n",
            "108/108 - 6s - loss: 2.3793 - val_loss: 2.5061\n",
            "108/108 - 6s - loss: 2.3656 - val_loss: 2.5024\n",
            "108/108 - 6s - loss: 2.3693 - val_loss: 2.5052\n",
            "108/108 - 6s - loss: 2.3739 - val_loss: 2.5027\n",
            "108/108 - 6s - loss: 2.3604 - val_loss: 2.5028\n",
            "108/108 - 6s - loss: 2.3598 - val_loss: 2.4957\n",
            "108/108 - 6s - loss: 2.3401 - val_loss: 2.4926\n",
            "108/108 - 6s - loss: 2.3423 - val_loss: 2.4760\n",
            "108/108 - 6s - loss: 2.3487 - val_loss: 2.4868\n",
            "108/108 - 6s - loss: 2.3309 - val_loss: 2.4885\n",
            "108/108 - 6s - loss: 2.3139 - val_loss: 2.4907\n",
            "108/108 - 6s - loss: 2.3489 - val_loss: 2.4867\n",
            "108/108 - 6s - loss: 2.3416 - val_loss: 2.4910\n",
            "108/108 - 6s - loss: 2.3471 - val_loss: 2.4735\n",
            "108/108 - 6s - loss: 2.3349 - val_loss: 2.4736\n",
            "108/108 - 6s - loss: 2.3127 - val_loss: 2.4744\n",
            "108/108 - 6s - loss: 2.3295 - val_loss: 2.4763\n",
            "108/108 - 6s - loss: 2.3412 - val_loss: 2.4839\n",
            "108/108 - 6s - loss: 2.3037 - val_loss: 2.4664\n",
            "108/108 - 6s - loss: 2.3281 - val_loss: 2.4690\n",
            "108/108 - 6s - loss: 2.3109 - val_loss: 2.4821\n",
            "108/108 - 6s - loss: 2.3215 - val_loss: 2.4798\n",
            "108/108 - 6s - loss: 2.2961 - val_loss: 2.4671\n",
            "108/108 - 6s - loss: 2.3080 - val_loss: 2.4915\n",
            "108/108 - 6s - loss: 2.3137 - val_loss: 2.4697\n",
            "108/108 - 6s - loss: 2.3091 - val_loss: 2.4867\n",
            "108/108 - 6s - loss: 2.3033 - val_loss: 2.4580\n",
            "108/108 - 6s - loss: 2.3037 - val_loss: 2.4854\n",
            "108/108 - 6s - loss: 2.3313 - val_loss: 2.4703\n",
            "108/108 - 6s - loss: 2.3078 - val_loss: 2.4686\n",
            "108/108 - 6s - loss: 2.3306 - val_loss: 2.4599\n",
            "108/108 - 6s - loss: 2.2738 - val_loss: 2.4673\n",
            "108/108 - 6s - loss: 2.2791 - val_loss: 2.4758\n",
            "108/108 - 6s - loss: 2.2821 - val_loss: 2.4497\n",
            "108/108 - 6s - loss: 2.2778 - val_loss: 2.4468\n",
            "108/108 - 6s - loss: 2.2606 - val_loss: 2.4403\n",
            "108/108 - 6s - loss: 2.2856 - val_loss: 2.4523\n",
            "108/108 - 6s - loss: 2.2627 - val_loss: 2.4637\n",
            "108/108 - 6s - loss: 2.2646 - val_loss: 2.4605\n",
            "108/108 - 6s - loss: 2.2755 - val_loss: 2.4637\n",
            "108/108 - 6s - loss: 2.2785 - val_loss: 2.4572\n",
            "108/108 - 6s - loss: 2.2638 - val_loss: 2.4551\n",
            "108/108 - 6s - loss: 2.2654 - val_loss: 2.4497\n",
            "108/108 - 6s - loss: 2.2648 - val_loss: 2.4604\n",
            "108/108 - 6s - loss: 2.2663 - val_loss: 2.4587\n",
            "108/108 - 6s - loss: 2.2596 - val_loss: 2.5016\n",
            "108/108 - 6s - loss: 2.2553 - val_loss: 2.4544\n",
            "108/108 - 6s - loss: 2.2559 - val_loss: 2.4503\n",
            "108/108 - 6s - loss: 2.2864 - val_loss: 2.4781\n",
            "108/108 - 6s - loss: 2.2613 - val_loss: 2.4718\n",
            "108/108 - 6s - loss: 2.2548 - val_loss: 2.4522\n",
            "108/108 - 6s - loss: 2.2655 - val_loss: 2.4964\n",
            "108/108 - 6s - loss: 2.2664 - val_loss: 2.4551\n",
            "108/108 - 6s - loss: 2.2557 - val_loss: 2.4688\n",
            "108/108 - 6s - loss: 2.2643 - val_loss: 2.4603\n",
            "108/108 - 6s - loss: 2.2634 - val_loss: 2.4363\n",
            "108/108 - 6s - loss: 2.2430 - val_loss: 2.4152\n",
            "108/108 - 6s - loss: 2.2223 - val_loss: 2.4456\n",
            "108/108 - 6s - loss: 2.2299 - val_loss: 2.4339\n",
            "108/108 - 6s - loss: 2.2334 - val_loss: 2.4594\n",
            "108/108 - 6s - loss: 2.2245 - val_loss: 2.4340\n",
            "108/108 - 6s - loss: 2.2387 - val_loss: 2.4372\n",
            "108/108 - 6s - loss: 2.2375 - val_loss: 2.4115\n",
            "108/108 - 6s - loss: 2.2169 - val_loss: 2.4425\n",
            "108/108 - 6s - loss: 2.2271 - val_loss: 2.4669\n",
            "108/108 - 6s - loss: 2.2149 - val_loss: 2.4507\n",
            "108/108 - 6s - loss: 2.2266 - val_loss: 2.4643\n",
            "108/108 - 6s - loss: 2.2515 - val_loss: 2.4541\n",
            "108/108 - 6s - loss: 2.2357 - val_loss: 2.4394\n",
            "108/108 - 6s - loss: 2.2444 - val_loss: 2.4377\n",
            "108/108 - 6s - loss: 2.2061 - val_loss: 2.4670\n",
            "108/108 - 6s - loss: 2.2108 - val_loss: 2.4562\n",
            "108/108 - 6s - loss: 2.2471 - val_loss: 2.4409\n",
            "108/108 - 6s - loss: 2.2209 - val_loss: 2.4416\n",
            "108/108 - 6s - loss: 2.2346 - val_loss: 2.4311\n",
            "108/108 - 6s - loss: 2.2422 - val_loss: 2.4045\n",
            "108/108 - 6s - loss: 2.2152 - val_loss: 2.4217\n",
            "108/108 - 6s - loss: 2.1893 - val_loss: 2.4379\n",
            "108/108 - 6s - loss: 2.2191 - val_loss: 2.4063\n",
            "108/108 - 6s - loss: 2.2016 - val_loss: 2.4072\n",
            "108/108 - 6s - loss: 2.2009 - val_loss: 2.4129\n",
            "108/108 - 6s - loss: 2.2043 - val_loss: 2.4114\n",
            "108/108 - 6s - loss: 2.1964 - val_loss: 2.4444\n",
            "108/108 - 6s - loss: 2.1930 - val_loss: 2.4210\n",
            "108/108 - 6s - loss: 2.1774 - val_loss: 2.4244\n",
            "108/108 - 6s - loss: 2.1751 - val_loss: 2.4012\n",
            "108/108 - 6s - loss: 2.2313 - val_loss: 2.4467\n",
            "108/108 - 6s - loss: 2.2048 - val_loss: 2.4292\n",
            "108/108 - 6s - loss: 2.1896 - val_loss: 2.4867\n",
            "108/108 - 6s - loss: 2.2119 - val_loss: 2.4343\n",
            "108/108 - 6s - loss: 2.1933 - val_loss: 2.4170\n",
            "108/108 - 6s - loss: 2.2086 - val_loss: 2.4342\n",
            "108/108 - 6s - loss: 2.1872 - val_loss: 2.4266\n",
            "108/108 - 6s - loss: 2.1648 - val_loss: 2.4265\n",
            "108/108 - 6s - loss: 2.1670 - val_loss: 2.4398\n",
            "108/108 - 6s - loss: 2.1742 - val_loss: 2.4359\n",
            "108/108 - 6s - loss: 2.1704 - val_loss: 2.4152\n",
            "108/108 - 6s - loss: 2.1929 - val_loss: 2.4495\n",
            "108/108 - 6s - loss: 2.2032 - val_loss: 2.4372\n",
            "108/108 - 6s - loss: 2.1214 - val_loss: 2.4059\n",
            "108/108 - 6s - loss: 2.2015 - val_loss: 2.3900\n",
            "108/108 - 6s - loss: 2.1824 - val_loss: 2.4224\n",
            "108/108 - 6s - loss: 2.1790 - val_loss: 2.4015\n",
            "108/108 - 6s - loss: 2.1843 - val_loss: 2.4190\n",
            "108/108 - 6s - loss: 2.1523 - val_loss: 2.4245\n",
            "108/108 - 6s - loss: 2.1520 - val_loss: 2.4301\n",
            "108/108 - 6s - loss: 2.1909 - val_loss: 2.4146\n",
            "108/108 - 6s - loss: 2.1746 - val_loss: 2.4399\n",
            "108/108 - 6s - loss: 2.1761 - val_loss: 2.4755\n",
            "108/108 - 6s - loss: 2.1647 - val_loss: 2.4533\n",
            "108/108 - 6s - loss: 2.1469 - val_loss: 2.4358\n",
            "108/108 - 6s - loss: 2.1259 - val_loss: 2.4538\n",
            "108/108 - 6s - loss: 2.1592 - val_loss: 2.4170\n",
            "108/108 - 6s - loss: 2.1539 - val_loss: 2.3994\n",
            "108/108 - 6s - loss: 2.1553 - val_loss: 2.4173\n",
            "108/108 - 6s - loss: 2.1247 - val_loss: 2.4382\n",
            "108/108 - 6s - loss: 2.1486 - val_loss: 2.4423\n",
            "108/108 - 6s - loss: 2.1563 - val_loss: 2.4412\n",
            "108/108 - 6s - loss: 2.1654 - val_loss: 2.4448\n",
            "108/108 - 6s - loss: 2.1339 - val_loss: 2.4402\n",
            "108/108 - 6s - loss: 2.1287 - val_loss: 2.4101\n",
            "108/108 - 6s - loss: 2.1582 - val_loss: 2.4345\n",
            "108/108 - 6s - loss: 2.1653 - val_loss: 2.4321\n",
            "108/108 - 6s - loss: 2.1526 - val_loss: 2.4377\n",
            "108/108 - 6s - loss: 2.1208 - val_loss: 2.4510\n",
            "108/108 - 6s - loss: 2.1395 - val_loss: 2.4473\n",
            "108/108 - 6s - loss: 2.1316 - val_loss: 2.4409\n",
            "108/108 - 6s - loss: 2.1435 - val_loss: 2.4341\n",
            "108/108 - 6s - loss: 2.1330 - val_loss: 2.4152\n",
            "108/108 - 6s - loss: 2.1290 - val_loss: 2.4369\n",
            "108/108 - 6s - loss: 2.1179 - val_loss: 2.4315\n",
            "108/108 - 6s - loss: 2.1278 - val_loss: 2.4580\n",
            "108/108 - 6s - loss: 2.0996 - val_loss: 2.4507\n",
            "108/108 - 6s - loss: 2.0910 - val_loss: 2.4513\n",
            "108/108 - 6s - loss: 2.1273 - val_loss: 2.4515\n",
            "108/108 - 6s - loss: 2.1224 - val_loss: 2.4678\n",
            "108/108 - 6s - loss: 2.1066 - val_loss: 2.4262\n",
            "108/108 - 6s - loss: 2.1157 - val_loss: 2.4277\n",
            "108/108 - 6s - loss: 2.1166 - val_loss: 2.4450\n",
            "108/108 - 6s - loss: 2.1178 - val_loss: 2.4158\n",
            "108/108 - 6s - loss: 2.1173 - val_loss: 2.4538\n",
            "108/108 - 6s - loss: 2.1459 - val_loss: 2.4432\n",
            "108/108 - 6s - loss: 2.1388 - val_loss: 2.4195\n",
            "108/108 - 6s - loss: 2.0899 - val_loss: 2.4659\n",
            "108/108 - 6s - loss: 2.1214 - val_loss: 2.4642\n",
            "108/108 - 6s - loss: 2.1452 - val_loss: 2.4030\n",
            "108/108 - 6s - loss: 2.0947 - val_loss: 2.4167\n",
            "108/108 - 6s - loss: 2.1051 - val_loss: 2.4112\n",
            "108/108 - 6s - loss: 2.0971 - val_loss: 2.4049\n",
            "108/108 - 6s - loss: 2.0783 - val_loss: 2.4337\n",
            "108/108 - 6s - loss: 2.1103 - val_loss: 2.4383\n",
            "108/108 - 6s - loss: 2.0949 - val_loss: 2.4278\n",
            "108/108 - 6s - loss: 2.0754 - val_loss: 2.4543\n",
            "108/108 - 6s - loss: 2.0737 - val_loss: 2.4453\n",
            "108/108 - 6s - loss: 2.0862 - val_loss: 2.3914\n",
            "108/108 - 6s - loss: 2.1009 - val_loss: 2.4165\n",
            "108/108 - 6s - loss: 2.1149 - val_loss: 2.3813\n",
            "108/108 - 6s - loss: 2.0939 - val_loss: 2.4383\n",
            "108/108 - 6s - loss: 2.0888 - val_loss: 2.4182\n",
            "108/108 - 6s - loss: 2.0757 - val_loss: 2.4347\n",
            "108/108 - 6s - loss: 2.0732 - val_loss: 2.4412\n",
            "108/108 - 6s - loss: 2.0715 - val_loss: 2.4223\n",
            "108/108 - 6s - loss: 2.0559 - val_loss: 2.4456\n",
            "108/108 - 6s - loss: 2.0711 - val_loss: 2.4412\n",
            "108/108 - 6s - loss: 2.0631 - val_loss: 2.4744\n",
            "108/108 - 6s - loss: 2.0735 - val_loss: 2.4268\n",
            "108/108 - 6s - loss: 2.0635 - val_loss: 2.4339\n",
            "108/108 - 6s - loss: 2.0753 - val_loss: 2.4263\n",
            "108/108 - 6s - loss: 2.0827 - val_loss: 2.3972\n",
            "108/108 - 6s - loss: 2.0792 - val_loss: 2.4104\n",
            "108/108 - 6s - loss: 2.0808 - val_loss: 2.4214\n",
            "108/108 - 6s - loss: 2.0596 - val_loss: 2.4146\n",
            "108/108 - 6s - loss: 2.0424 - val_loss: 2.4273\n",
            "108/108 - 6s - loss: 2.0503 - val_loss: 2.4280\n",
            "108/108 - 6s - loss: 2.0768 - val_loss: 2.4183\n",
            "108/108 - 6s - loss: 2.0546 - val_loss: 2.4141\n",
            "108/108 - 6s - loss: 2.0362 - val_loss: 2.4125\n",
            "108/108 - 6s - loss: 2.0441 - val_loss: 2.4422\n",
            "108/108 - 6s - loss: 2.0531 - val_loss: 2.4641\n",
            "108/108 - 6s - loss: 2.0563 - val_loss: 2.3918\n",
            "108/108 - 6s - loss: 2.0479 - val_loss: 2.4300\n",
            "108/108 - 6s - loss: 2.0652 - val_loss: 2.4113\n",
            "108/108 - 6s - loss: 2.0815 - val_loss: 2.3992\n",
            "108/108 - 6s - loss: 2.0614 - val_loss: 2.4472\n",
            "108/108 - 6s - loss: 2.0550 - val_loss: 2.4299\n",
            "108/108 - 6s - loss: 2.0553 - val_loss: 2.4088\n",
            "108/108 - 6s - loss: 2.0735 - val_loss: 2.4027\n",
            "108/108 - 6s - loss: 2.0338 - val_loss: 2.4313\n",
            "108/108 - 6s - loss: 2.0691 - val_loss: 2.4141\n",
            "108/108 - 6s - loss: 2.0618 - val_loss: 2.4054\n",
            "108/108 - 6s - loss: 2.0496 - val_loss: 2.3996\n",
            "108/108 - 6s - loss: 2.0589 - val_loss: 2.3556\n",
            "108/108 - 6s - loss: 1.9936 - val_loss: 2.4004\n",
            "108/108 - 6s - loss: 2.0583 - val_loss: 2.4188\n",
            "108/108 - 6s - loss: 2.0285 - val_loss: 2.4174\n",
            "108/108 - 6s - loss: 2.0374 - val_loss: 2.4093\n",
            "108/108 - 6s - loss: 2.0221 - val_loss: 2.4237\n",
            "108/108 - 6s - loss: 2.0296 - val_loss: 2.4269\n",
            "108/108 - 6s - loss: 2.0624 - val_loss: 2.4310\n",
            "108/108 - 6s - loss: 2.0213 - val_loss: 2.4079\n",
            "108/108 - 6s - loss: 2.0199 - val_loss: 2.3984\n",
            "108/108 - 6s - loss: 2.0136 - val_loss: 2.4158\n",
            "108/108 - 6s - loss: 2.0207 - val_loss: 2.4121\n",
            "108/108 - 6s - loss: 2.0438 - val_loss: 2.3814\n",
            "108/108 - 6s - loss: 2.0096 - val_loss: 2.4152\n",
            "108/108 - 6s - loss: 2.0286 - val_loss: 2.4233\n",
            "108/108 - 6s - loss: 2.0433 - val_loss: 2.4183\n",
            "108/108 - 6s - loss: 2.0286 - val_loss: 2.4752\n",
            "108/108 - 6s - loss: 2.0398 - val_loss: 2.4372\n",
            "108/108 - 6s - loss: 2.0353 - val_loss: 2.4492\n",
            "108/108 - 6s - loss: 2.0267 - val_loss: 2.3906\n",
            "108/108 - 6s - loss: 2.0445 - val_loss: 2.3792\n",
            "108/108 - 6s - loss: 2.0177 - val_loss: 2.3973\n",
            "108/108 - 6s - loss: 2.0197 - val_loss: 2.4195\n",
            "108/108 - 6s - loss: 2.0184 - val_loss: 2.3695\n",
            "108/108 - 6s - loss: 2.0381 - val_loss: 2.4246\n",
            "108/108 - 6s - loss: 2.0094 - val_loss: 2.4268\n",
            "108/108 - 6s - loss: 2.0153 - val_loss: 2.4695\n",
            "108/108 - 6s - loss: 2.0099 - val_loss: 2.4420\n",
            "108/108 - 6s - loss: 2.0237 - val_loss: 2.4331\n",
            "108/108 - 6s - loss: 2.0267 - val_loss: 2.4408\n",
            "108/108 - 6s - loss: 2.0164 - val_loss: 2.4227\n",
            "108/108 - 6s - loss: 2.0468 - val_loss: 2.4537\n",
            "108/108 - 6s - loss: 2.0181 - val_loss: 2.4136\n",
            "108/108 - 6s - loss: 2.0021 - val_loss: 2.4364\n",
            "108/108 - 6s - loss: 2.0339 - val_loss: 2.4229\n",
            "108/108 - 6s - loss: 2.0078 - val_loss: 2.4291\n",
            "108/108 - 6s - loss: 2.0269 - val_loss: 2.4600\n",
            "108/108 - 6s - loss: 2.0342 - val_loss: 2.4579\n",
            "108/108 - 6s - loss: 1.9987 - val_loss: 2.4366\n",
            "108/108 - 6s - loss: 2.0201 - val_loss: 2.4345\n",
            "108/108 - 6s - loss: 2.0200 - val_loss: 2.4123\n",
            "108/108 - 6s - loss: 2.0130 - val_loss: 2.4305\n",
            "108/108 - 6s - loss: 1.9810 - val_loss: 2.3850\n",
            "108/108 - 6s - loss: 1.9899 - val_loss: 2.4080\n",
            "108/108 - 6s - loss: 2.0085 - val_loss: 2.4543\n",
            "108/108 - 6s - loss: 2.0148 - val_loss: 2.4708\n",
            "108/108 - 6s - loss: 2.0312 - val_loss: 2.4817\n",
            "108/108 - 6s - loss: 2.0114 - val_loss: 2.4904\n",
            "108/108 - 6s - loss: 2.0035 - val_loss: 2.4762\n",
            "108/108 - 6s - loss: 2.0222 - val_loss: 2.4364\n",
            "108/108 - 6s - loss: 1.9814 - val_loss: 2.4413\n",
            "108/108 - 6s - loss: 1.9797 - val_loss: 2.4338\n",
            "108/108 - 6s - loss: 2.0044 - val_loss: 2.4083\n",
            "108/108 - 6s - loss: 2.0030 - val_loss: 2.3441\n",
            "108/108 - 6s - loss: 2.0226 - val_loss: 2.3835\n",
            "108/108 - 6s - loss: 1.9956 - val_loss: 2.4547\n",
            "108/108 - 6s - loss: 1.9960 - val_loss: 2.3974\n",
            "108/108 - 6s - loss: 1.9911 - val_loss: 2.3965\n",
            "108/108 - 6s - loss: 2.0347 - val_loss: 2.4803\n",
            "108/108 - 6s - loss: 1.9978 - val_loss: 2.3827\n",
            "108/108 - 6s - loss: 2.0077 - val_loss: 2.4416\n",
            "108/108 - 6s - loss: 1.9881 - val_loss: 2.4648\n",
            "108/108 - 6s - loss: 1.9639 - val_loss: 2.4201\n",
            "108/108 - 6s - loss: 1.9700 - val_loss: 2.4063\n",
            "108/108 - 6s - loss: 1.9414 - val_loss: 2.4248\n",
            "108/108 - 6s - loss: 1.9664 - val_loss: 2.4070\n",
            "108/108 - 6s - loss: 1.9874 - val_loss: 2.4607\n",
            "108/108 - 6s - loss: 1.9960 - val_loss: 2.4156\n",
            "108/108 - 6s - loss: 1.9606 - val_loss: 2.4515\n",
            "108/108 - 6s - loss: 2.0074 - val_loss: 2.4783\n",
            "108/108 - 6s - loss: 2.0227 - val_loss: 2.4484\n",
            "108/108 - 6s - loss: 1.9819 - val_loss: 2.4789\n",
            "108/108 - 6s - loss: 2.0379 - val_loss: 2.3971\n",
            "108/108 - 6s - loss: 2.0103 - val_loss: 2.4227\n",
            "108/108 - 6s - loss: 1.9945 - val_loss: 2.4101\n",
            "108/108 - 6s - loss: 1.9738 - val_loss: 2.4303\n",
            "108/108 - 6s - loss: 1.9995 - val_loss: 2.4005\n",
            "108/108 - 6s - loss: 1.9731 - val_loss: 2.4303\n",
            "108/108 - 6s - loss: 1.9850 - val_loss: 2.4532\n",
            "108/108 - 6s - loss: 2.0205 - val_loss: 2.4113\n",
            "108/108 - 6s - loss: 1.9911 - val_loss: 2.4171\n",
            "108/108 - 6s - loss: 1.9673 - val_loss: 2.4136\n",
            "108/108 - 6s - loss: 1.9614 - val_loss: 2.4423\n",
            "108/108 - 6s - loss: 1.9597 - val_loss: 2.4030\n",
            "108/108 - 6s - loss: 1.9696 - val_loss: 2.4274\n",
            "108/108 - 6s - loss: 1.9531 - val_loss: 2.4414\n",
            "108/108 - 6s - loss: 1.9519 - val_loss: 2.4456\n",
            "108/108 - 6s - loss: 1.9382 - val_loss: 2.4459\n",
            "108/108 - 6s - loss: 1.9755 - val_loss: 2.4619\n",
            "108/108 - 6s - loss: 1.9668 - val_loss: 2.4368\n",
            "108/108 - 6s - loss: 1.9427 - val_loss: 2.3919\n",
            "108/108 - 6s - loss: 2.0156 - val_loss: 2.3644\n",
            "0.30000000000000004\n",
            "108/108 - 13s - loss: 5.3019 - val_loss: 5.3126\n",
            "108/108 - 6s - loss: 5.0042 - val_loss: 5.0346\n",
            "108/108 - 6s - loss: 4.7726 - val_loss: 4.8463\n",
            "108/108 - 6s - loss: 4.5916 - val_loss: 4.6894\n",
            "108/108 - 6s - loss: 4.4520 - val_loss: 4.5559\n",
            "108/108 - 6s - loss: 4.3322 - val_loss: 4.4384\n",
            "108/108 - 6s - loss: 4.2214 - val_loss: 4.3324\n",
            "108/108 - 6s - loss: 4.1257 - val_loss: 4.2374\n",
            "108/108 - 6s - loss: 4.0428 - val_loss: 4.1522\n",
            "108/108 - 6s - loss: 3.9575 - val_loss: 4.0741\n",
            "108/108 - 6s - loss: 3.8911 - val_loss: 4.0042\n",
            "108/108 - 6s - loss: 3.8267 - val_loss: 3.9395\n",
            "108/108 - 6s - loss: 3.7800 - val_loss: 3.8830\n",
            "108/108 - 6s - loss: 3.7188 - val_loss: 3.8313\n",
            "108/108 - 6s - loss: 3.6807 - val_loss: 3.7841\n",
            "108/108 - 6s - loss: 3.6513 - val_loss: 3.7424\n",
            "108/108 - 6s - loss: 3.6040 - val_loss: 3.7043\n",
            "108/108 - 6s - loss: 3.5863 - val_loss: 3.6713\n",
            "108/108 - 6s - loss: 3.5409 - val_loss: 3.6409\n",
            "108/108 - 6s - loss: 3.5267 - val_loss: 3.6140\n",
            "108/108 - 6s - loss: 3.5293 - val_loss: 3.5900\n",
            "108/108 - 6s - loss: 3.4954 - val_loss: 3.5684\n",
            "108/108 - 6s - loss: 3.4832 - val_loss: 3.5499\n",
            "108/108 - 6s - loss: 3.4532 - val_loss: 3.5311\n",
            "108/108 - 6s - loss: 3.4353 - val_loss: 3.5153\n",
            "108/108 - 6s - loss: 3.4272 - val_loss: 3.5005\n",
            "108/108 - 6s - loss: 3.3973 - val_loss: 3.4877\n",
            "108/108 - 6s - loss: 3.4152 - val_loss: 3.4783\n",
            "108/108 - 6s - loss: 3.4396 - val_loss: 3.4706\n",
            "108/108 - 6s - loss: 3.4289 - val_loss: 3.4621\n",
            "108/108 - 6s - loss: 3.4188 - val_loss: 3.4554\n",
            "108/108 - 6s - loss: 3.4151 - val_loss: 3.4497\n",
            "108/108 - 6s - loss: 3.4012 - val_loss: 3.4442\n",
            "108/108 - 6s - loss: 3.4209 - val_loss: 3.4396\n",
            "108/108 - 6s - loss: 3.4016 - val_loss: 3.4355\n",
            "108/108 - 6s - loss: 3.3822 - val_loss: 3.4309\n",
            "108/108 - 6s - loss: 3.4055 - val_loss: 3.4280\n",
            "108/108 - 6s - loss: 3.4026 - val_loss: 3.4262\n",
            "108/108 - 6s - loss: 3.4044 - val_loss: 3.4252\n",
            "108/108 - 6s - loss: 3.4149 - val_loss: 3.4245\n",
            "108/108 - 6s - loss: 3.4220 - val_loss: 3.4235\n",
            "108/108 - 6s - loss: 3.4257 - val_loss: 3.4221\n",
            "108/108 - 6s - loss: 3.4052 - val_loss: 3.4218\n",
            "108/108 - 6s - loss: 3.3868 - val_loss: 3.4199\n",
            "108/108 - 6s - loss: 3.3965 - val_loss: 3.4186\n",
            "108/108 - 6s - loss: 3.4081 - val_loss: 3.4185\n",
            "108/108 - 6s - loss: 3.3969 - val_loss: 3.4175\n",
            "108/108 - 6s - loss: 3.4299 - val_loss: 3.4173\n",
            "108/108 - 6s - loss: 3.4025 - val_loss: 3.4163\n",
            "108/108 - 6s - loss: 3.3755 - val_loss: 3.4148\n",
            "108/108 - 6s - loss: 3.4218 - val_loss: 3.4146\n",
            "108/108 - 6s - loss: 3.4040 - val_loss: 3.4146\n",
            "108/108 - 6s - loss: 3.4142 - val_loss: 3.4144\n",
            "108/108 - 6s - loss: 3.4181 - val_loss: 3.4146\n",
            "108/108 - 6s - loss: 3.4035 - val_loss: 3.4146\n",
            "108/108 - 6s - loss: 3.4146 - val_loss: 3.4153\n",
            "108/108 - 6s - loss: 3.4132 - val_loss: 3.4160\n",
            "108/108 - 6s - loss: 3.3966 - val_loss: 3.4140\n",
            "108/108 - 6s - loss: 3.3859 - val_loss: 3.4138\n",
            "108/108 - 6s - loss: 3.3751 - val_loss: 3.4138\n",
            "108/108 - 6s - loss: 3.3479 - val_loss: 3.4124\n",
            "108/108 - 6s - loss: 3.4186 - val_loss: 3.4131\n",
            "108/108 - 6s - loss: 3.3870 - val_loss: 3.4130\n",
            "108/108 - 6s - loss: 3.3967 - val_loss: 3.4139\n",
            "108/108 - 6s - loss: 3.4011 - val_loss: 3.4135\n",
            "108/108 - 6s - loss: 3.3938 - val_loss: 3.4126\n",
            "108/108 - 6s - loss: 3.3984 - val_loss: 3.4126\n",
            "108/108 - 6s - loss: 3.3923 - val_loss: 3.4122\n",
            "108/108 - 6s - loss: 3.4150 - val_loss: 3.4125\n",
            "108/108 - 6s - loss: 3.4219 - val_loss: 3.4122\n",
            "108/108 - 6s - loss: 3.4316 - val_loss: 3.4133\n",
            "108/108 - 6s - loss: 3.4101 - val_loss: 3.4124\n",
            "108/108 - 6s - loss: 3.4051 - val_loss: 3.4125\n",
            "108/108 - 6s - loss: 3.4010 - val_loss: 3.4124\n",
            "108/108 - 6s - loss: 3.4162 - val_loss: 3.4117\n",
            "108/108 - 6s - loss: 3.3931 - val_loss: 3.4118\n",
            "108/108 - 6s - loss: 3.4075 - val_loss: 3.4125\n",
            "108/108 - 6s - loss: 3.4132 - val_loss: 3.4131\n",
            "108/108 - 6s - loss: 3.4015 - val_loss: 3.4125\n",
            "108/108 - 6s - loss: 3.4010 - val_loss: 3.4121\n",
            "108/108 - 6s - loss: 3.3785 - val_loss: 3.4107\n",
            "108/108 - 6s - loss: 3.4012 - val_loss: 3.4118\n",
            "108/108 - 6s - loss: 3.3878 - val_loss: 3.4112\n",
            "108/108 - 6s - loss: 3.3826 - val_loss: 3.4117\n",
            "108/108 - 6s - loss: 3.4243 - val_loss: 3.4124\n",
            "108/108 - 6s - loss: 3.4087 - val_loss: 3.4121\n",
            "108/108 - 6s - loss: 3.4235 - val_loss: 3.4123\n",
            "108/108 - 6s - loss: 3.4226 - val_loss: 3.4133\n",
            "108/108 - 6s - loss: 3.3848 - val_loss: 3.4120\n",
            "108/108 - 6s - loss: 3.4257 - val_loss: 3.4133\n",
            "108/108 - 6s - loss: 3.4129 - val_loss: 3.4136\n",
            "108/108 - 6s - loss: 3.4034 - val_loss: 3.4142\n",
            "108/108 - 6s - loss: 3.3772 - val_loss: 3.4137\n",
            "108/108 - 6s - loss: 3.4313 - val_loss: 3.4134\n",
            "108/108 - 6s - loss: 3.3723 - val_loss: 3.4122\n",
            "108/108 - 6s - loss: 3.4250 - val_loss: 3.4131\n",
            "108/108 - 6s - loss: 3.3944 - val_loss: 3.4130\n",
            "108/108 - 6s - loss: 3.3994 - val_loss: 3.4126\n",
            "108/108 - 6s - loss: 3.4105 - val_loss: 3.4130\n",
            "108/108 - 6s - loss: 3.3778 - val_loss: 3.4126\n",
            "108/108 - 6s - loss: 3.4314 - val_loss: 3.4128\n",
            "108/108 - 6s - loss: 3.4127 - val_loss: 3.4134\n",
            "108/108 - 6s - loss: 3.4038 - val_loss: 3.4135\n",
            "108/108 - 6s - loss: 3.3822 - val_loss: 3.4134\n",
            "108/108 - 6s - loss: 3.3857 - val_loss: 3.4134\n",
            "108/108 - 6s - loss: 3.4114 - val_loss: 3.4136\n",
            "108/108 - 6s - loss: 3.3919 - val_loss: 3.4134\n",
            "108/108 - 6s - loss: 3.3901 - val_loss: 3.4128\n",
            "108/108 - 6s - loss: 3.4246 - val_loss: 3.4127\n",
            "108/108 - 6s - loss: 3.4147 - val_loss: 3.4121\n",
            "108/108 - 6s - loss: 3.5027 - val_loss: 3.5380\n",
            "108/108 - 6s - loss: 3.4346 - val_loss: 3.4803\n",
            "108/108 - 6s - loss: 3.4066 - val_loss: 3.4697\n",
            "108/108 - 6s - loss: 3.4284 - val_loss: 3.4630\n",
            "108/108 - 6s - loss: 3.4091 - val_loss: 3.4572\n",
            "108/108 - 6s - loss: 3.4336 - val_loss: 3.4516\n",
            "108/108 - 6s - loss: 3.4255 - val_loss: 3.4466\n",
            "108/108 - 6s - loss: 3.3937 - val_loss: 3.4410\n",
            "108/108 - 6s - loss: 3.4066 - val_loss: 3.4374\n",
            "108/108 - 6s - loss: 3.4312 - val_loss: 3.4354\n",
            "108/108 - 6s - loss: 3.3876 - val_loss: 3.4312\n",
            "108/108 - 6s - loss: 3.3984 - val_loss: 3.4293\n",
            "108/108 - 6s - loss: 3.4317 - val_loss: 3.4282\n",
            "108/108 - 6s - loss: 3.3821 - val_loss: 3.4261\n",
            "108/108 - 6s - loss: 3.4158 - val_loss: 3.4245\n",
            "108/108 - 6s - loss: 3.4235 - val_loss: 3.4233\n",
            "108/108 - 6s - loss: 3.4322 - val_loss: 3.4229\n",
            "108/108 - 6s - loss: 3.4082 - val_loss: 3.4216\n",
            "108/108 - 6s - loss: 3.4034 - val_loss: 3.4207\n",
            "108/108 - 6s - loss: 3.4245 - val_loss: 3.4204\n",
            "108/108 - 6s - loss: 3.4140 - val_loss: 3.4202\n",
            "108/108 - 6s - loss: 3.3931 - val_loss: 3.4196\n",
            "108/108 - 6s - loss: 3.4405 - val_loss: 3.4209\n",
            "108/108 - 6s - loss: 3.4063 - val_loss: 3.4198\n",
            "108/108 - 6s - loss: 3.4131 - val_loss: 3.4204\n",
            "108/108 - 6s - loss: 3.3944 - val_loss: 3.4195\n",
            "108/108 - 6s - loss: 3.4187 - val_loss: 3.4195\n",
            "108/108 - 6s - loss: 3.3982 - val_loss: 3.4180\n",
            "108/108 - 6s - loss: 3.4089 - val_loss: 3.4180\n",
            "108/108 - 6s - loss: 3.4039 - val_loss: 3.4174\n",
            "108/108 - 6s - loss: 3.4151 - val_loss: 3.4164\n",
            "108/108 - 6s - loss: 3.4138 - val_loss: 3.4161\n",
            "108/108 - 6s - loss: 3.4297 - val_loss: 3.4167\n",
            "108/108 - 6s - loss: 3.4302 - val_loss: 3.4178\n",
            "108/108 - 6s - loss: 3.4042 - val_loss: 3.4177\n",
            "108/108 - 6s - loss: 3.4228 - val_loss: 3.4176\n",
            "108/108 - 6s - loss: 3.3886 - val_loss: 3.4163\n",
            "108/108 - 6s - loss: 3.4307 - val_loss: 3.4166\n",
            "108/108 - 6s - loss: 3.4389 - val_loss: 3.4170\n",
            "108/108 - 6s - loss: 3.4023 - val_loss: 3.4163\n",
            "108/108 - 6s - loss: 3.3987 - val_loss: 3.4160\n",
            "108/108 - 6s - loss: 3.4089 - val_loss: 3.4158\n",
            "108/108 - 6s - loss: 3.3834 - val_loss: 3.4156\n",
            "108/108 - 6s - loss: 3.4174 - val_loss: 3.4155\n",
            "108/108 - 6s - loss: 3.4247 - val_loss: 3.4151\n",
            "108/108 - 6s - loss: 3.4218 - val_loss: 3.4162\n",
            "108/108 - 6s - loss: 3.4270 - val_loss: 3.4162\n",
            "108/108 - 6s - loss: 3.4201 - val_loss: 3.4159\n",
            "108/108 - 6s - loss: 3.4161 - val_loss: 3.4169\n",
            "108/108 - 6s - loss: 3.4192 - val_loss: 3.4169\n",
            "108/108 - 6s - loss: 3.4428 - val_loss: 3.4182\n",
            "108/108 - 6s - loss: 3.4229 - val_loss: 3.4184\n",
            "108/108 - 6s - loss: 3.4156 - val_loss: 3.4181\n",
            "108/108 - 6s - loss: 3.3886 - val_loss: 3.4178\n",
            "108/108 - 6s - loss: 3.3981 - val_loss: 3.4175\n",
            "108/108 - 6s - loss: 3.4202 - val_loss: 3.4171\n",
            "108/108 - 6s - loss: 3.4116 - val_loss: 3.4167\n",
            "108/108 - 6s - loss: 3.4098 - val_loss: 3.4164\n",
            "108/108 - 6s - loss: 3.4034 - val_loss: 3.4163\n",
            "108/108 - 6s - loss: 3.4015 - val_loss: 3.4158\n",
            "108/108 - 6s - loss: 3.3959 - val_loss: 3.4152\n",
            "108/108 - 6s - loss: 3.4017 - val_loss: 3.4147\n",
            "108/108 - 6s - loss: 3.4162 - val_loss: 3.4140\n",
            "108/108 - 6s - loss: 3.3897 - val_loss: 3.4137\n",
            "108/108 - 6s - loss: 3.4212 - val_loss: 3.4146\n",
            "108/108 - 6s - loss: 3.4060 - val_loss: 3.4149\n",
            "108/108 - 6s - loss: 3.4337 - val_loss: 3.4155\n",
            "108/108 - 6s - loss: 3.4095 - val_loss: 3.4152\n",
            "108/108 - 6s - loss: 3.4046 - val_loss: 3.4149\n",
            "108/108 - 6s - loss: 3.4282 - val_loss: 3.4153\n",
            "108/108 - 6s - loss: 3.4128 - val_loss: 3.4161\n",
            "108/108 - 6s - loss: 3.4137 - val_loss: 3.4156\n",
            "108/108 - 6s - loss: 3.4118 - val_loss: 3.4156\n",
            "108/108 - 6s - loss: 3.4037 - val_loss: 3.4158\n",
            "108/108 - 6s - loss: 3.4085 - val_loss: 3.4156\n",
            "108/108 - 6s - loss: 3.4266 - val_loss: 3.4156\n",
            "108/108 - 6s - loss: 3.3908 - val_loss: 3.4148\n",
            "108/108 - 6s - loss: 3.3980 - val_loss: 3.4147\n",
            "108/108 - 6s - loss: 3.3949 - val_loss: 3.4145\n",
            "108/108 - 6s - loss: 3.3867 - val_loss: 3.4141\n",
            "108/108 - 6s - loss: 3.4138 - val_loss: 3.4141\n",
            "108/108 - 6s - loss: 3.3908 - val_loss: 3.4135\n",
            "108/108 - 6s - loss: 3.3778 - val_loss: 3.4131\n",
            "108/108 - 6s - loss: 3.3853 - val_loss: 3.4124\n",
            "108/108 - 6s - loss: 3.4232 - val_loss: 3.4125\n",
            "108/108 - 6s - loss: 3.4073 - val_loss: 3.4124\n",
            "108/108 - 6s - loss: 3.4312 - val_loss: 3.4135\n",
            "108/108 - 6s - loss: 3.4210 - val_loss: 3.4134\n",
            "108/108 - 6s - loss: 3.4087 - val_loss: 3.4141\n",
            "108/108 - 6s - loss: 3.3906 - val_loss: 3.4143\n",
            "108/108 - 6s - loss: 3.3816 - val_loss: 3.4132\n",
            "108/108 - 6s - loss: 3.3968 - val_loss: 3.4135\n",
            "108/108 - 6s - loss: 3.3905 - val_loss: 3.4135\n",
            "108/108 - 6s - loss: 3.3792 - val_loss: 3.4131\n",
            "108/108 - 6s - loss: 3.3908 - val_loss: 3.4127\n",
            "108/108 - 6s - loss: 3.4165 - val_loss: 3.4134\n",
            "108/108 - 6s - loss: 3.4076 - val_loss: 3.4134\n",
            "108/108 - 6s - loss: 3.3995 - val_loss: 3.4128\n",
            "108/108 - 6s - loss: 3.3893 - val_loss: 3.4126\n",
            "108/108 - 6s - loss: 3.4190 - val_loss: 3.4127\n",
            "108/108 - 6s - loss: 3.4300 - val_loss: 3.4103\n",
            "108/108 - 6s - loss: 3.3971 - val_loss: 3.3974\n",
            "108/108 - 6s - loss: 3.3547 - val_loss: 3.3768\n",
            "108/108 - 6s - loss: 3.3333 - val_loss: 3.3691\n",
            "108/108 - 6s - loss: 3.2788 - val_loss: 3.2928\n",
            "108/108 - 6s - loss: 3.4056 - val_loss: 3.4312\n",
            "108/108 - 6s - loss: 3.3650 - val_loss: 3.3345\n",
            "108/108 - 6s - loss: 3.2906 - val_loss: 3.3125\n",
            "108/108 - 6s - loss: 3.2740 - val_loss: 3.3202\n",
            "108/108 - 6s - loss: 3.2679 - val_loss: 3.2852\n",
            "108/108 - 6s - loss: 3.2491 - val_loss: 3.2602\n",
            "108/108 - 6s - loss: 3.2264 - val_loss: 3.2540\n",
            "108/108 - 6s - loss: 3.2470 - val_loss: 3.2683\n",
            "108/108 - 6s - loss: 3.2184 - val_loss: 3.2663\n",
            "108/108 - 6s - loss: 3.2113 - val_loss: 3.2596\n",
            "108/108 - 6s - loss: 3.2287 - val_loss: 3.2260\n",
            "108/108 - 6s - loss: 3.1960 - val_loss: 3.2086\n",
            "108/108 - 6s - loss: 3.2001 - val_loss: 3.1981\n",
            "108/108 - 6s - loss: 3.1396 - val_loss: 3.1502\n",
            "108/108 - 6s - loss: 3.1148 - val_loss: 3.1482\n",
            "108/108 - 6s - loss: 3.1030 - val_loss: 3.1643\n",
            "108/108 - 6s - loss: 3.1157 - val_loss: 3.1553\n",
            "108/108 - 6s - loss: 3.1035 - val_loss: 3.1458\n",
            "108/108 - 6s - loss: 3.1057 - val_loss: 3.1665\n",
            "108/108 - 6s - loss: 3.0707 - val_loss: 3.2239\n",
            "108/108 - 6s - loss: 3.0656 - val_loss: 3.1394\n",
            "108/108 - 6s - loss: 3.0518 - val_loss: 3.0875\n",
            "108/108 - 6s - loss: 3.0520 - val_loss: 3.1092\n",
            "108/108 - 6s - loss: 3.0348 - val_loss: 3.0814\n",
            "108/108 - 6s - loss: 3.0564 - val_loss: 3.1025\n",
            "108/108 - 6s - loss: 3.0230 - val_loss: 3.0747\n",
            "108/108 - 6s - loss: 3.0343 - val_loss: 3.0704\n",
            "108/108 - 6s - loss: 2.9669 - val_loss: 3.0830\n",
            "108/108 - 6s - loss: 3.0406 - val_loss: 3.0737\n",
            "108/108 - 6s - loss: 2.9945 - val_loss: 3.0493\n",
            "108/108 - 6s - loss: 3.0076 - val_loss: 3.0414\n",
            "108/108 - 6s - loss: 2.9805 - val_loss: 3.0353\n",
            "108/108 - 6s - loss: 2.9642 - val_loss: 3.0301\n",
            "108/108 - 6s - loss: 2.9814 - val_loss: 3.0396\n",
            "108/108 - 6s - loss: 2.9292 - val_loss: 3.0139\n",
            "108/108 - 6s - loss: 2.9983 - val_loss: 3.0114\n",
            "108/108 - 6s - loss: 2.9709 - val_loss: 3.0494\n",
            "108/108 - 6s - loss: 2.9853 - val_loss: 3.0165\n",
            "108/108 - 6s - loss: 2.9711 - val_loss: 3.0335\n",
            "108/108 - 6s - loss: 2.9411 - val_loss: 3.0202\n",
            "108/108 - 6s - loss: 2.9364 - val_loss: 3.0243\n",
            "108/108 - 6s - loss: 2.9370 - val_loss: 3.0347\n",
            "108/108 - 6s - loss: 2.9331 - val_loss: 2.9762\n",
            "108/108 - 6s - loss: 2.9355 - val_loss: 2.9932\n",
            "108/108 - 6s - loss: 2.9157 - val_loss: 3.0256\n",
            "108/108 - 6s - loss: 2.9204 - val_loss: 3.0300\n",
            "108/108 - 6s - loss: 2.9133 - val_loss: 2.9974\n",
            "108/108 - 6s - loss: 2.9292 - val_loss: 3.0265\n",
            "108/108 - 6s - loss: 2.9233 - val_loss: 3.0450\n",
            "108/108 - 6s - loss: 2.9291 - val_loss: 2.9833\n",
            "108/108 - 6s - loss: 2.9062 - val_loss: 2.9983\n",
            "108/108 - 6s - loss: 2.9157 - val_loss: 2.9919\n",
            "108/108 - 6s - loss: 2.8831 - val_loss: 2.9955\n",
            "108/108 - 6s - loss: 2.9280 - val_loss: 2.9862\n",
            "108/108 - 6s - loss: 2.9007 - val_loss: 2.9785\n",
            "108/108 - 6s - loss: 2.8595 - val_loss: 2.9933\n",
            "108/108 - 6s - loss: 2.8955 - val_loss: 2.9749\n",
            "108/108 - 6s - loss: 2.8813 - val_loss: 2.9668\n",
            "108/108 - 6s - loss: 2.8893 - val_loss: 3.0023\n",
            "108/108 - 6s - loss: 2.8807 - val_loss: 2.9704\n",
            "108/108 - 6s - loss: 2.9035 - val_loss: 2.9869\n",
            "108/108 - 6s - loss: 2.8938 - val_loss: 2.9651\n",
            "108/108 - 6s - loss: 2.8937 - val_loss: 2.9757\n",
            "108/108 - 6s - loss: 2.8715 - val_loss: 3.0047\n",
            "108/108 - 6s - loss: 2.8687 - val_loss: 2.9642\n",
            "108/108 - 6s - loss: 2.8770 - val_loss: 2.9753\n",
            "108/108 - 6s - loss: 2.8655 - val_loss: 2.9804\n",
            "108/108 - 6s - loss: 2.8855 - val_loss: 2.9728\n",
            "108/108 - 6s - loss: 2.8730 - val_loss: 2.9722\n",
            "108/108 - 6s - loss: 2.9002 - val_loss: 2.9631\n",
            "108/108 - 6s - loss: 2.8220 - val_loss: 2.9628\n",
            "108/108 - 6s - loss: 2.8734 - val_loss: 2.9610\n",
            "108/108 - 6s - loss: 2.8463 - val_loss: 2.9843\n",
            "108/108 - 6s - loss: 2.8783 - val_loss: 2.9479\n",
            "108/108 - 6s - loss: 2.8769 - val_loss: 3.0053\n",
            "108/108 - 6s - loss: 2.8799 - val_loss: 2.9650\n",
            "108/108 - 6s - loss: 2.8373 - val_loss: 2.9442\n",
            "108/108 - 6s - loss: 2.8491 - val_loss: 2.9525\n",
            "108/108 - 6s - loss: 2.8788 - val_loss: 2.9657\n",
            "108/108 - 6s - loss: 2.8344 - val_loss: 2.9524\n",
            "108/108 - 6s - loss: 2.8308 - val_loss: 2.9741\n",
            "108/108 - 6s - loss: 2.8801 - val_loss: 2.9548\n",
            "108/108 - 6s - loss: 2.8338 - val_loss: 2.9624\n",
            "108/108 - 6s - loss: 2.8347 - val_loss: 2.9831\n",
            "108/108 - 6s - loss: 2.8265 - val_loss: 2.9844\n",
            "108/108 - 6s - loss: 2.8213 - val_loss: 2.9611\n",
            "108/108 - 6s - loss: 2.8489 - val_loss: 2.9894\n",
            "108/108 - 6s - loss: 2.8184 - val_loss: 2.9506\n",
            "108/108 - 6s - loss: 2.8304 - val_loss: 2.9855\n",
            "108/108 - 6s - loss: 2.8186 - val_loss: 2.9520\n",
            "108/108 - 6s - loss: 2.8162 - val_loss: 2.9626\n",
            "108/108 - 6s - loss: 2.8451 - val_loss: 2.9512\n",
            "108/108 - 6s - loss: 2.8626 - val_loss: 2.9655\n",
            "108/108 - 6s - loss: 2.8454 - val_loss: 2.9932\n",
            "108/108 - 6s - loss: 2.8478 - val_loss: 2.9787\n",
            "108/108 - 6s - loss: 2.8099 - val_loss: 2.9628\n",
            "108/108 - 6s - loss: 2.8213 - val_loss: 2.9669\n",
            "108/108 - 6s - loss: 2.8324 - val_loss: 2.9890\n",
            "108/108 - 6s - loss: 2.8268 - val_loss: 3.0048\n",
            "108/108 - 6s - loss: 2.7677 - val_loss: 2.9873\n",
            "108/108 - 6s - loss: 2.8212 - val_loss: 2.9995\n",
            "108/108 - 6s - loss: 2.8057 - val_loss: 2.9770\n",
            "108/108 - 6s - loss: 2.8556 - val_loss: 3.0081\n",
            "108/108 - 6s - loss: 2.8456 - val_loss: 2.9395\n",
            "108/108 - 6s - loss: 2.8013 - val_loss: 2.9497\n",
            "108/108 - 6s - loss: 2.7811 - val_loss: 2.9986\n",
            "108/108 - 6s - loss: 2.7848 - val_loss: 2.9870\n",
            "108/108 - 6s - loss: 2.8019 - val_loss: 2.9769\n",
            "108/108 - 6s - loss: 2.8137 - val_loss: 3.0114\n",
            "108/108 - 6s - loss: 2.8207 - val_loss: 2.9654\n",
            "108/108 - 6s - loss: 2.8015 - val_loss: 3.0233\n",
            "108/108 - 6s - loss: 2.8111 - val_loss: 2.9926\n",
            "108/108 - 6s - loss: 2.8096 - val_loss: 3.0316\n",
            "108/108 - 6s - loss: 2.7643 - val_loss: 3.0010\n",
            "108/108 - 6s - loss: 2.7800 - val_loss: 2.9711\n",
            "108/108 - 6s - loss: 2.7767 - val_loss: 2.9548\n",
            "108/108 - 6s - loss: 2.8060 - val_loss: 3.0131\n",
            "108/108 - 6s - loss: 2.7847 - val_loss: 2.9597\n",
            "108/108 - 6s - loss: 2.7879 - val_loss: 2.9638\n",
            "108/108 - 6s - loss: 2.7377 - val_loss: 3.0108\n",
            "108/108 - 6s - loss: 2.7606 - val_loss: 2.9878\n",
            "108/108 - 6s - loss: 2.7673 - val_loss: 2.9653\n",
            "108/108 - 6s - loss: 2.8079 - val_loss: 2.9819\n",
            "108/108 - 6s - loss: 2.7871 - val_loss: 2.9830\n",
            "108/108 - 6s - loss: 2.7896 - val_loss: 3.0152\n",
            "108/108 - 6s - loss: 2.7572 - val_loss: 2.9922\n",
            "108/108 - 6s - loss: 2.7747 - val_loss: 2.9596\n",
            "108/108 - 6s - loss: 2.7544 - val_loss: 2.9538\n",
            "108/108 - 6s - loss: 2.7521 - val_loss: 2.9974\n",
            "108/108 - 6s - loss: 2.7348 - val_loss: 2.9678\n",
            "108/108 - 6s - loss: 2.7930 - val_loss: 2.9710\n",
            "108/108 - 6s - loss: 2.7363 - val_loss: 2.9765\n",
            "108/108 - 6s - loss: 2.7605 - val_loss: 2.9812\n",
            "108/108 - 6s - loss: 2.7354 - val_loss: 3.0155\n",
            "108/108 - 6s - loss: 2.7728 - val_loss: 3.0280\n",
            "108/108 - 6s - loss: 2.7417 - val_loss: 3.0346\n",
            "108/108 - 6s - loss: 2.7540 - val_loss: 3.0941\n",
            "108/108 - 6s - loss: 2.7341 - val_loss: 3.0353\n",
            "108/108 - 6s - loss: 2.7682 - val_loss: 2.9929\n",
            "108/108 - 6s - loss: 2.7436 - val_loss: 2.9829\n",
            "108/108 - 6s - loss: 2.7500 - val_loss: 3.0190\n",
            "108/108 - 6s - loss: 2.7255 - val_loss: 3.0478\n",
            "108/108 - 6s - loss: 2.7345 - val_loss: 3.0283\n",
            "108/108 - 6s - loss: 2.7411 - val_loss: 3.0333\n",
            "108/108 - 6s - loss: 2.7513 - val_loss: 2.9600\n",
            "108/108 - 6s - loss: 2.7256 - val_loss: 3.0190\n",
            "108/108 - 6s - loss: 2.7219 - val_loss: 3.0212\n",
            "108/108 - 6s - loss: 2.6830 - val_loss: 3.0166\n",
            "108/108 - 6s - loss: 2.7382 - val_loss: 2.9730\n",
            "108/108 - 6s - loss: 2.7302 - val_loss: 2.9677\n",
            "108/108 - 6s - loss: 2.7223 - val_loss: 3.0180\n",
            "108/108 - 6s - loss: 2.7259 - val_loss: 3.0339\n",
            "108/108 - 6s - loss: 2.7266 - val_loss: 3.0336\n",
            "108/108 - 6s - loss: 2.7444 - val_loss: 3.0420\n",
            "108/108 - 6s - loss: 2.7045 - val_loss: 2.9954\n",
            "108/108 - 6s - loss: 2.7163 - val_loss: 3.0385\n",
            "108/108 - 6s - loss: 2.7036 - val_loss: 2.9874\n",
            "108/108 - 6s - loss: 2.6866 - val_loss: 3.0435\n",
            "108/108 - 6s - loss: 2.6860 - val_loss: 3.0111\n",
            "108/108 - 6s - loss: 2.7237 - val_loss: 2.9872\n",
            "108/108 - 6s - loss: 2.6946 - val_loss: 3.0114\n",
            "108/108 - 6s - loss: 2.7123 - val_loss: 2.9368\n",
            "108/108 - 6s - loss: 2.7475 - val_loss: 2.9612\n",
            "108/108 - 6s - loss: 2.7214 - val_loss: 3.0534\n",
            "108/108 - 6s - loss: 2.7116 - val_loss: 2.9822\n",
            "108/108 - 6s - loss: 2.7867 - val_loss: 2.9786\n",
            "108/108 - 6s - loss: 2.7141 - val_loss: 2.9746\n",
            "108/108 - 6s - loss: 2.6739 - val_loss: 2.9426\n",
            "108/108 - 6s - loss: 2.7230 - val_loss: 2.9842\n",
            "108/108 - 6s - loss: 2.7280 - val_loss: 2.9637\n",
            "108/108 - 6s - loss: 2.6831 - val_loss: 3.0315\n",
            "108/108 - 6s - loss: 2.6800 - val_loss: 2.9828\n",
            "108/108 - 6s - loss: 2.7194 - val_loss: 3.0107\n",
            "108/108 - 6s - loss: 2.6825 - val_loss: 2.9400\n",
            "108/108 - 6s - loss: 2.6961 - val_loss: 2.9854\n",
            "108/108 - 6s - loss: 2.7140 - val_loss: 2.9589\n",
            "108/108 - 6s - loss: 2.7204 - val_loss: 3.1020\n",
            "108/108 - 6s - loss: 2.7061 - val_loss: 2.9495\n",
            "108/108 - 6s - loss: 2.6743 - val_loss: 2.9693\n",
            "108/108 - 6s - loss: 2.6757 - val_loss: 2.9573\n",
            "108/108 - 6s - loss: 2.6930 - val_loss: 2.9811\n",
            "108/108 - 6s - loss: 2.6873 - val_loss: 3.0096\n",
            "108/108 - 6s - loss: 2.7057 - val_loss: 2.9435\n",
            "108/108 - 6s - loss: 2.6899 - val_loss: 2.9536\n",
            "108/108 - 6s - loss: 2.6629 - val_loss: 2.9147\n",
            "108/108 - 6s - loss: 2.6676 - val_loss: 3.0008\n",
            "108/108 - 6s - loss: 2.6792 - val_loss: 2.9594\n",
            "108/108 - 6s - loss: 2.6452 - val_loss: 2.9538\n",
            "108/108 - 6s - loss: 2.6499 - val_loss: 2.9743\n",
            "108/108 - 6s - loss: 2.6237 - val_loss: 2.9658\n",
            "108/108 - 6s - loss: 2.6889 - val_loss: 2.9438\n",
            "108/108 - 6s - loss: 2.6916 - val_loss: 3.0038\n",
            "108/108 - 6s - loss: 2.6948 - val_loss: 2.9045\n",
            "108/108 - 6s - loss: 2.7087 - val_loss: 2.9497\n",
            "108/108 - 6s - loss: 2.6878 - val_loss: 2.9298\n",
            "108/108 - 6s - loss: 2.6710 - val_loss: 2.9716\n",
            "108/108 - 6s - loss: 2.6885 - val_loss: 2.9628\n",
            "108/108 - 6s - loss: 2.6142 - val_loss: 3.0000\n",
            "108/108 - 6s - loss: 2.6224 - val_loss: 2.9971\n",
            "108/108 - 6s - loss: 2.6439 - val_loss: 3.0035\n",
            "108/108 - 6s - loss: 2.6510 - val_loss: 3.1260\n",
            "108/108 - 6s - loss: 2.7110 - val_loss: 3.0532\n",
            "108/108 - 6s - loss: 2.7033 - val_loss: 2.9846\n",
            "108/108 - 6s - loss: 2.6925 - val_loss: 3.0213\n",
            "108/108 - 6s - loss: 2.6656 - val_loss: 2.9795\n",
            "108/108 - 6s - loss: 2.6590 - val_loss: 3.0619\n",
            "108/108 - 6s - loss: 2.6173 - val_loss: 3.0102\n",
            "108/108 - 6s - loss: 2.6238 - val_loss: 3.0140\n",
            "108/108 - 6s - loss: 2.6369 - val_loss: 2.9967\n",
            "108/108 - 6s - loss: 2.6603 - val_loss: 3.0070\n",
            "108/108 - 6s - loss: 2.6343 - val_loss: 2.9658\n",
            "108/108 - 6s - loss: 2.5921 - val_loss: 2.9506\n",
            "108/108 - 6s - loss: 2.6340 - val_loss: 2.9866\n",
            "108/108 - 6s - loss: 2.6201 - val_loss: 2.9925\n",
            "108/108 - 6s - loss: 2.6482 - val_loss: 2.9920\n",
            "108/108 - 6s - loss: 2.6142 - val_loss: 2.9364\n",
            "108/108 - 6s - loss: 2.6038 - val_loss: 3.0248\n",
            "108/108 - 6s - loss: 2.6443 - val_loss: 2.9763\n",
            "108/108 - 6s - loss: 2.6350 - val_loss: 2.9608\n",
            "108/108 - 6s - loss: 2.6175 - val_loss: 2.9481\n",
            "108/108 - 6s - loss: 2.6324 - val_loss: 3.0004\n",
            "108/108 - 6s - loss: 2.6055 - val_loss: 2.9795\n",
            "108/108 - 6s - loss: 2.5842 - val_loss: 2.9816\n",
            "108/108 - 6s - loss: 2.6273 - val_loss: 2.9802\n",
            "108/108 - 6s - loss: 2.6079 - val_loss: 3.0333\n",
            "108/108 - 6s - loss: 2.6373 - val_loss: 3.0022\n",
            "108/108 - 6s - loss: 2.6315 - val_loss: 2.9699\n",
            "108/108 - 6s - loss: 2.6095 - val_loss: 3.1104\n",
            "108/108 - 6s - loss: 2.6520 - val_loss: 2.9851\n",
            "108/108 - 6s - loss: 2.6424 - val_loss: 2.9801\n",
            "108/108 - 6s - loss: 2.6188 - val_loss: 2.9553\n",
            "108/108 - 6s - loss: 2.6015 - val_loss: 2.9172\n",
            "108/108 - 6s - loss: 2.6220 - val_loss: 2.9216\n",
            "108/108 - 6s - loss: 2.5832 - val_loss: 2.9055\n",
            "108/108 - 6s - loss: 2.6337 - val_loss: 2.9725\n",
            "108/108 - 6s - loss: 2.5833 - val_loss: 2.9431\n",
            "108/108 - 6s - loss: 2.6089 - val_loss: 2.9961\n",
            "108/108 - 6s - loss: 2.6012 - val_loss: 2.9561\n",
            "108/108 - 6s - loss: 2.6193 - val_loss: 2.9717\n",
            "108/108 - 6s - loss: 2.5952 - val_loss: 2.9466\n",
            "108/108 - 6s - loss: 2.5973 - val_loss: 3.0322\n",
            "108/108 - 6s - loss: 2.6375 - val_loss: 2.9487\n",
            "108/108 - 6s - loss: 2.6179 - val_loss: 2.9160\n",
            "108/108 - 6s - loss: 2.5956 - val_loss: 2.9827\n",
            "108/108 - 6s - loss: 2.5851 - val_loss: 2.9384\n",
            "108/108 - 6s - loss: 2.5876 - val_loss: 2.9548\n",
            "108/108 - 6s - loss: 2.5909 - val_loss: 2.9719\n",
            "108/108 - 6s - loss: 2.5779 - val_loss: 2.9393\n",
            "108/108 - 6s - loss: 2.5733 - val_loss: 2.9514\n",
            "108/108 - 6s - loss: 2.5684 - val_loss: 2.9763\n",
            "108/108 - 6s - loss: 2.5998 - val_loss: 2.9819\n",
            "108/108 - 6s - loss: 2.6035 - val_loss: 2.9372\n",
            "108/108 - 6s - loss: 2.5703 - val_loss: 2.9595\n",
            "108/108 - 6s - loss: 2.5922 - val_loss: 2.9267\n",
            "108/108 - 6s - loss: 2.5938 - val_loss: 2.9379\n",
            "108/108 - 6s - loss: 2.5586 - val_loss: 2.9256\n",
            "108/108 - 6s - loss: 2.5496 - val_loss: 2.9171\n",
            "108/108 - 6s - loss: 2.5716 - val_loss: 2.9469\n",
            "108/108 - 6s - loss: 2.5784 - val_loss: 2.9526\n",
            "108/108 - 6s - loss: 2.5696 - val_loss: 3.0080\n",
            "108/108 - 6s - loss: 2.5698 - val_loss: 2.9033\n",
            "108/108 - 6s - loss: 2.5940 - val_loss: 2.9208\n",
            "108/108 - 6s - loss: 2.5746 - val_loss: 2.9862\n",
            "108/108 - 6s - loss: 2.5832 - val_loss: 2.9038\n",
            "108/108 - 6s - loss: 2.5703 - val_loss: 2.9317\n",
            "108/108 - 6s - loss: 2.5799 - val_loss: 3.0153\n",
            "108/108 - 6s - loss: 2.6331 - val_loss: 2.9034\n",
            "108/108 - 6s - loss: 2.5650 - val_loss: 2.9399\n",
            "108/108 - 6s - loss: 2.5754 - val_loss: 2.9607\n",
            "108/108 - 6s - loss: 2.5520 - val_loss: 2.9652\n",
            "108/108 - 6s - loss: 2.5748 - val_loss: 3.0305\n",
            "108/108 - 6s - loss: 2.5984 - val_loss: 3.0672\n",
            "108/108 - 6s - loss: 2.6018 - val_loss: 2.9475\n",
            "108/108 - 6s - loss: 2.5587 - val_loss: 2.9091\n",
            "108/108 - 6s - loss: 2.5532 - val_loss: 2.9902\n",
            "108/108 - 6s - loss: 2.5639 - val_loss: 2.9308\n",
            "108/108 - 6s - loss: 2.5439 - val_loss: 2.9564\n",
            "108/108 - 6s - loss: 2.5387 - val_loss: 2.9321\n",
            "108/108 - 6s - loss: 2.5709 - val_loss: 2.9338\n",
            "108/108 - 6s - loss: 2.5823 - val_loss: 2.9289\n",
            "108/108 - 6s - loss: 2.5367 - val_loss: 2.8938\n",
            "108/108 - 6s - loss: 2.5199 - val_loss: 2.8713\n",
            "108/108 - 6s - loss: 2.5454 - val_loss: 2.8889\n",
            "108/108 - 6s - loss: 2.5538 - val_loss: 2.8947\n",
            "108/108 - 6s - loss: 2.5793 - val_loss: 2.9171\n",
            "0.4\n",
            "108/108 - 13s - loss: 7.0387 - val_loss: 7.0264\n",
            "108/108 - 6s - loss: 6.6373 - val_loss: 6.6912\n",
            "108/108 - 6s - loss: 6.3351 - val_loss: 6.4334\n",
            "108/108 - 6s - loss: 6.0877 - val_loss: 6.2080\n",
            "108/108 - 6s - loss: 5.8741 - val_loss: 6.0083\n",
            "108/108 - 6s - loss: 5.6846 - val_loss: 5.8263\n",
            "108/108 - 6s - loss: 5.5108 - val_loss: 5.6590\n",
            "108/108 - 6s - loss: 5.3770 - val_loss: 5.5058\n",
            "108/108 - 6s - loss: 5.2245 - val_loss: 5.3651\n",
            "108/108 - 6s - loss: 5.0870 - val_loss: 5.2347\n",
            "108/108 - 6s - loss: 4.9784 - val_loss: 5.1126\n",
            "108/108 - 6s - loss: 4.8799 - val_loss: 5.0016\n",
            "108/108 - 6s - loss: 4.7460 - val_loss: 4.8983\n",
            "108/108 - 6s - loss: 4.6669 - val_loss: 4.8031\n",
            "108/108 - 6s - loss: 4.5927 - val_loss: 4.7154\n",
            "108/108 - 6s - loss: 4.5109 - val_loss: 4.6351\n",
            "108/108 - 6s - loss: 4.4326 - val_loss: 4.5611\n",
            "108/108 - 6s - loss: 4.3861 - val_loss: 4.4941\n",
            "108/108 - 6s - loss: 4.3069 - val_loss: 4.4297\n",
            "108/108 - 6s - loss: 4.3099 - val_loss: 4.3736\n",
            "108/108 - 6s - loss: 4.2367 - val_loss: 4.3204\n",
            "108/108 - 6s - loss: 4.1782 - val_loss: 4.2729\n",
            "108/108 - 6s - loss: 4.1201 - val_loss: 4.2277\n",
            "108/108 - 6s - loss: 4.0915 - val_loss: 4.1871\n",
            "108/108 - 6s - loss: 4.0959 - val_loss: 4.1525\n",
            "108/108 - 6s - loss: 4.0653 - val_loss: 4.1217\n",
            "108/108 - 6s - loss: 4.0651 - val_loss: 4.0965\n",
            "108/108 - 6s - loss: 4.0599 - val_loss: 4.0741\n",
            "108/108 - 6s - loss: 3.9951 - val_loss: 4.0528\n",
            "108/108 - 6s - loss: 4.0278 - val_loss: 4.0356\n",
            "108/108 - 6s - loss: 4.0135 - val_loss: 4.0199\n",
            "108/108 - 6s - loss: 3.9939 - val_loss: 4.0066\n",
            "108/108 - 6s - loss: 3.9773 - val_loss: 3.9965\n",
            "108/108 - 6s - loss: 4.0116 - val_loss: 3.9884\n",
            "108/108 - 6s - loss: 4.0019 - val_loss: 3.9797\n",
            "108/108 - 6s - loss: 4.0015 - val_loss: 3.9724\n",
            "108/108 - 6s - loss: 3.9816 - val_loss: 3.9671\n",
            "108/108 - 6s - loss: 3.9615 - val_loss: 3.9603\n",
            "108/108 - 6s - loss: 3.9626 - val_loss: 3.9556\n",
            "108/108 - 6s - loss: 3.9671 - val_loss: 3.9526\n",
            "108/108 - 6s - loss: 3.9559 - val_loss: 3.9490\n",
            "108/108 - 6s - loss: 3.9638 - val_loss: 3.9459\n",
            "108/108 - 6s - loss: 3.9480 - val_loss: 3.9438\n",
            "108/108 - 6s - loss: 3.9769 - val_loss: 3.9422\n",
            "108/108 - 6s - loss: 3.9721 - val_loss: 3.9404\n",
            "108/108 - 6s - loss: 3.9621 - val_loss: 3.9389\n",
            "108/108 - 6s - loss: 3.9789 - val_loss: 3.9370\n",
            "108/108 - 6s - loss: 3.9443 - val_loss: 3.9362\n",
            "108/108 - 6s - loss: 3.9381 - val_loss: 3.9349\n",
            "108/108 - 6s - loss: 3.9343 - val_loss: 3.9337\n",
            "108/108 - 6s - loss: 3.9642 - val_loss: 3.9334\n",
            "108/108 - 6s - loss: 3.9681 - val_loss: 3.9335\n",
            "108/108 - 6s - loss: 3.9545 - val_loss: 3.9326\n",
            "108/108 - 6s - loss: 3.9883 - val_loss: 3.9318\n",
            "108/108 - 6s - loss: 3.9474 - val_loss: 3.9317\n",
            "108/108 - 6s - loss: 3.9205 - val_loss: 3.9296\n",
            "108/108 - 6s - loss: 3.9793 - val_loss: 3.9301\n",
            "108/108 - 6s - loss: 3.9681 - val_loss: 3.9304\n",
            "108/108 - 6s - loss: 3.9712 - val_loss: 3.9303\n",
            "108/108 - 6s - loss: 3.9436 - val_loss: 3.9296\n",
            "108/108 - 6s - loss: 3.9929 - val_loss: 3.9305\n",
            "108/108 - 6s - loss: 3.9573 - val_loss: 3.9306\n",
            "108/108 - 6s - loss: 3.9934 - val_loss: 3.9308\n",
            "108/108 - 6s - loss: 3.9650 - val_loss: 3.9308\n",
            "108/108 - 6s - loss: 3.9584 - val_loss: 3.9296\n",
            "108/108 - 6s - loss: 3.9757 - val_loss: 3.9290\n",
            "108/108 - 6s - loss: 3.9452 - val_loss: 3.9284\n",
            "108/108 - 6s - loss: 3.9744 - val_loss: 3.9294\n",
            "108/108 - 6s - loss: 3.9372 - val_loss: 3.9288\n",
            "108/108 - 6s - loss: 3.9293 - val_loss: 3.9288\n",
            "108/108 - 6s - loss: 3.9625 - val_loss: 3.9295\n",
            "108/108 - 6s - loss: 3.9327 - val_loss: 3.9292\n",
            "108/108 - 6s - loss: 3.9616 - val_loss: 3.9294\n",
            "108/108 - 6s - loss: 3.9715 - val_loss: 3.9300\n",
            "108/108 - 6s - loss: 3.9524 - val_loss: 3.9306\n",
            "108/108 - 6s - loss: 3.9440 - val_loss: 3.9299\n",
            "108/108 - 6s - loss: 3.9937 - val_loss: 3.9349\n",
            "108/108 - 6s - loss: 3.9527 - val_loss: 3.9306\n",
            "108/108 - 6s - loss: 3.9656 - val_loss: 3.9333\n",
            "108/108 - 6s - loss: 3.9296 - val_loss: 3.9291\n",
            "108/108 - 6s - loss: 3.9449 - val_loss: 3.9277\n",
            "108/108 - 6s - loss: 3.9159 - val_loss: 3.9269\n",
            "108/108 - 6s - loss: 3.9970 - val_loss: 3.9280\n",
            "108/108 - 6s - loss: 3.9467 - val_loss: 3.9289\n",
            "108/108 - 6s - loss: 3.9378 - val_loss: 3.9313\n",
            "108/108 - 6s - loss: 3.9696 - val_loss: 3.9325\n",
            "108/108 - 6s - loss: 3.9629 - val_loss: 3.9246\n",
            "108/108 - 6s - loss: 3.8884 - val_loss: 3.8611\n",
            "108/108 - 6s - loss: 3.8590 - val_loss: 3.8347\n",
            "108/108 - 6s - loss: 3.8230 - val_loss: 3.7750\n",
            "108/108 - 6s - loss: 3.7488 - val_loss: 3.7480\n",
            "108/108 - 6s - loss: 3.7132 - val_loss: 3.7176\n",
            "108/108 - 6s - loss: 3.7400 - val_loss: 3.7238\n",
            "108/108 - 6s - loss: 3.6744 - val_loss: 3.7299\n",
            "108/108 - 6s - loss: 3.7217 - val_loss: 3.6653\n",
            "108/108 - 6s - loss: 3.6455 - val_loss: 3.6931\n",
            "108/108 - 6s - loss: 3.6804 - val_loss: 3.6239\n",
            "108/108 - 6s - loss: 3.5923 - val_loss: 3.5876\n",
            "108/108 - 6s - loss: 3.5453 - val_loss: 3.5640\n",
            "108/108 - 6s - loss: 3.5525 - val_loss: 3.5916\n",
            "108/108 - 6s - loss: 3.5458 - val_loss: 3.5926\n",
            "108/108 - 6s - loss: 3.5058 - val_loss: 3.5182\n",
            "108/108 - 6s - loss: 3.5125 - val_loss: 3.5284\n",
            "108/108 - 6s - loss: 3.4631 - val_loss: 3.4520\n",
            "108/108 - 6s - loss: 3.4546 - val_loss: 3.3935\n",
            "108/108 - 6s - loss: 3.4154 - val_loss: 3.4457\n",
            "108/108 - 6s - loss: 3.4129 - val_loss: 3.4262\n",
            "108/108 - 6s - loss: 3.4190 - val_loss: 3.4417\n",
            "108/108 - 6s - loss: 3.3889 - val_loss: 3.3679\n",
            "108/108 - 6s - loss: 3.3860 - val_loss: 3.3269\n",
            "108/108 - 6s - loss: 3.3745 - val_loss: 3.3416\n",
            "108/108 - 6s - loss: 3.3495 - val_loss: 3.3430\n",
            "108/108 - 6s - loss: 3.3297 - val_loss: 3.2647\n",
            "108/108 - 6s - loss: 3.3298 - val_loss: 3.2949\n",
            "108/108 - 6s - loss: 3.3116 - val_loss: 3.3085\n",
            "108/108 - 6s - loss: 3.2986 - val_loss: 3.2432\n",
            "108/108 - 6s - loss: 3.3169 - val_loss: 3.2397\n",
            "108/108 - 6s - loss: 3.2683 - val_loss: 3.2294\n",
            "108/108 - 6s - loss: 3.2462 - val_loss: 3.2228\n",
            "108/108 - 6s - loss: 3.2927 - val_loss: 3.2066\n",
            "108/108 - 6s - loss: 3.2877 - val_loss: 3.2129\n",
            "108/108 - 6s - loss: 3.2377 - val_loss: 3.2141\n",
            "108/108 - 6s - loss: 3.2483 - val_loss: 3.1757\n",
            "108/108 - 6s - loss: 3.2769 - val_loss: 3.1807\n",
            "108/108 - 6s - loss: 3.2435 - val_loss: 3.1818\n",
            "108/108 - 6s - loss: 3.2876 - val_loss: 3.1822\n",
            "108/108 - 6s - loss: 3.2543 - val_loss: 3.1513\n",
            "108/108 - 6s - loss: 3.2486 - val_loss: 3.1991\n",
            "108/108 - 6s - loss: 3.2494 - val_loss: 3.2207\n",
            "108/108 - 6s - loss: 3.1883 - val_loss: 3.1189\n",
            "108/108 - 6s - loss: 3.1962 - val_loss: 3.1753\n",
            "108/108 - 6s - loss: 3.2202 - val_loss: 3.2416\n",
            "108/108 - 6s - loss: 3.2048 - val_loss: 3.1702\n",
            "108/108 - 6s - loss: 3.1952 - val_loss: 3.1406\n",
            "108/108 - 6s - loss: 3.2093 - val_loss: 3.1256\n",
            "108/108 - 6s - loss: 3.1506 - val_loss: 3.1251\n",
            "108/108 - 6s - loss: 3.1564 - val_loss: 3.1068\n",
            "108/108 - 6s - loss: 3.2002 - val_loss: 3.1508\n",
            "108/108 - 6s - loss: 3.2323 - val_loss: 3.1243\n",
            "108/108 - 6s - loss: 3.1851 - val_loss: 3.2283\n",
            "108/108 - 6s - loss: 3.2033 - val_loss: 3.1396\n",
            "108/108 - 6s - loss: 3.2039 - val_loss: 3.1978\n",
            "108/108 - 6s - loss: 3.1771 - val_loss: 3.1065\n",
            "108/108 - 6s - loss: 3.1693 - val_loss: 3.1251\n",
            "108/108 - 6s - loss: 3.1534 - val_loss: 3.1196\n",
            "108/108 - 6s - loss: 3.1513 - val_loss: 3.1201\n",
            "108/108 - 6s - loss: 3.1018 - val_loss: 3.1583\n",
            "108/108 - 6s - loss: 3.1551 - val_loss: 3.1343\n",
            "108/108 - 6s - loss: 3.1753 - val_loss: 3.0961\n",
            "108/108 - 6s - loss: 3.1308 - val_loss: 3.1112\n",
            "108/108 - 6s - loss: 3.1325 - val_loss: 3.1469\n",
            "108/108 - 6s - loss: 3.1350 - val_loss: 3.0974\n",
            "108/108 - 6s - loss: 3.1049 - val_loss: 3.1055\n",
            "108/108 - 6s - loss: 3.1096 - val_loss: 3.0828\n",
            "108/108 - 6s - loss: 3.1400 - val_loss: 3.1123\n",
            "108/108 - 6s - loss: 3.1063 - val_loss: 3.1032\n",
            "108/108 - 6s - loss: 3.1106 - val_loss: 3.1079\n",
            "108/108 - 6s - loss: 3.1031 - val_loss: 3.1091\n",
            "108/108 - 6s - loss: 3.1099 - val_loss: 3.1257\n",
            "108/108 - 6s - loss: 3.0991 - val_loss: 3.0923\n",
            "108/108 - 6s - loss: 3.1120 - val_loss: 3.1827\n",
            "108/108 - 6s - loss: 3.0770 - val_loss: 3.1114\n",
            "108/108 - 6s - loss: 3.0890 - val_loss: 3.1422\n",
            "108/108 - 6s - loss: 3.1339 - val_loss: 3.0898\n",
            "108/108 - 6s - loss: 3.0825 - val_loss: 3.1445\n",
            "108/108 - 6s - loss: 3.0347 - val_loss: 3.1335\n",
            "108/108 - 6s - loss: 3.0787 - val_loss: 3.1498\n",
            "108/108 - 6s - loss: 3.0631 - val_loss: 3.1778\n",
            "108/108 - 6s - loss: 3.0683 - val_loss: 3.1788\n",
            "108/108 - 6s - loss: 3.0758 - val_loss: 3.1158\n",
            "108/108 - 6s - loss: 3.5256 - val_loss: 4.3173\n",
            "108/108 - 6s - loss: 3.9242 - val_loss: 3.2187\n",
            "108/108 - 6s - loss: 3.2869 - val_loss: 3.1043\n",
            "108/108 - 6s - loss: 3.2285 - val_loss: 3.1555\n",
            "108/108 - 6s - loss: 3.1839 - val_loss: 3.1561\n",
            "108/108 - 6s - loss: 3.1542 - val_loss: 3.1838\n",
            "108/108 - 6s - loss: 3.1344 - val_loss: 3.1756\n",
            "108/108 - 6s - loss: 3.1121 - val_loss: 3.2456\n",
            "108/108 - 6s - loss: 3.0969 - val_loss: 3.1062\n",
            "108/108 - 6s - loss: 3.0891 - val_loss: 3.1652\n",
            "108/108 - 6s - loss: 3.1327 - val_loss: 3.2336\n",
            "108/108 - 6s - loss: 3.1094 - val_loss: 3.1340\n",
            "108/108 - 6s - loss: 3.0951 - val_loss: 3.1522\n",
            "108/108 - 6s - loss: 3.1629 - val_loss: 3.1030\n",
            "108/108 - 6s - loss: 3.1109 - val_loss: 3.1166\n",
            "108/108 - 6s - loss: 3.1557 - val_loss: 3.1039\n",
            "108/108 - 6s - loss: 3.0964 - val_loss: 3.1056\n",
            "108/108 - 6s - loss: 3.0559 - val_loss: 3.0735\n",
            "108/108 - 6s - loss: 3.0871 - val_loss: 3.1632\n",
            "108/108 - 6s - loss: 3.0865 - val_loss: 3.1354\n",
            "108/108 - 6s - loss: 3.0756 - val_loss: 3.1210\n",
            "108/108 - 6s - loss: 3.0563 - val_loss: 3.0909\n",
            "108/108 - 6s - loss: 3.0752 - val_loss: 3.1376\n",
            "108/108 - 6s - loss: 3.1123 - val_loss: 3.1397\n",
            "108/108 - 6s - loss: 3.0611 - val_loss: 3.0597\n",
            "108/108 - 6s - loss: 3.0464 - val_loss: 3.0830\n",
            "108/108 - 6s - loss: 3.0447 - val_loss: 3.0579\n",
            "108/108 - 6s - loss: 3.0817 - val_loss: 3.0923\n",
            "108/108 - 6s - loss: 3.0536 - val_loss: 3.1255\n",
            "108/108 - 6s - loss: 3.0731 - val_loss: 3.0859\n",
            "108/108 - 6s - loss: 3.0298 - val_loss: 3.0492\n",
            "108/108 - 6s - loss: 3.0414 - val_loss: 3.0798\n",
            "108/108 - 6s - loss: 3.0615 - val_loss: 3.0616\n",
            "108/108 - 6s - loss: 3.0661 - val_loss: 3.0825\n",
            "108/108 - 6s - loss: 3.0685 - val_loss: 3.0841\n",
            "108/108 - 6s - loss: 3.0510 - val_loss: 3.0910\n",
            "108/108 - 6s - loss: 3.0625 - val_loss: 3.1163\n",
            "108/108 - 6s - loss: 3.0592 - val_loss: 3.0582\n",
            "108/108 - 6s - loss: 3.0724 - val_loss: 3.0766\n",
            "108/108 - 6s - loss: 3.0511 - val_loss: 3.1268\n",
            "108/108 - 6s - loss: 3.0561 - val_loss: 3.0400\n",
            "108/108 - 6s - loss: 3.0396 - val_loss: 3.0661\n",
            "108/108 - 6s - loss: 3.0574 - val_loss: 3.1222\n",
            "108/108 - 6s - loss: 3.0577 - val_loss: 3.1134\n",
            "108/108 - 6s - loss: 3.0523 - val_loss: 3.0896\n",
            "108/108 - 6s - loss: 3.0487 - val_loss: 3.0600\n",
            "108/108 - 6s - loss: 3.0389 - val_loss: 3.1152\n",
            "108/108 - 6s - loss: 3.0396 - val_loss: 3.0789\n",
            "108/108 - 6s - loss: 3.0393 - val_loss: 3.0590\n",
            "108/108 - 6s - loss: 3.0089 - val_loss: 3.0975\n",
            "108/108 - 6s - loss: 3.0198 - val_loss: 3.0879\n",
            "108/108 - 6s - loss: 3.0462 - val_loss: 3.1002\n",
            "108/108 - 6s - loss: 3.0418 - val_loss: 3.0834\n",
            "108/108 - 6s - loss: 3.0705 - val_loss: 3.1362\n",
            "108/108 - 6s - loss: 3.0120 - val_loss: 3.1135\n",
            "108/108 - 6s - loss: 3.0123 - val_loss: 3.0831\n",
            "108/108 - 6s - loss: 3.0406 - val_loss: 3.1110\n",
            "108/108 - 6s - loss: 2.9900 - val_loss: 3.0955\n",
            "108/108 - 6s - loss: 2.9963 - val_loss: 3.1141\n",
            "108/108 - 6s - loss: 3.0190 - val_loss: 3.1397\n",
            "108/108 - 6s - loss: 2.9952 - val_loss: 3.0807\n",
            "108/108 - 6s - loss: 3.0390 - val_loss: 3.1007\n",
            "108/108 - 6s - loss: 2.9949 - val_loss: 3.1506\n",
            "108/108 - 6s - loss: 3.0932 - val_loss: 3.1033\n",
            "108/108 - 6s - loss: 3.0435 - val_loss: 3.0964\n",
            "108/108 - 6s - loss: 3.0202 - val_loss: 3.1217\n",
            "108/108 - 6s - loss: 3.0149 - val_loss: 3.0381\n",
            "108/108 - 6s - loss: 2.9857 - val_loss: 3.0524\n",
            "108/108 - 6s - loss: 2.9956 - val_loss: 3.1183\n",
            "108/108 - 6s - loss: 2.9789 - val_loss: 3.1258\n",
            "108/108 - 6s - loss: 2.9928 - val_loss: 3.0408\n",
            "108/108 - 6s - loss: 3.0129 - val_loss: 3.0895\n",
            "108/108 - 6s - loss: 3.0121 - val_loss: 3.0642\n",
            "108/108 - 6s - loss: 3.0190 - val_loss: 3.0905\n",
            "108/108 - 6s - loss: 3.0000 - val_loss: 3.0535\n",
            "108/108 - 6s - loss: 2.9742 - val_loss: 3.0553\n",
            "108/108 - 6s - loss: 2.9739 - val_loss: 3.0925\n",
            "108/108 - 6s - loss: 3.0098 - val_loss: 3.0889\n",
            "108/108 - 6s - loss: 2.9449 - val_loss: 3.0926\n",
            "108/108 - 6s - loss: 2.9489 - val_loss: 3.1095\n",
            "108/108 - 6s - loss: 3.0125 - val_loss: 3.1117\n",
            "108/108 - 6s - loss: 2.9561 - val_loss: 3.1093\n",
            "108/108 - 6s - loss: 2.9948 - val_loss: 3.1455\n",
            "108/108 - 6s - loss: 2.9070 - val_loss: 3.1271\n",
            "108/108 - 6s - loss: 2.9626 - val_loss: 3.0377\n",
            "108/108 - 6s - loss: 3.0045 - val_loss: 3.0811\n",
            "108/108 - 6s - loss: 3.0102 - val_loss: 3.1282\n",
            "108/108 - 6s - loss: 2.9957 - val_loss: 3.1034\n",
            "108/108 - 6s - loss: 2.9662 - val_loss: 3.1644\n",
            "108/108 - 6s - loss: 2.9688 - val_loss: 3.1079\n",
            "108/108 - 6s - loss: 2.9438 - val_loss: 3.1516\n",
            "108/108 - 6s - loss: 3.0035 - val_loss: 3.1198\n",
            "108/108 - 6s - loss: 3.0148 - val_loss: 3.1551\n",
            "108/108 - 6s - loss: 2.9843 - val_loss: 3.1795\n",
            "108/108 - 6s - loss: 2.9977 - val_loss: 3.0994\n",
            "108/108 - 6s - loss: 3.0004 - val_loss: 3.1069\n",
            "108/108 - 6s - loss: 2.9654 - val_loss: 3.0769\n",
            "108/108 - 6s - loss: 2.9616 - val_loss: 3.1737\n",
            "108/108 - 6s - loss: 3.0565 - val_loss: 3.1365\n",
            "108/108 - 6s - loss: 2.9836 - val_loss: 3.1846\n",
            "108/108 - 6s - loss: 3.0061 - val_loss: 3.1298\n",
            "108/108 - 6s - loss: 2.9844 - val_loss: 3.1389\n",
            "108/108 - 6s - loss: 2.9703 - val_loss: 3.1360\n",
            "108/108 - 6s - loss: 2.9780 - val_loss: 3.2105\n",
            "108/108 - 6s - loss: 2.9487 - val_loss: 3.1824\n",
            "108/108 - 6s - loss: 2.9145 - val_loss: 3.1240\n",
            "108/108 - 6s - loss: 2.9691 - val_loss: 3.1178\n",
            "108/108 - 6s - loss: 2.9183 - val_loss: 3.0872\n",
            "108/108 - 6s - loss: 2.9392 - val_loss: 3.0827\n",
            "108/108 - 6s - loss: 2.9009 - val_loss: 3.1503\n",
            "108/108 - 6s - loss: 2.9386 - val_loss: 3.1414\n",
            "108/108 - 6s - loss: 2.9504 - val_loss: 3.1384\n",
            "108/108 - 6s - loss: 2.9459 - val_loss: 3.1689\n",
            "108/108 - 6s - loss: 2.9827 - val_loss: 3.1294\n",
            "108/108 - 6s - loss: 3.0019 - val_loss: 3.1343\n",
            "108/108 - 6s - loss: 2.9264 - val_loss: 3.1225\n",
            "108/108 - 6s - loss: 2.9490 - val_loss: 3.1893\n",
            "108/108 - 6s - loss: 2.9526 - val_loss: 3.1802\n",
            "108/108 - 6s - loss: 2.9334 - val_loss: 3.1597\n",
            "108/108 - 6s - loss: 2.9256 - val_loss: 3.2233\n",
            "108/108 - 6s - loss: 2.9377 - val_loss: 3.1080\n",
            "108/108 - 6s - loss: 2.9422 - val_loss: 3.1473\n",
            "108/108 - 6s - loss: 2.9346 - val_loss: 3.2207\n",
            "108/108 - 6s - loss: 2.9258 - val_loss: 3.1575\n",
            "108/108 - 6s - loss: 2.9197 - val_loss: 3.1789\n",
            "108/108 - 6s - loss: 2.9083 - val_loss: 3.2153\n",
            "108/108 - 6s - loss: 2.9367 - val_loss: 3.1280\n",
            "108/108 - 6s - loss: 2.8820 - val_loss: 3.1386\n",
            "108/108 - 6s - loss: 2.9320 - val_loss: 3.1440\n",
            "108/108 - 6s - loss: 2.8828 - val_loss: 3.1589\n",
            "108/108 - 6s - loss: 2.8908 - val_loss: 3.1534\n",
            "108/108 - 6s - loss: 2.8808 - val_loss: 3.1866\n",
            "108/108 - 6s - loss: 2.9014 - val_loss: 3.1248\n",
            "108/108 - 6s - loss: 2.9261 - val_loss: 3.1253\n",
            "108/108 - 6s - loss: 2.9086 - val_loss: 3.0870\n",
            "108/108 - 6s - loss: 2.8996 - val_loss: 3.0880\n",
            "108/108 - 6s - loss: 2.9224 - val_loss: 3.0929\n",
            "108/108 - 6s - loss: 2.8580 - val_loss: 3.1049\n",
            "108/108 - 6s - loss: 2.8667 - val_loss: 3.1424\n",
            "108/108 - 6s - loss: 2.8712 - val_loss: 3.0891\n",
            "108/108 - 6s - loss: 2.8819 - val_loss: 3.1649\n",
            "108/108 - 6s - loss: 2.8856 - val_loss: 3.1638\n",
            "108/108 - 6s - loss: 2.9051 - val_loss: 3.1173\n",
            "108/108 - 6s - loss: 2.8476 - val_loss: 3.1009\n",
            "108/108 - 6s - loss: 2.8229 - val_loss: 3.0937\n",
            "108/108 - 6s - loss: 2.8913 - val_loss: 3.1042\n",
            "108/108 - 6s - loss: 2.8535 - val_loss: 3.1216\n",
            "108/108 - 6s - loss: 2.8604 - val_loss: 3.0940\n",
            "108/108 - 6s - loss: 2.8960 - val_loss: 3.1593\n",
            "108/108 - 6s - loss: 2.8939 - val_loss: 3.1132\n",
            "108/108 - 6s - loss: 2.8769 - val_loss: 3.0973\n",
            "108/108 - 6s - loss: 2.8438 - val_loss: 3.1152\n",
            "108/108 - 6s - loss: 2.8691 - val_loss: 3.1410\n",
            "108/108 - 6s - loss: 2.8549 - val_loss: 3.1367\n",
            "108/108 - 6s - loss: 2.8662 - val_loss: 3.1170\n",
            "108/108 - 6s - loss: 2.8878 - val_loss: 3.1322\n",
            "108/108 - 6s - loss: 2.8436 - val_loss: 3.1137\n",
            "108/108 - 6s - loss: 2.8239 - val_loss: 3.1383\n",
            "108/108 - 6s - loss: 2.8565 - val_loss: 3.1534\n",
            "108/108 - 6s - loss: 2.8389 - val_loss: 3.1202\n",
            "108/108 - 6s - loss: 2.8596 - val_loss: 3.0898\n",
            "108/108 - 6s - loss: 2.8725 - val_loss: 3.1188\n",
            "108/108 - 6s - loss: 2.8825 - val_loss: 3.0919\n",
            "108/108 - 6s - loss: 2.8374 - val_loss: 3.0725\n",
            "108/108 - 6s - loss: 2.8073 - val_loss: 3.0959\n",
            "108/108 - 6s - loss: 2.8318 - val_loss: 3.1009\n",
            "108/108 - 6s - loss: 2.8536 - val_loss: 3.1306\n",
            "108/108 - 6s - loss: 2.8363 - val_loss: 3.1399\n",
            "108/108 - 6s - loss: 2.8412 - val_loss: 3.1336\n",
            "108/108 - 6s - loss: 2.8580 - val_loss: 3.1059\n",
            "108/108 - 6s - loss: 2.8426 - val_loss: 3.1456\n",
            "108/108 - 6s - loss: 2.8143 - val_loss: 3.1267\n",
            "108/108 - 6s - loss: 2.8720 - val_loss: 3.0749\n",
            "108/108 - 6s - loss: 2.8473 - val_loss: 3.1497\n",
            "108/108 - 6s - loss: 2.8604 - val_loss: 3.0945\n",
            "108/108 - 6s - loss: 2.8734 - val_loss: 3.1224\n",
            "108/108 - 6s - loss: 2.8223 - val_loss: 3.1198\n",
            "108/108 - 6s - loss: 2.8578 - val_loss: 3.1082\n",
            "108/108 - 6s - loss: 2.8111 - val_loss: 3.0485\n",
            "108/108 - 6s - loss: 2.8671 - val_loss: 3.1743\n",
            "108/108 - 6s - loss: 2.8085 - val_loss: 3.1814\n",
            "108/108 - 6s - loss: 2.8521 - val_loss: 3.0320\n",
            "108/108 - 6s - loss: 2.9220 - val_loss: 3.1917\n",
            "108/108 - 6s - loss: 2.8851 - val_loss: 3.1179\n",
            "108/108 - 6s - loss: 2.8222 - val_loss: 3.1794\n",
            "108/108 - 6s - loss: 2.8738 - val_loss: 3.1126\n",
            "108/108 - 6s - loss: 2.8777 - val_loss: 3.1405\n",
            "108/108 - 6s - loss: 2.8521 - val_loss: 3.1273\n",
            "108/108 - 6s - loss: 2.8379 - val_loss: 3.1195\n",
            "108/108 - 6s - loss: 2.8084 - val_loss: 3.1521\n",
            "108/108 - 6s - loss: 2.8368 - val_loss: 3.1382\n",
            "108/108 - 6s - loss: 2.8500 - val_loss: 3.1165\n",
            "108/108 - 6s - loss: 2.8212 - val_loss: 3.0916\n",
            "108/108 - 6s - loss: 2.8388 - val_loss: 3.1065\n",
            "108/108 - 6s - loss: 2.7931 - val_loss: 3.1326\n",
            "108/108 - 6s - loss: 2.8064 - val_loss: 3.0936\n",
            "108/108 - 6s - loss: 2.7956 - val_loss: 3.1201\n",
            "108/108 - 6s - loss: 2.7974 - val_loss: 3.1696\n",
            "108/108 - 6s - loss: 2.8127 - val_loss: 3.1762\n",
            "108/108 - 6s - loss: 2.8020 - val_loss: 3.1632\n",
            "108/108 - 6s - loss: 2.8364 - val_loss: 3.1021\n",
            "108/108 - 6s - loss: 2.8357 - val_loss: 3.1231\n",
            "108/108 - 6s - loss: 2.8768 - val_loss: 3.1106\n",
            "108/108 - 6s - loss: 2.7852 - val_loss: 3.0933\n",
            "108/108 - 6s - loss: 2.7991 - val_loss: 3.1342\n",
            "108/108 - 6s - loss: 2.7906 - val_loss: 3.0694\n",
            "108/108 - 6s - loss: 2.8062 - val_loss: 3.1201\n",
            "108/108 - 6s - loss: 2.8180 - val_loss: 3.1036\n",
            "108/108 - 6s - loss: 2.7729 - val_loss: 3.1163\n",
            "108/108 - 6s - loss: 2.7853 - val_loss: 3.1199\n",
            "108/108 - 6s - loss: 2.8086 - val_loss: 3.1195\n",
            "108/108 - 6s - loss: 2.7824 - val_loss: 3.0906\n",
            "108/108 - 6s - loss: 2.8101 - val_loss: 3.0404\n",
            "108/108 - 6s - loss: 2.8367 - val_loss: 3.1021\n",
            "108/108 - 6s - loss: 2.7864 - val_loss: 3.0597\n",
            "108/108 - 6s - loss: 2.8123 - val_loss: 3.1781\n",
            "108/108 - 6s - loss: 2.7832 - val_loss: 3.1783\n",
            "108/108 - 6s - loss: 2.7680 - val_loss: 3.1438\n",
            "108/108 - 6s - loss: 2.7701 - val_loss: 3.0512\n",
            "108/108 - 6s - loss: 2.7650 - val_loss: 3.1022\n",
            "108/108 - 6s - loss: 2.7573 - val_loss: 3.0740\n",
            "108/108 - 6s - loss: 2.7953 - val_loss: 3.0864\n",
            "108/108 - 6s - loss: 2.8221 - val_loss: 3.1435\n",
            "108/108 - 6s - loss: 2.7735 - val_loss: 3.0914\n",
            "108/108 - 6s - loss: 2.7782 - val_loss: 3.0409\n",
            "108/108 - 6s - loss: 2.7924 - val_loss: 3.1258\n",
            "108/108 - 6s - loss: 2.7910 - val_loss: 3.1070\n",
            "108/108 - 6s - loss: 2.7349 - val_loss: 3.1993\n",
            "108/108 - 6s - loss: 2.7928 - val_loss: 3.1340\n",
            "108/108 - 6s - loss: 2.7649 - val_loss: 3.1556\n",
            "108/108 - 6s - loss: 2.7762 - val_loss: 3.1255\n",
            "108/108 - 6s - loss: 2.7576 - val_loss: 3.1084\n",
            "108/108 - 6s - loss: 2.7697 - val_loss: 3.1556\n",
            "108/108 - 6s - loss: 2.7704 - val_loss: 3.1553\n",
            "108/108 - 6s - loss: 2.7744 - val_loss: 3.1191\n",
            "108/108 - 6s - loss: 2.7551 - val_loss: 3.1333\n",
            "108/108 - 6s - loss: 2.7686 - val_loss: 3.1809\n",
            "108/108 - 6s - loss: 2.7854 - val_loss: 3.1069\n",
            "108/108 - 6s - loss: 2.7433 - val_loss: 3.1292\n",
            "108/108 - 6s - loss: 2.7864 - val_loss: 3.1627\n",
            "108/108 - 6s - loss: 2.7625 - val_loss: 3.1572\n",
            "108/108 - 6s - loss: 2.7343 - val_loss: 3.1326\n",
            "108/108 - 6s - loss: 2.7592 - val_loss: 3.1317\n",
            "108/108 - 6s - loss: 2.7434 - val_loss: 3.2025\n",
            "108/108 - 6s - loss: 2.7607 - val_loss: 3.0912\n",
            "108/108 - 6s - loss: 2.7771 - val_loss: 3.0986\n",
            "108/108 - 6s - loss: 2.7122 - val_loss: 3.0936\n",
            "108/108 - 6s - loss: 2.7388 - val_loss: 3.1884\n",
            "108/108 - 6s - loss: 2.7635 - val_loss: 3.0992\n",
            "108/108 - 6s - loss: 2.7261 - val_loss: 3.0804\n",
            "108/108 - 6s - loss: 2.7682 - val_loss: 3.1464\n",
            "108/108 - 6s - loss: 2.7753 - val_loss: 3.0917\n",
            "108/108 - 6s - loss: 2.7151 - val_loss: 3.1973\n",
            "108/108 - 6s - loss: 2.7532 - val_loss: 3.1692\n",
            "108/108 - 6s - loss: 2.7303 - val_loss: 3.1771\n",
            "108/108 - 6s - loss: 2.7302 - val_loss: 3.1780\n",
            "108/108 - 6s - loss: 2.7777 - val_loss: 3.1313\n",
            "108/108 - 6s - loss: 2.7600 - val_loss: 3.0930\n",
            "108/108 - 6s - loss: 2.7692 - val_loss: 3.1183\n",
            "108/108 - 6s - loss: 2.7223 - val_loss: 3.1638\n",
            "108/108 - 6s - loss: 2.7538 - val_loss: 3.1328\n",
            "108/108 - 6s - loss: 2.7164 - val_loss: 3.2035\n",
            "108/108 - 6s - loss: 2.7431 - val_loss: 3.1685\n",
            "108/108 - 6s - loss: 2.7303 - val_loss: 3.0915\n",
            "108/108 - 6s - loss: 2.7273 - val_loss: 3.1940\n",
            "108/108 - 6s - loss: 2.7124 - val_loss: 3.0875\n",
            "108/108 - 6s - loss: 2.7448 - val_loss: 3.1408\n",
            "108/108 - 6s - loss: 2.7257 - val_loss: 3.1745\n",
            "108/108 - 6s - loss: 2.6972 - val_loss: 3.1589\n",
            "108/108 - 6s - loss: 2.7025 - val_loss: 3.1490\n",
            "108/108 - 6s - loss: 2.7607 - val_loss: 3.1645\n",
            "108/108 - 6s - loss: 2.7242 - val_loss: 3.1178\n",
            "108/108 - 6s - loss: 2.6922 - val_loss: 3.1540\n",
            "108/108 - 6s - loss: 2.7399 - val_loss: 3.1330\n",
            "108/108 - 6s - loss: 2.7665 - val_loss: 3.1306\n",
            "108/108 - 6s - loss: 2.7457 - val_loss: 3.1481\n",
            "108/108 - 6s - loss: 2.7303 - val_loss: 3.1201\n",
            "108/108 - 6s - loss: 2.7686 - val_loss: 3.1556\n",
            "108/108 - 6s - loss: 2.7457 - val_loss: 3.1068\n",
            "108/108 - 6s - loss: 2.7485 - val_loss: 3.1142\n",
            "108/108 - 6s - loss: 2.7127 - val_loss: 3.1444\n",
            "108/108 - 6s - loss: 2.7132 - val_loss: 3.1601\n",
            "108/108 - 6s - loss: 2.7098 - val_loss: 3.1494\n",
            "108/108 - 6s - loss: 2.7051 - val_loss: 3.0847\n",
            "108/108 - 6s - loss: 2.7009 - val_loss: 3.0989\n",
            "108/108 - 6s - loss: 2.6805 - val_loss: 3.0910\n",
            "108/108 - 6s - loss: 2.7164 - val_loss: 3.0681\n",
            "108/108 - 6s - loss: 2.7232 - val_loss: 3.1441\n",
            "108/108 - 6s - loss: 2.7122 - val_loss: 3.0748\n",
            "108/108 - 6s - loss: 2.7021 - val_loss: 3.0825\n",
            "108/108 - 6s - loss: 2.7274 - val_loss: 3.0712\n",
            "108/108 - 6s - loss: 2.7021 - val_loss: 3.1530\n",
            "108/108 - 6s - loss: 2.7275 - val_loss: 3.0780\n",
            "108/108 - 6s - loss: 2.7107 - val_loss: 3.1666\n",
            "108/108 - 6s - loss: 2.7246 - val_loss: 3.1427\n",
            "108/108 - 6s - loss: 2.7258 - val_loss: 3.0717\n",
            "108/108 - 6s - loss: 2.7407 - val_loss: 3.1063\n",
            "108/108 - 6s - loss: 2.7252 - val_loss: 3.1211\n",
            "108/108 - 6s - loss: 2.6974 - val_loss: 3.1801\n",
            "108/108 - 6s - loss: 2.6986 - val_loss: 3.1887\n",
            "108/108 - 6s - loss: 2.7643 - val_loss: 3.1617\n",
            "108/108 - 6s - loss: 2.6693 - val_loss: 3.1365\n",
            "108/108 - 6s - loss: 2.6773 - val_loss: 3.1937\n",
            "108/108 - 6s - loss: 2.6827 - val_loss: 3.1690\n",
            "108/108 - 6s - loss: 2.6453 - val_loss: 3.1659\n",
            "108/108 - 6s - loss: 2.6704 - val_loss: 3.1599\n",
            "108/108 - 6s - loss: 2.6847 - val_loss: 3.0865\n",
            "108/108 - 6s - loss: 2.6915 - val_loss: 3.1085\n",
            "108/108 - 6s - loss: 2.6707 - val_loss: 3.0827\n",
            "108/108 - 6s - loss: 2.6557 - val_loss: 3.1715\n",
            "108/108 - 6s - loss: 2.7202 - val_loss: 3.1173\n",
            "108/108 - 6s - loss: 2.6739 - val_loss: 3.1887\n",
            "108/108 - 6s - loss: 2.6675 - val_loss: 3.1840\n",
            "108/108 - 6s - loss: 2.6892 - val_loss: 3.1481\n",
            "108/108 - 6s - loss: 2.6422 - val_loss: 3.1509\n",
            "108/108 - 6s - loss: 2.7140 - val_loss: 3.1203\n",
            "108/108 - 6s - loss: 2.6596 - val_loss: 3.1199\n",
            "108/108 - 6s - loss: 2.6653 - val_loss: 3.1901\n",
            "108/108 - 6s - loss: 2.6850 - val_loss: 3.1299\n",
            "108/108 - 6s - loss: 2.6702 - val_loss: 3.1733\n",
            "108/108 - 6s - loss: 2.6947 - val_loss: 3.1565\n",
            "108/108 - 6s - loss: 2.6890 - val_loss: 3.1422\n",
            "108/108 - 6s - loss: 2.6425 - val_loss: 3.0947\n",
            "108/108 - 6s - loss: 2.6529 - val_loss: 3.1200\n",
            "108/108 - 6s - loss: 2.6880 - val_loss: 3.0854\n",
            "108/108 - 6s - loss: 2.6735 - val_loss: 3.1236\n",
            "108/108 - 6s - loss: 2.7308 - val_loss: 3.1416\n",
            "108/108 - 6s - loss: 2.6534 - val_loss: 3.1104\n",
            "108/108 - 6s - loss: 2.6604 - val_loss: 3.1726\n",
            "108/108 - 6s - loss: 2.6811 - val_loss: 3.1802\n",
            "0.5\n",
            "108/108 - 13s - loss: 8.7288 - val_loss: 8.6987\n",
            "108/108 - 6s - loss: 8.2249 - val_loss: 8.3056\n",
            "108/108 - 6s - loss: 7.8640 - val_loss: 7.9794\n",
            "108/108 - 6s - loss: 7.5389 - val_loss: 7.6848\n",
            "108/108 - 6s - loss: 7.2564 - val_loss: 7.4154\n",
            "108/108 - 6s - loss: 7.0037 - val_loss: 7.1673\n",
            "108/108 - 6s - loss: 6.7837 - val_loss: 6.9349\n",
            "108/108 - 6s - loss: 6.5720 - val_loss: 6.7198\n",
            "108/108 - 6s - loss: 6.3599 - val_loss: 6.5179\n",
            "108/108 - 6s - loss: 6.1808 - val_loss: 6.3295\n",
            "108/108 - 6s - loss: 6.0078 - val_loss: 6.1522\n",
            "108/108 - 6s - loss: 5.8370 - val_loss: 5.9870\n",
            "108/108 - 6s - loss: 5.6683 - val_loss: 5.8307\n",
            "108/108 - 6s - loss: 5.5275 - val_loss: 5.6854\n",
            "108/108 - 6s - loss: 5.3747 - val_loss: 5.5493\n",
            "108/108 - 6s - loss: 5.2909 - val_loss: 5.4229\n",
            "108/108 - 6s - loss: 5.1774 - val_loss: 5.3062\n",
            "108/108 - 6s - loss: 5.0575 - val_loss: 5.1970\n",
            "108/108 - 6s - loss: 4.9616 - val_loss: 5.0937\n",
            "108/108 - 6s - loss: 4.8629 - val_loss: 4.9973\n",
            "108/108 - 6s - loss: 4.8401 - val_loss: 4.9099\n",
            "108/108 - 6s - loss: 4.7477 - val_loss: 4.8296\n",
            "108/108 - 6s - loss: 4.6894 - val_loss: 4.7587\n",
            "108/108 - 6s - loss: 4.6534 - val_loss: 4.6933\n",
            "108/108 - 6s - loss: 4.5528 - val_loss: 4.6324\n",
            "108/108 - 6s - loss: 4.5249 - val_loss: 4.5769\n",
            "108/108 - 6s - loss: 4.5177 - val_loss: 4.5292\n",
            "108/108 - 6s - loss: 4.4075 - val_loss: 4.4854\n",
            "108/108 - 6s - loss: 4.4337 - val_loss: 4.4482\n",
            "108/108 - 6s - loss: 4.3697 - val_loss: 4.4137\n",
            "108/108 - 6s - loss: 4.3936 - val_loss: 4.3824\n",
            "108/108 - 6s - loss: 4.3714 - val_loss: 4.3565\n",
            "108/108 - 6s - loss: 4.3165 - val_loss: 4.3333\n",
            "108/108 - 6s - loss: 4.2813 - val_loss: 4.3137\n",
            "108/108 - 6s - loss: 4.3147 - val_loss: 4.2969\n",
            "108/108 - 6s - loss: 4.2705 - val_loss: 4.2827\n",
            "108/108 - 6s - loss: 4.2143 - val_loss: 4.2683\n",
            "108/108 - 6s - loss: 4.2898 - val_loss: 4.2592\n",
            "108/108 - 6s - loss: 4.2884 - val_loss: 4.2499\n",
            "108/108 - 6s - loss: 4.3099 - val_loss: 4.2439\n",
            "108/108 - 6s - loss: 4.2950 - val_loss: 4.2365\n",
            "108/108 - 6s - loss: 4.2859 - val_loss: 4.2310\n",
            "108/108 - 6s - loss: 4.2672 - val_loss: 4.2265\n",
            "108/108 - 6s - loss: 4.2097 - val_loss: 4.2209\n",
            "108/108 - 6s - loss: 4.2157 - val_loss: 4.2158\n",
            "108/108 - 6s - loss: 4.2319 - val_loss: 4.2133\n",
            "108/108 - 6s - loss: 4.2102 - val_loss: 4.2095\n",
            "108/108 - 6s - loss: 4.2599 - val_loss: 4.2068\n",
            "108/108 - 6s - loss: 4.2177 - val_loss: 4.2044\n",
            "108/108 - 6s - loss: 4.2500 - val_loss: 4.2015\n",
            "108/108 - 6s - loss: 4.2678 - val_loss: 4.2014\n",
            "108/108 - 6s - loss: 4.2364 - val_loss: 4.2003\n",
            "108/108 - 6s - loss: 4.2153 - val_loss: 4.1996\n",
            "108/108 - 6s - loss: 4.2321 - val_loss: 4.1983\n",
            "108/108 - 6s - loss: 4.1668 - val_loss: 4.1967\n",
            "108/108 - 6s - loss: 4.2094 - val_loss: 4.1951\n",
            "108/108 - 6s - loss: 4.2347 - val_loss: 4.1940\n",
            "108/108 - 6s - loss: 4.2283 - val_loss: 4.1934\n",
            "108/108 - 6s - loss: 4.1648 - val_loss: 4.1925\n",
            "108/108 - 6s - loss: 4.2318 - val_loss: 4.1931\n",
            "108/108 - 6s - loss: 4.2357 - val_loss: 4.1920\n",
            "108/108 - 6s - loss: 4.2907 - val_loss: 4.1912\n",
            "108/108 - 6s - loss: 4.2257 - val_loss: 4.1911\n",
            "108/108 - 6s - loss: 4.1968 - val_loss: 4.1912\n",
            "108/108 - 6s - loss: 4.2321 - val_loss: 4.1918\n",
            "108/108 - 6s - loss: 4.2363 - val_loss: 4.1916\n",
            "108/108 - 6s - loss: 4.2395 - val_loss: 4.1910\n",
            "108/108 - 6s - loss: 4.2482 - val_loss: 4.1901\n",
            "108/108 - 6s - loss: 4.2003 - val_loss: 4.1902\n",
            "108/108 - 6s - loss: 4.2397 - val_loss: 4.1901\n",
            "108/108 - 6s - loss: 4.2029 - val_loss: 4.1901\n",
            "108/108 - 6s - loss: 4.2257 - val_loss: 4.1896\n",
            "108/108 - 6s - loss: 4.2332 - val_loss: 4.1896\n",
            "108/108 - 6s - loss: 4.1741 - val_loss: 4.1886\n",
            "108/108 - 6s - loss: 4.2349 - val_loss: 4.1881\n",
            "108/108 - 6s - loss: 4.2996 - val_loss: 4.1891\n",
            "108/108 - 6s - loss: 4.2295 - val_loss: 4.1888\n",
            "108/108 - 6s - loss: 4.2273 - val_loss: 4.1880\n",
            "108/108 - 6s - loss: 4.3019 - val_loss: 4.1886\n",
            "108/108 - 6s - loss: 4.2669 - val_loss: 4.1894\n",
            "108/108 - 6s - loss: 4.1970 - val_loss: 4.1898\n",
            "108/108 - 6s - loss: 4.2193 - val_loss: 4.1895\n",
            "108/108 - 6s - loss: 4.2490 - val_loss: 4.1893\n",
            "108/108 - 6s - loss: 4.2568 - val_loss: 4.1902\n",
            "108/108 - 6s - loss: 4.2301 - val_loss: 4.1896\n",
            "108/108 - 6s - loss: 4.2395 - val_loss: 4.1890\n",
            "108/108 - 6s - loss: 4.2240 - val_loss: 4.1882\n",
            "108/108 - 6s - loss: 4.2064 - val_loss: 4.1882\n",
            "108/108 - 6s - loss: 4.2554 - val_loss: 4.1893\n",
            "108/108 - 6s - loss: 4.2671 - val_loss: 4.1896\n",
            "108/108 - 6s - loss: 4.2686 - val_loss: 4.1896\n",
            "108/108 - 6s - loss: 4.2195 - val_loss: 4.1898\n",
            "108/108 - 6s - loss: 4.2076 - val_loss: 4.1892\n",
            "108/108 - 6s - loss: 4.2202 - val_loss: 4.1886\n",
            "108/108 - 6s - loss: 4.2473 - val_loss: 4.1890\n",
            "108/108 - 6s - loss: 4.2524 - val_loss: 4.1893\n",
            "108/108 - 6s - loss: 4.2633 - val_loss: 4.1885\n",
            "108/108 - 6s - loss: 4.2580 - val_loss: 4.1893\n",
            "108/108 - 6s - loss: 4.2083 - val_loss: 4.1889\n",
            "108/108 - 6s - loss: 4.2258 - val_loss: 4.1886\n",
            "108/108 - 6s - loss: 4.2331 - val_loss: 4.1883\n",
            "108/108 - 6s - loss: 4.1855 - val_loss: 4.1884\n",
            "108/108 - 6s - loss: 4.2552 - val_loss: 4.1884\n",
            "108/108 - 6s - loss: 4.2132 - val_loss: 4.1877\n",
            "108/108 - 6s - loss: 4.2524 - val_loss: 4.1876\n",
            "108/108 - 6s - loss: 4.2116 - val_loss: 4.1875\n",
            "108/108 - 6s - loss: 4.2023 - val_loss: 4.1875\n",
            "108/108 - 6s - loss: 4.2452 - val_loss: 4.1879\n",
            "108/108 - 6s - loss: 4.2587 - val_loss: 4.1879\n",
            "108/108 - 6s - loss: 4.2213 - val_loss: 4.1880\n",
            "108/108 - 6s - loss: 4.2055 - val_loss: 4.1888\n",
            "108/108 - 6s - loss: 4.2154 - val_loss: 4.1890\n",
            "108/108 - 6s - loss: 4.2684 - val_loss: 4.1897\n",
            "108/108 - 6s - loss: 4.1881 - val_loss: 4.1889\n",
            "108/108 - 6s - loss: 4.2216 - val_loss: 4.1884\n",
            "108/108 - 6s - loss: 4.2293 - val_loss: 4.1885\n",
            "108/108 - 6s - loss: 4.2540 - val_loss: 4.1896\n",
            "108/108 - 6s - loss: 4.2117 - val_loss: 4.1894\n",
            "108/108 - 6s - loss: 4.2222 - val_loss: 4.1894\n",
            "108/108 - 6s - loss: 4.2348 - val_loss: 4.1896\n",
            "108/108 - 6s - loss: 4.2478 - val_loss: 4.1894\n",
            "108/108 - 6s - loss: 4.2281 - val_loss: 4.1887\n",
            "108/108 - 6s - loss: 4.2633 - val_loss: 4.1891\n",
            "108/108 - 6s - loss: 4.2150 - val_loss: 4.1892\n",
            "108/108 - 6s - loss: 4.2126 - val_loss: 4.1887\n",
            "108/108 - 6s - loss: 4.2821 - val_loss: 4.1890\n",
            "108/108 - 6s - loss: 4.2504 - val_loss: 4.1895\n",
            "108/108 - 6s - loss: 4.3533 - val_loss: 4.2276\n",
            "108/108 - 6s - loss: 4.2727 - val_loss: 4.2217\n",
            "108/108 - 6s - loss: 4.2435 - val_loss: 4.2169\n",
            "108/108 - 6s - loss: 4.2795 - val_loss: 4.2135\n",
            "108/108 - 6s - loss: 4.2507 - val_loss: 4.2104\n",
            "108/108 - 6s - loss: 4.2447 - val_loss: 4.2073\n",
            "108/108 - 6s - loss: 4.2498 - val_loss: 4.2048\n",
            "108/108 - 6s - loss: 4.2513 - val_loss: 4.2027\n",
            "108/108 - 6s - loss: 4.2562 - val_loss: 4.2016\n",
            "108/108 - 6s - loss: 4.2523 - val_loss: 4.2000\n",
            "108/108 - 6s - loss: 4.2512 - val_loss: 4.1984\n",
            "108/108 - 6s - loss: 4.2549 - val_loss: 4.1974\n",
            "108/108 - 6s - loss: 4.2521 - val_loss: 4.1964\n",
            "108/108 - 6s - loss: 4.2075 - val_loss: 4.1956\n",
            "108/108 - 6s - loss: 4.2485 - val_loss: 4.1946\n",
            "108/108 - 6s - loss: 4.2125 - val_loss: 4.1935\n",
            "108/108 - 6s - loss: 4.2706 - val_loss: 4.1930\n",
            "108/108 - 6s - loss: 4.2608 - val_loss: 4.1932\n",
            "108/108 - 6s - loss: 4.2668 - val_loss: 4.1938\n",
            "108/108 - 6s - loss: 4.2287 - val_loss: 4.1934\n",
            "108/108 - 6s - loss: 4.2601 - val_loss: 4.1934\n",
            "108/108 - 6s - loss: 4.2438 - val_loss: 4.1930\n",
            "108/108 - 6s - loss: 4.2277 - val_loss: 4.1925\n",
            "108/108 - 6s - loss: 4.2330 - val_loss: 4.1925\n",
            "108/108 - 6s - loss: 4.2097 - val_loss: 4.1920\n",
            "108/108 - 6s - loss: 4.2150 - val_loss: 4.1916\n",
            "108/108 - 6s - loss: 4.2421 - val_loss: 4.1913\n",
            "108/108 - 6s - loss: 4.2198 - val_loss: 4.1916\n",
            "108/108 - 6s - loss: 4.2369 - val_loss: 4.1919\n",
            "108/108 - 6s - loss: 4.1987 - val_loss: 4.1921\n",
            "108/108 - 6s - loss: 4.2056 - val_loss: 4.1909\n",
            "108/108 - 6s - loss: 4.1881 - val_loss: 4.1902\n",
            "108/108 - 6s - loss: 4.2881 - val_loss: 4.1897\n",
            "108/108 - 6s - loss: 4.1978 - val_loss: 4.1892\n",
            "108/108 - 6s - loss: 4.2317 - val_loss: 4.1895\n",
            "108/108 - 6s - loss: 4.2281 - val_loss: 4.1894\n",
            "108/108 - 6s - loss: 4.2637 - val_loss: 4.1900\n",
            "108/108 - 6s - loss: 4.2330 - val_loss: 4.1894\n",
            "108/108 - 6s - loss: 4.2713 - val_loss: 4.1897\n",
            "108/108 - 6s - loss: 4.2442 - val_loss: 4.1899\n",
            "108/108 - 6s - loss: 4.2019 - val_loss: 4.1895\n",
            "108/108 - 6s - loss: 4.2031 - val_loss: 4.1890\n",
            "108/108 - 6s - loss: 4.2451 - val_loss: 4.1888\n",
            "108/108 - 6s - loss: 4.1912 - val_loss: 4.1883\n",
            "108/108 - 6s - loss: 4.2343 - val_loss: 4.1886\n",
            "108/108 - 6s - loss: 4.2042 - val_loss: 4.1890\n",
            "108/108 - 6s - loss: 4.2611 - val_loss: 4.1893\n",
            "108/108 - 6s - loss: 4.2383 - val_loss: 4.1897\n",
            "108/108 - 6s - loss: 4.2230 - val_loss: 4.1901\n",
            "108/108 - 6s - loss: 4.2081 - val_loss: 4.1903\n",
            "108/108 - 6s - loss: 4.2222 - val_loss: 4.1906\n",
            "108/108 - 6s - loss: 4.2694 - val_loss: 4.1898\n",
            "108/108 - 6s - loss: 4.2193 - val_loss: 4.1902\n",
            "108/108 - 6s - loss: 4.2552 - val_loss: 4.1902\n",
            "108/108 - 6s - loss: 4.2305 - val_loss: 4.1901\n",
            "108/108 - 6s - loss: 4.2140 - val_loss: 4.1909\n",
            "108/108 - 6s - loss: 4.2177 - val_loss: 4.1908\n",
            "108/108 - 6s - loss: 4.2509 - val_loss: 4.1904\n",
            "108/108 - 6s - loss: 4.2260 - val_loss: 4.1905\n",
            "108/108 - 6s - loss: 4.2280 - val_loss: 4.1904\n",
            "108/108 - 6s - loss: 4.2588 - val_loss: 4.1912\n",
            "108/108 - 6s - loss: 4.2655 - val_loss: 4.1911\n",
            "108/108 - 6s - loss: 4.1628 - val_loss: 4.1905\n",
            "108/108 - 6s - loss: 4.2383 - val_loss: 4.1901\n",
            "108/108 - 6s - loss: 4.2418 - val_loss: 4.1899\n",
            "108/108 - 6s - loss: 4.2550 - val_loss: 4.1903\n",
            "108/108 - 6s - loss: 4.3072 - val_loss: 4.1911\n",
            "108/108 - 6s - loss: 4.2390 - val_loss: 4.1918\n",
            "108/108 - 6s - loss: 4.2050 - val_loss: 4.1915\n",
            "108/108 - 6s - loss: 4.2239 - val_loss: 4.1926\n",
            "108/108 - 6s - loss: 4.2459 - val_loss: 4.1925\n",
            "108/108 - 6s - loss: 4.2575 - val_loss: 4.1922\n",
            "108/108 - 6s - loss: 4.2235 - val_loss: 4.1913\n",
            "108/108 - 6s - loss: 4.2590 - val_loss: 4.1912\n",
            "108/108 - 6s - loss: 4.2140 - val_loss: 4.1910\n",
            "108/108 - 6s - loss: 4.2192 - val_loss: 4.1907\n",
            "108/108 - 6s - loss: 4.1914 - val_loss: 4.1900\n",
            "108/108 - 6s - loss: 4.2133 - val_loss: 4.1901\n",
            "108/108 - 6s - loss: 4.2871 - val_loss: 4.1903\n",
            "108/108 - 6s - loss: 4.2422 - val_loss: 4.1898\n",
            "108/108 - 6s - loss: 4.2490 - val_loss: 4.1891\n",
            "108/108 - 6s - loss: 4.2325 - val_loss: 4.1890\n",
            "108/108 - 6s - loss: 4.2256 - val_loss: 4.1896\n",
            "108/108 - 6s - loss: 4.2382 - val_loss: 4.1894\n",
            "108/108 - 6s - loss: 4.2609 - val_loss: 4.1903\n",
            "108/108 - 6s - loss: 4.2860 - val_loss: 4.1906\n",
            "108/108 - 6s - loss: 4.2317 - val_loss: 4.1905\n",
            "108/108 - 6s - loss: 4.2274 - val_loss: 4.1903\n",
            "108/108 - 6s - loss: 4.2717 - val_loss: 4.1903\n",
            "108/108 - 6s - loss: 4.1785 - val_loss: 4.1911\n",
            "108/108 - 6s - loss: 4.2290 - val_loss: 4.1913\n",
            "108/108 - 6s - loss: 4.2184 - val_loss: 4.1920\n",
            "108/108 - 6s - loss: 4.2766 - val_loss: 4.1916\n",
            "108/108 - 6s - loss: 4.2458 - val_loss: 4.1913\n",
            "108/108 - 6s - loss: 4.2447 - val_loss: 4.1910\n",
            "108/108 - 6s - loss: 4.2188 - val_loss: 4.1913\n",
            "108/108 - 6s - loss: 4.2328 - val_loss: 4.1911\n",
            "108/108 - 6s - loss: 4.2463 - val_loss: 4.1912\n",
            "108/108 - 6s - loss: 4.2521 - val_loss: 4.1911\n",
            "108/108 - 6s - loss: 4.2278 - val_loss: 4.1904\n",
            "108/108 - 6s - loss: 4.2044 - val_loss: 4.1903\n",
            "108/108 - 6s - loss: 4.2431 - val_loss: 4.1050\n",
            "108/108 - 6s - loss: 4.0994 - val_loss: 3.9582\n",
            "108/108 - 6s - loss: 3.9345 - val_loss: 3.7312\n",
            "108/108 - 6s - loss: 3.7679 - val_loss: 3.7014\n",
            "108/108 - 6s - loss: 3.6896 - val_loss: 3.5901\n",
            "108/108 - 6s - loss: 3.6806 - val_loss: 3.6075\n",
            "108/108 - 6s - loss: 3.6510 - val_loss: 3.6156\n",
            "108/108 - 6s - loss: 3.5856 - val_loss: 3.5287\n",
            "108/108 - 6s - loss: 3.5457 - val_loss: 3.4695\n",
            "108/108 - 6s - loss: 3.5494 - val_loss: 3.4447\n",
            "108/108 - 6s - loss: 3.5177 - val_loss: 3.3565\n",
            "108/108 - 6s - loss: 3.5102 - val_loss: 3.3975\n",
            "108/108 - 6s - loss: 3.4294 - val_loss: 3.3514\n",
            "108/108 - 6s - loss: 3.4455 - val_loss: 3.2923\n",
            "108/108 - 6s - loss: 3.4039 - val_loss: 3.3223\n",
            "108/108 - 6s - loss: 3.4305 - val_loss: 3.2666\n",
            "108/108 - 6s - loss: 3.3978 - val_loss: 3.2867\n",
            "108/108 - 6s - loss: 3.4231 - val_loss: 3.2500\n",
            "108/108 - 6s - loss: 3.3391 - val_loss: 3.2839\n",
            "108/108 - 6s - loss: 3.3653 - val_loss: 3.1863\n",
            "108/108 - 6s - loss: 3.3981 - val_loss: 3.1418\n",
            "108/108 - 6s - loss: 3.3258 - val_loss: 3.2327\n",
            "108/108 - 6s - loss: 3.3276 - val_loss: 3.1631\n",
            "108/108 - 6s - loss: 3.3080 - val_loss: 3.3006\n",
            "108/108 - 6s - loss: 3.3372 - val_loss: 3.1348\n",
            "108/108 - 6s - loss: 3.3117 - val_loss: 3.1742\n",
            "108/108 - 6s - loss: 3.3518 - val_loss: 3.1083\n",
            "108/108 - 6s - loss: 3.2636 - val_loss: 3.1897\n",
            "108/108 - 6s - loss: 3.2764 - val_loss: 3.1606\n",
            "108/108 - 6s - loss: 3.2746 - val_loss: 3.0933\n",
            "108/108 - 6s - loss: 3.3113 - val_loss: 3.0755\n",
            "108/108 - 6s - loss: 3.2501 - val_loss: 3.1190\n",
            "108/108 - 6s - loss: 3.2889 - val_loss: 3.1829\n",
            "108/108 - 6s - loss: 3.2440 - val_loss: 3.0883\n",
            "108/108 - 6s - loss: 3.2535 - val_loss: 3.0751\n",
            "108/108 - 6s - loss: 3.2368 - val_loss: 3.0378\n",
            "108/108 - 6s - loss: 3.2149 - val_loss: 3.1269\n",
            "108/108 - 6s - loss: 3.2069 - val_loss: 3.1167\n",
            "108/108 - 6s - loss: 3.2232 - val_loss: 3.0295\n",
            "108/108 - 6s - loss: 3.2073 - val_loss: 3.1118\n",
            "108/108 - 6s - loss: 3.2662 - val_loss: 3.0507\n",
            "108/108 - 6s - loss: 3.2329 - val_loss: 3.1417\n",
            "108/108 - 6s - loss: 3.2345 - val_loss: 3.0847\n",
            "108/108 - 6s - loss: 3.2161 - val_loss: 3.0713\n",
            "108/108 - 6s - loss: 3.2135 - val_loss: 3.0099\n",
            "108/108 - 6s - loss: 3.1887 - val_loss: 3.0287\n",
            "108/108 - 6s - loss: 3.2227 - val_loss: 3.0378\n",
            "108/108 - 6s - loss: 3.1811 - val_loss: 2.9811\n",
            "108/108 - 6s - loss: 3.2171 - val_loss: 3.0090\n",
            "108/108 - 6s - loss: 3.1331 - val_loss: 3.0477\n",
            "108/108 - 6s - loss: 3.1782 - val_loss: 3.0355\n",
            "108/108 - 6s - loss: 3.1776 - val_loss: 3.0373\n",
            "108/108 - 6s - loss: 3.1715 - val_loss: 3.0352\n",
            "108/108 - 6s - loss: 3.1523 - val_loss: 3.0460\n",
            "108/108 - 6s - loss: 3.1912 - val_loss: 2.9829\n",
            "108/108 - 6s - loss: 3.2018 - val_loss: 3.0269\n",
            "108/108 - 6s - loss: 3.1214 - val_loss: 2.9454\n",
            "108/108 - 6s - loss: 3.1192 - val_loss: 2.9925\n",
            "108/108 - 6s - loss: 3.1530 - val_loss: 2.9833\n",
            "108/108 - 6s - loss: 3.1549 - val_loss: 2.9822\n",
            "108/108 - 6s - loss: 3.1286 - val_loss: 3.1309\n",
            "108/108 - 6s - loss: 3.1190 - val_loss: 2.9723\n",
            "108/108 - 6s - loss: 3.1406 - val_loss: 2.9806\n",
            "108/108 - 6s - loss: 3.1241 - val_loss: 3.0351\n",
            "108/108 - 6s - loss: 3.1294 - val_loss: 2.9569\n",
            "108/108 - 6s - loss: 3.1515 - val_loss: 2.9608\n",
            "108/108 - 6s - loss: 3.1264 - val_loss: 2.9924\n",
            "108/108 - 6s - loss: 3.1201 - val_loss: 2.9821\n",
            "108/108 - 6s - loss: 3.0873 - val_loss: 2.9559\n",
            "108/108 - 6s - loss: 3.0743 - val_loss: 2.9803\n",
            "108/108 - 6s - loss: 3.1442 - val_loss: 2.9583\n",
            "108/108 - 6s - loss: 3.1304 - val_loss: 2.9550\n",
            "108/108 - 6s - loss: 3.1313 - val_loss: 3.0295\n",
            "108/108 - 6s - loss: 3.1680 - val_loss: 2.9895\n",
            "108/108 - 6s - loss: 3.0943 - val_loss: 2.9522\n",
            "108/108 - 6s - loss: 3.0700 - val_loss: 2.9636\n",
            "108/108 - 6s - loss: 3.1219 - val_loss: 2.9866\n",
            "108/108 - 6s - loss: 3.0844 - val_loss: 2.9869\n",
            "108/108 - 6s - loss: 3.0768 - val_loss: 2.9903\n",
            "108/108 - 6s - loss: 3.1165 - val_loss: 2.9653\n",
            "108/108 - 6s - loss: 3.1419 - val_loss: 3.0341\n",
            "108/108 - 6s - loss: 3.0501 - val_loss: 2.9873\n",
            "108/108 - 6s - loss: 3.0540 - val_loss: 3.0263\n",
            "108/108 - 6s - loss: 3.0499 - val_loss: 3.0567\n",
            "108/108 - 6s - loss: 3.0958 - val_loss: 2.9637\n",
            "108/108 - 6s - loss: 3.1053 - val_loss: 3.1122\n",
            "108/108 - 6s - loss: 3.1215 - val_loss: 3.0222\n",
            "108/108 - 6s - loss: 3.1168 - val_loss: 3.0125\n",
            "108/108 - 6s - loss: 3.0202 - val_loss: 3.0534\n",
            "108/108 - 6s - loss: 3.1071 - val_loss: 2.9768\n",
            "108/108 - 6s - loss: 3.0853 - val_loss: 2.9897\n",
            "108/108 - 6s - loss: 3.0737 - val_loss: 3.0795\n",
            "108/108 - 6s - loss: 3.0412 - val_loss: 3.0271\n",
            "108/108 - 6s - loss: 3.0565 - val_loss: 2.9647\n",
            "108/108 - 6s - loss: 3.0711 - val_loss: 3.0733\n",
            "108/108 - 6s - loss: 3.0508 - val_loss: 2.9395\n",
            "108/108 - 6s - loss: 3.0459 - val_loss: 2.9901\n",
            "108/108 - 6s - loss: 3.0987 - val_loss: 2.9899\n",
            "108/108 - 6s - loss: 3.0477 - val_loss: 3.0538\n",
            "108/108 - 6s - loss: 3.0370 - val_loss: 2.9248\n",
            "108/108 - 6s - loss: 3.0134 - val_loss: 2.9621\n",
            "108/108 - 6s - loss: 3.0393 - val_loss: 2.9685\n",
            "108/108 - 6s - loss: 3.0841 - val_loss: 2.9596\n",
            "108/108 - 6s - loss: 3.0355 - val_loss: 2.9861\n",
            "108/108 - 6s - loss: 3.0183 - val_loss: 2.9941\n",
            "108/108 - 6s - loss: 3.0469 - val_loss: 3.0133\n",
            "108/108 - 6s - loss: 3.0383 - val_loss: 3.0090\n",
            "108/108 - 6s - loss: 3.0752 - val_loss: 2.9242\n",
            "108/108 - 6s - loss: 2.9976 - val_loss: 2.9064\n",
            "108/108 - 6s - loss: 3.0032 - val_loss: 2.9752\n",
            "108/108 - 6s - loss: 3.0435 - val_loss: 2.9538\n",
            "108/108 - 6s - loss: 3.0448 - val_loss: 3.0166\n",
            "108/108 - 6s - loss: 3.0419 - val_loss: 3.0621\n",
            "108/108 - 6s - loss: 3.0412 - val_loss: 2.9973\n",
            "108/108 - 6s - loss: 3.0394 - val_loss: 2.9637\n",
            "108/108 - 6s - loss: 3.0452 - val_loss: 3.1007\n",
            "108/108 - 6s - loss: 3.0351 - val_loss: 2.9383\n",
            "108/108 - 6s - loss: 3.0417 - val_loss: 2.9973\n",
            "108/108 - 6s - loss: 3.0466 - val_loss: 2.9773\n",
            "108/108 - 6s - loss: 3.0005 - val_loss: 2.9905\n",
            "108/108 - 6s - loss: 3.0340 - val_loss: 2.9805\n",
            "108/108 - 6s - loss: 2.9941 - val_loss: 2.9567\n",
            "108/108 - 6s - loss: 2.9856 - val_loss: 2.9651\n",
            "108/108 - 6s - loss: 3.0466 - val_loss: 2.9800\n",
            "108/108 - 6s - loss: 3.0040 - val_loss: 2.9756\n",
            "108/108 - 6s - loss: 2.9816 - val_loss: 2.9631\n",
            "108/108 - 6s - loss: 2.9761 - val_loss: 2.9799\n",
            "108/108 - 6s - loss: 2.9898 - val_loss: 3.0101\n",
            "108/108 - 6s - loss: 3.0175 - val_loss: 3.0451\n",
            "108/108 - 6s - loss: 2.9700 - val_loss: 2.9851\n",
            "108/108 - 6s - loss: 2.9687 - val_loss: 2.9427\n",
            "108/108 - 6s - loss: 2.9306 - val_loss: 3.0029\n",
            "108/108 - 6s - loss: 2.9498 - val_loss: 2.9966\n",
            "108/108 - 6s - loss: 2.9651 - val_loss: 2.9907\n",
            "108/108 - 6s - loss: 2.9425 - val_loss: 3.0108\n",
            "108/108 - 6s - loss: 2.9594 - val_loss: 3.0536\n",
            "108/108 - 6s - loss: 2.9588 - val_loss: 3.0256\n",
            "108/108 - 6s - loss: 2.9405 - val_loss: 2.9331\n",
            "108/108 - 6s - loss: 2.9600 - val_loss: 3.0126\n",
            "108/108 - 6s - loss: 2.9930 - val_loss: 3.0719\n",
            "108/108 - 6s - loss: 2.9364 - val_loss: 3.0194\n",
            "108/108 - 6s - loss: 3.0066 - val_loss: 2.9723\n",
            "108/108 - 6s - loss: 3.0142 - val_loss: 2.9645\n",
            "108/108 - 6s - loss: 2.9593 - val_loss: 3.0022\n",
            "108/108 - 6s - loss: 2.9962 - val_loss: 3.0018\n",
            "108/108 - 6s - loss: 2.9885 - val_loss: 2.9879\n",
            "108/108 - 6s - loss: 2.9405 - val_loss: 3.0653\n",
            "108/108 - 6s - loss: 2.9431 - val_loss: 3.0249\n",
            "108/108 - 6s - loss: 2.9424 - val_loss: 3.0760\n",
            "108/108 - 6s - loss: 2.9363 - val_loss: 2.9431\n",
            "108/108 - 6s - loss: 2.9448 - val_loss: 2.8940\n",
            "108/108 - 6s - loss: 2.9495 - val_loss: 2.8738\n",
            "108/108 - 6s - loss: 2.9187 - val_loss: 2.9647\n",
            "108/108 - 6s - loss: 2.9719 - val_loss: 2.9870\n",
            "108/108 - 6s - loss: 2.9319 - val_loss: 2.9502\n",
            "108/108 - 6s - loss: 2.9404 - val_loss: 2.8935\n",
            "108/108 - 6s - loss: 2.9969 - val_loss: 2.9732\n",
            "108/108 - 6s - loss: 2.9415 - val_loss: 3.0369\n",
            "108/108 - 6s - loss: 2.9879 - val_loss: 3.0075\n",
            "108/108 - 6s - loss: 2.9651 - val_loss: 3.0737\n",
            "108/108 - 6s - loss: 2.9475 - val_loss: 2.9314\n",
            "108/108 - 6s - loss: 2.9634 - val_loss: 2.9665\n",
            "108/108 - 6s - loss: 2.9236 - val_loss: 2.9703\n",
            "108/108 - 6s - loss: 2.9844 - val_loss: 3.0223\n",
            "108/108 - 6s - loss: 2.9257 - val_loss: 2.9457\n",
            "108/108 - 6s - loss: 2.8914 - val_loss: 2.9650\n",
            "108/108 - 6s - loss: 2.9294 - val_loss: 3.0160\n",
            "108/108 - 6s - loss: 2.9527 - val_loss: 2.9141\n",
            "108/108 - 6s - loss: 2.8923 - val_loss: 2.9185\n",
            "108/108 - 6s - loss: 2.9346 - val_loss: 2.8936\n",
            "108/108 - 6s - loss: 2.8900 - val_loss: 2.8741\n",
            "108/108 - 6s - loss: 2.9560 - val_loss: 2.9851\n",
            "108/108 - 6s - loss: 2.9556 - val_loss: 2.9397\n",
            "108/108 - 6s - loss: 2.9658 - val_loss: 2.9283\n",
            "108/108 - 6s - loss: 2.8755 - val_loss: 2.9156\n",
            "108/108 - 6s - loss: 2.8736 - val_loss: 2.9147\n",
            "108/108 - 6s - loss: 2.8954 - val_loss: 2.9273\n",
            "108/108 - 6s - loss: 2.8667 - val_loss: 3.0661\n",
            "108/108 - 6s - loss: 2.8754 - val_loss: 2.9430\n",
            "108/108 - 6s - loss: 2.8881 - val_loss: 2.9573\n",
            "108/108 - 6s - loss: 2.8915 - val_loss: 2.9892\n",
            "108/108 - 6s - loss: 2.8977 - val_loss: 2.9609\n",
            "108/108 - 6s - loss: 2.8939 - val_loss: 2.9179\n",
            "108/108 - 6s - loss: 2.9288 - val_loss: 3.0110\n",
            "108/108 - 6s - loss: 2.9044 - val_loss: 2.9899\n",
            "108/108 - 6s - loss: 2.8733 - val_loss: 2.9363\n",
            "108/108 - 6s - loss: 2.9060 - val_loss: 2.9571\n",
            "108/108 - 6s - loss: 2.9123 - val_loss: 2.9954\n",
            "108/108 - 6s - loss: 2.9320 - val_loss: 2.9220\n",
            "108/108 - 6s - loss: 2.8663 - val_loss: 2.9737\n",
            "108/108 - 6s - loss: 2.8874 - val_loss: 2.9229\n",
            "108/108 - 6s - loss: 2.9062 - val_loss: 2.9222\n",
            "108/108 - 6s - loss: 2.8632 - val_loss: 3.0266\n",
            "108/108 - 6s - loss: 2.8757 - val_loss: 3.0691\n",
            "108/108 - 6s - loss: 2.8876 - val_loss: 3.0300\n",
            "108/108 - 6s - loss: 2.8708 - val_loss: 2.9782\n",
            "108/108 - 6s - loss: 2.9019 - val_loss: 2.9721\n",
            "108/108 - 6s - loss: 2.9017 - val_loss: 2.9467\n",
            "108/108 - 6s - loss: 2.8838 - val_loss: 3.0564\n",
            "108/108 - 6s - loss: 2.8722 - val_loss: 2.9639\n",
            "108/108 - 6s - loss: 2.8371 - val_loss: 2.9947\n",
            "108/108 - 6s - loss: 2.8784 - val_loss: 2.9457\n",
            "108/108 - 6s - loss: 2.8951 - val_loss: 2.9731\n",
            "108/108 - 6s - loss: 2.8541 - val_loss: 2.9247\n",
            "108/108 - 6s - loss: 2.8823 - val_loss: 3.0328\n",
            "108/108 - 6s - loss: 2.9634 - val_loss: 2.9309\n",
            "108/108 - 6s - loss: 2.8918 - val_loss: 2.8906\n",
            "108/108 - 6s - loss: 2.8218 - val_loss: 2.9378\n",
            "108/108 - 6s - loss: 2.9200 - val_loss: 2.9953\n",
            "108/108 - 6s - loss: 2.9021 - val_loss: 3.0144\n",
            "108/108 - 6s - loss: 2.8867 - val_loss: 2.9542\n",
            "108/108 - 6s - loss: 2.8687 - val_loss: 2.9576\n",
            "108/108 - 6s - loss: 2.8922 - val_loss: 2.9355\n",
            "108/108 - 6s - loss: 2.8567 - val_loss: 2.9245\n",
            "108/108 - 6s - loss: 2.8463 - val_loss: 2.9712\n",
            "108/108 - 6s - loss: 2.8371 - val_loss: 2.9548\n",
            "108/108 - 6s - loss: 2.8433 - val_loss: 2.9913\n",
            "108/108 - 6s - loss: 2.9124 - val_loss: 2.9702\n",
            "108/108 - 6s - loss: 2.8562 - val_loss: 2.9210\n",
            "108/108 - 6s - loss: 2.8382 - val_loss: 2.9199\n",
            "108/108 - 6s - loss: 2.8505 - val_loss: 3.0287\n",
            "108/108 - 6s - loss: 2.7940 - val_loss: 2.9329\n",
            "108/108 - 6s - loss: 2.8367 - val_loss: 2.9282\n",
            "108/108 - 6s - loss: 2.8482 - val_loss: 2.9588\n",
            "108/108 - 6s - loss: 2.8719 - val_loss: 2.9301\n",
            "108/108 - 6s - loss: 2.8602 - val_loss: 3.0286\n",
            "108/108 - 6s - loss: 2.8711 - val_loss: 2.9402\n",
            "108/108 - 6s - loss: 2.8459 - val_loss: 2.9155\n",
            "108/108 - 6s - loss: 2.8978 - val_loss: 2.9302\n",
            "108/108 - 6s - loss: 2.8845 - val_loss: 2.8659\n",
            "108/108 - 6s - loss: 2.8316 - val_loss: 2.9495\n",
            "108/108 - 6s - loss: 2.8497 - val_loss: 2.9129\n",
            "108/108 - 6s - loss: 2.8688 - val_loss: 2.9235\n",
            "108/108 - 6s - loss: 2.8437 - val_loss: 2.9151\n",
            "108/108 - 6s - loss: 2.8514 - val_loss: 2.9695\n",
            "108/108 - 6s - loss: 2.8152 - val_loss: 3.0159\n",
            "108/108 - 6s - loss: 2.8532 - val_loss: 2.9735\n",
            "108/108 - 6s - loss: 2.8504 - val_loss: 2.9630\n",
            "108/108 - 6s - loss: 2.8415 - val_loss: 2.9021\n",
            "108/108 - 6s - loss: 2.8477 - val_loss: 2.8901\n",
            "108/108 - 6s - loss: 2.8516 - val_loss: 2.9667\n",
            "108/108 - 6s - loss: 2.8415 - val_loss: 2.8915\n",
            "108/108 - 6s - loss: 2.8247 - val_loss: 2.8728\n",
            "108/108 - 6s - loss: 2.8022 - val_loss: 2.9590\n",
            "108/108 - 6s - loss: 2.8619 - val_loss: 2.9112\n",
            "108/108 - 6s - loss: 2.8619 - val_loss: 2.9207\n",
            "108/108 - 6s - loss: 2.8713 - val_loss: 2.9714\n",
            "108/108 - 6s - loss: 2.8323 - val_loss: 2.9724\n",
            "108/108 - 6s - loss: 2.8284 - val_loss: 2.9181\n",
            "108/108 - 6s - loss: 2.8640 - val_loss: 2.9359\n",
            "108/108 - 6s - loss: 2.8053 - val_loss: 2.9181\n",
            "108/108 - 6s - loss: 2.8077 - val_loss: 2.9486\n",
            "108/108 - 6s - loss: 2.8257 - val_loss: 2.9165\n",
            "108/108 - 6s - loss: 2.8398 - val_loss: 2.9765\n",
            "108/108 - 6s - loss: 2.8176 - val_loss: 2.9352\n",
            "108/108 - 6s - loss: 2.8115 - val_loss: 2.9388\n",
            "108/108 - 6s - loss: 2.8713 - val_loss: 2.9725\n",
            "108/108 - 6s - loss: 2.7727 - val_loss: 3.0216\n",
            "108/108 - 6s - loss: 2.7883 - val_loss: 3.0328\n",
            "108/108 - 6s - loss: 2.8025 - val_loss: 3.0231\n",
            "108/108 - 6s - loss: 2.8226 - val_loss: 3.0203\n",
            "108/108 - 6s - loss: 2.8418 - val_loss: 2.8962\n",
            "108/108 - 6s - loss: 2.7926 - val_loss: 3.0346\n",
            "108/108 - 6s - loss: 2.7989 - val_loss: 2.9433\n",
            "108/108 - 6s - loss: 2.8003 - val_loss: 2.8773\n",
            "108/108 - 6s - loss: 2.8405 - val_loss: 3.0039\n",
            "108/108 - 6s - loss: 2.8516 - val_loss: 2.8507\n",
            "108/108 - 6s - loss: 2.8561 - val_loss: 2.9345\n",
            "108/108 - 6s - loss: 2.8142 - val_loss: 2.9341\n",
            "108/108 - 6s - loss: 2.7946 - val_loss: 2.9496\n",
            "108/108 - 6s - loss: 2.8375 - val_loss: 2.9421\n",
            "108/108 - 6s - loss: 2.8136 - val_loss: 2.9518\n",
            "0.6\n",
            "108/108 - 13s - loss: 10.4423 - val_loss: 10.4452\n",
            "108/108 - 6s - loss: 9.8664 - val_loss: 9.9695\n",
            "108/108 - 6s - loss: 9.4310 - val_loss: 9.5602\n",
            "108/108 - 6s - loss: 9.0458 - val_loss: 9.1881\n",
            "108/108 - 6s - loss: 8.6942 - val_loss: 8.8442\n",
            "108/108 - 6s - loss: 8.3413 - val_loss: 8.5192\n",
            "108/108 - 6s - loss: 8.0361 - val_loss: 8.2141\n",
            "108/108 - 6s - loss: 7.7630 - val_loss: 7.9288\n",
            "108/108 - 6s - loss: 7.4857 - val_loss: 7.6585\n",
            "108/108 - 6s - loss: 7.2440 - val_loss: 7.4016\n",
            "108/108 - 6s - loss: 7.0053 - val_loss: 7.1603\n",
            "108/108 - 6s - loss: 6.7808 - val_loss: 6.9314\n",
            "108/108 - 6s - loss: 6.5277 - val_loss: 6.7136\n",
            "108/108 - 6s - loss: 6.3348 - val_loss: 6.5102\n",
            "108/108 - 6s - loss: 6.1748 - val_loss: 6.3192\n",
            "108/108 - 6s - loss: 5.9956 - val_loss: 6.1385\n",
            "108/108 - 6s - loss: 5.7991 - val_loss: 5.9669\n",
            "108/108 - 6s - loss: 5.6489 - val_loss: 5.8054\n",
            "108/108 - 6s - loss: 5.5515 - val_loss: 5.6568\n",
            "108/108 - 6s - loss: 5.3607 - val_loss: 5.5120\n",
            "108/108 - 6s - loss: 5.2247 - val_loss: 5.3792\n",
            "108/108 - 6s - loss: 5.1724 - val_loss: 5.2568\n",
            "108/108 - 6s - loss: 5.0896 - val_loss: 5.1431\n",
            "108/108 - 6s - loss: 5.0003 - val_loss: 5.0381\n",
            "108/108 - 6s - loss: 4.8753 - val_loss: 4.9430\n",
            "108/108 - 6s - loss: 4.8499 - val_loss: 4.8556\n",
            "108/108 - 6s - loss: 4.7537 - val_loss: 4.7765\n",
            "108/108 - 6s - loss: 4.6487 - val_loss: 4.7025\n",
            "108/108 - 6s - loss: 4.6181 - val_loss: 4.6359\n",
            "108/108 - 6s - loss: 4.5883 - val_loss: 4.5779\n",
            "108/108 - 6s - loss: 4.5943 - val_loss: 4.5242\n",
            "108/108 - 6s - loss: 4.4820 - val_loss: 4.4773\n",
            "108/108 - 6s - loss: 4.4022 - val_loss: 4.4351\n",
            "108/108 - 6s - loss: 4.4506 - val_loss: 4.3993\n",
            "108/108 - 6s - loss: 4.4226 - val_loss: 4.3667\n",
            "108/108 - 6s - loss: 4.3662 - val_loss: 4.3350\n",
            "108/108 - 6s - loss: 4.3244 - val_loss: 4.3074\n",
            "108/108 - 6s - loss: 4.3274 - val_loss: 4.2831\n",
            "108/108 - 6s - loss: 4.3105 - val_loss: 4.2606\n",
            "108/108 - 6s - loss: 4.2649 - val_loss: 4.2425\n",
            "108/108 - 6s - loss: 4.2552 - val_loss: 4.2258\n",
            "108/108 - 6s - loss: 4.2564 - val_loss: 4.2105\n",
            "108/108 - 6s - loss: 4.2705 - val_loss: 4.1984\n",
            "108/108 - 6s - loss: 4.2796 - val_loss: 4.1877\n",
            "108/108 - 6s - loss: 4.2266 - val_loss: 4.1773\n",
            "108/108 - 6s - loss: 4.1891 - val_loss: 4.1662\n",
            "108/108 - 6s - loss: 4.1586 - val_loss: 4.1556\n",
            "108/108 - 6s - loss: 4.1665 - val_loss: 4.1478\n",
            "108/108 - 6s - loss: 4.2449 - val_loss: 4.1410\n",
            "108/108 - 6s - loss: 4.2410 - val_loss: 4.1363\n",
            "108/108 - 6s - loss: 4.1828 - val_loss: 4.1298\n",
            "108/108 - 6s - loss: 4.2023 - val_loss: 4.1252\n",
            "108/108 - 6s - loss: 4.1674 - val_loss: 4.1214\n",
            "108/108 - 6s - loss: 4.2371 - val_loss: 4.1184\n",
            "108/108 - 6s - loss: 4.1913 - val_loss: 4.1174\n",
            "108/108 - 6s - loss: 4.2219 - val_loss: 4.1130\n",
            "108/108 - 6s - loss: 4.1661 - val_loss: 4.1086\n",
            "108/108 - 6s - loss: 4.2235 - val_loss: 4.1081\n",
            "108/108 - 6s - loss: 4.2438 - val_loss: 4.1067\n",
            "108/108 - 6s - loss: 4.1922 - val_loss: 4.1033\n",
            "108/108 - 6s - loss: 4.2223 - val_loss: 4.1012\n",
            "108/108 - 6s - loss: 4.2186 - val_loss: 4.0992\n",
            "108/108 - 6s - loss: 4.1790 - val_loss: 4.0989\n",
            "108/108 - 6s - loss: 4.2204 - val_loss: 4.0957\n",
            "108/108 - 6s - loss: 4.1704 - val_loss: 4.0945\n",
            "108/108 - 6s - loss: 4.2253 - val_loss: 4.0958\n",
            "108/108 - 6s - loss: 4.2231 - val_loss: 4.0951\n",
            "108/108 - 6s - loss: 4.2312 - val_loss: 4.0933\n",
            "108/108 - 6s - loss: 4.1934 - val_loss: 4.0935\n",
            "108/108 - 6s - loss: 4.1631 - val_loss: 4.0932\n",
            "108/108 - 6s - loss: 4.2553 - val_loss: 4.0929\n",
            "108/108 - 6s - loss: 4.1222 - val_loss: 4.0909\n",
            "108/108 - 6s - loss: 4.2458 - val_loss: 4.0906\n",
            "108/108 - 6s - loss: 4.2271 - val_loss: 4.0895\n",
            "108/108 - 6s - loss: 4.2646 - val_loss: 4.0901\n",
            "108/108 - 6s - loss: 4.2057 - val_loss: 4.0905\n",
            "108/108 - 6s - loss: 4.2162 - val_loss: 4.0892\n",
            "108/108 - 6s - loss: 4.1704 - val_loss: 4.0886\n",
            "108/108 - 6s - loss: 4.2524 - val_loss: 4.0887\n",
            "108/108 - 6s - loss: 4.1884 - val_loss: 4.0895\n",
            "108/108 - 6s - loss: 4.1112 - val_loss: 4.0879\n",
            "108/108 - 6s - loss: 4.2082 - val_loss: 4.0898\n",
            "108/108 - 6s - loss: 4.1619 - val_loss: 4.0886\n",
            "108/108 - 6s - loss: 4.2782 - val_loss: 4.0899\n",
            "108/108 - 6s - loss: 4.2071 - val_loss: 4.0903\n",
            "108/108 - 6s - loss: 4.1905 - val_loss: 4.0914\n",
            "108/108 - 6s - loss: 4.2561 - val_loss: 4.0914\n",
            "108/108 - 6s - loss: 4.2477 - val_loss: 4.0905\n",
            "108/108 - 6s - loss: 4.2021 - val_loss: 4.0903\n",
            "108/108 - 6s - loss: 4.2295 - val_loss: 4.0894\n",
            "108/108 - 6s - loss: 4.2307 - val_loss: 4.0892\n",
            "108/108 - 6s - loss: 4.1662 - val_loss: 4.0889\n",
            "108/108 - 6s - loss: 4.2680 - val_loss: 4.0886\n",
            "108/108 - 6s - loss: 4.2003 - val_loss: 4.0873\n",
            "108/108 - 6s - loss: 4.2571 - val_loss: 4.0886\n",
            "108/108 - 6s - loss: 4.2021 - val_loss: 4.0906\n",
            "108/108 - 6s - loss: 4.2290 - val_loss: 4.0918\n",
            "108/108 - 6s - loss: 4.2404 - val_loss: 4.0907\n",
            "108/108 - 6s - loss: 4.1959 - val_loss: 4.0893\n",
            "108/108 - 6s - loss: 4.2305 - val_loss: 4.0896\n",
            "108/108 - 6s - loss: 4.2204 - val_loss: 4.0906\n",
            "108/108 - 6s - loss: 4.2110 - val_loss: 4.0923\n",
            "108/108 - 6s - loss: 4.2604 - val_loss: 4.0924\n",
            "108/108 - 6s - loss: 4.2271 - val_loss: 4.0923\n",
            "108/108 - 6s - loss: 4.2091 - val_loss: 4.0923\n",
            "108/108 - 6s - loss: 4.1677 - val_loss: 4.0902\n",
            "108/108 - 6s - loss: 4.2051 - val_loss: 4.0900\n",
            "108/108 - 6s - loss: 4.2238 - val_loss: 4.0897\n",
            "108/108 - 6s - loss: 4.2018 - val_loss: 4.0910\n",
            "108/108 - 6s - loss: 4.1663 - val_loss: 4.0889\n",
            "108/108 - 6s - loss: 4.1671 - val_loss: 4.0894\n",
            "108/108 - 6s - loss: 4.2254 - val_loss: 4.0896\n",
            "108/108 - 6s - loss: 4.2033 - val_loss: 4.0888\n",
            "108/108 - 6s - loss: 4.2141 - val_loss: 4.0883\n",
            "108/108 - 6s - loss: 4.1613 - val_loss: 4.0899\n",
            "108/108 - 6s - loss: 4.1917 - val_loss: 4.0900\n",
            "108/108 - 6s - loss: 4.2274 - val_loss: 4.0896\n",
            "108/108 - 6s - loss: 4.1856 - val_loss: 4.0894\n",
            "108/108 - 6s - loss: 4.1507 - val_loss: 4.0878\n",
            "108/108 - 6s - loss: 4.1597 - val_loss: 4.0873\n",
            "108/108 - 6s - loss: 4.2117 - val_loss: 4.0875\n",
            "108/108 - 6s - loss: 4.1408 - val_loss: 4.0889\n",
            "108/108 - 6s - loss: 4.2080 - val_loss: 4.0900\n",
            "108/108 - 6s - loss: 4.1571 - val_loss: 4.0880\n",
            "108/108 - 6s - loss: 4.1666 - val_loss: 4.0870\n",
            "108/108 - 6s - loss: 4.1965 - val_loss: 4.0872\n",
            "108/108 - 6s - loss: 4.2480 - val_loss: 4.0879\n",
            "108/108 - 6s - loss: 4.1586 - val_loss: 4.0874\n",
            "108/108 - 6s - loss: 4.2260 - val_loss: 4.0867\n",
            "108/108 - 6s - loss: 4.1702 - val_loss: 4.0870\n",
            "108/108 - 6s - loss: 4.2336 - val_loss: 4.0875\n",
            "108/108 - 6s - loss: 4.2088 - val_loss: 4.0884\n",
            "108/108 - 6s - loss: 4.2051 - val_loss: 4.0879\n",
            "108/108 - 6s - loss: 4.2133 - val_loss: 4.0866\n",
            "108/108 - 6s - loss: 4.1858 - val_loss: 4.0870\n",
            "108/108 - 6s - loss: 4.2331 - val_loss: 4.0871\n",
            "108/108 - 6s - loss: 4.1652 - val_loss: 4.0867\n",
            "108/108 - 6s - loss: 4.1585 - val_loss: 4.0883\n",
            "108/108 - 6s - loss: 4.1268 - val_loss: 4.0873\n",
            "108/108 - 6s - loss: 4.2338 - val_loss: 4.0876\n",
            "108/108 - 6s - loss: 4.1871 - val_loss: 4.0881\n",
            "108/108 - 6s - loss: 4.1437 - val_loss: 4.0876\n",
            "108/108 - 6s - loss: 4.2161 - val_loss: 4.0881\n",
            "108/108 - 6s - loss: 4.2084 - val_loss: 4.0884\n",
            "108/108 - 6s - loss: 4.2341 - val_loss: 4.0877\n",
            "108/108 - 6s - loss: 4.1699 - val_loss: 4.0881\n",
            "108/108 - 6s - loss: 4.2250 - val_loss: 4.0890\n",
            "108/108 - 6s - loss: 4.1904 - val_loss: 4.0880\n",
            "108/108 - 6s - loss: 4.2419 - val_loss: 4.0871\n",
            "108/108 - 6s - loss: 4.1669 - val_loss: 4.0876\n",
            "108/108 - 6s - loss: 4.2024 - val_loss: 4.0884\n",
            "108/108 - 6s - loss: 4.2093 - val_loss: 4.0887\n",
            "108/108 - 6s - loss: 4.1737 - val_loss: 4.0874\n",
            "108/108 - 6s - loss: 4.2376 - val_loss: 4.0883\n",
            "108/108 - 6s - loss: 4.2030 - val_loss: 4.0889\n",
            "108/108 - 6s - loss: 4.2069 - val_loss: 4.0877\n",
            "108/108 - 6s - loss: 4.2212 - val_loss: 4.0876\n",
            "108/108 - 6s - loss: 4.1794 - val_loss: 4.0876\n",
            "108/108 - 6s - loss: 4.1757 - val_loss: 4.0867\n",
            "108/108 - 6s - loss: 4.1751 - val_loss: 4.0868\n",
            "108/108 - 6s - loss: 4.2130 - val_loss: 4.0871\n",
            "108/108 - 6s - loss: 4.1498 - val_loss: 4.0848\n",
            "108/108 - 6s - loss: 4.1975 - val_loss: 4.0868\n",
            "108/108 - 6s - loss: 4.1932 - val_loss: 4.0867\n",
            "108/108 - 6s - loss: 4.1936 - val_loss: 4.0873\n",
            "108/108 - 6s - loss: 4.2186 - val_loss: 4.0874\n",
            "108/108 - 6s - loss: 4.2052 - val_loss: 4.0879\n",
            "108/108 - 6s - loss: 4.2114 - val_loss: 4.0877\n",
            "108/108 - 6s - loss: 4.1553 - val_loss: 4.0879\n",
            "108/108 - 6s - loss: 4.2493 - val_loss: 4.0890\n",
            "108/108 - 6s - loss: 4.2082 - val_loss: 4.0906\n",
            "108/108 - 6s - loss: 4.2118 - val_loss: 4.0902\n",
            "108/108 - 6s - loss: 4.1675 - val_loss: 4.0905\n",
            "108/108 - 6s - loss: 4.2042 - val_loss: 4.0917\n",
            "108/108 - 6s - loss: 4.2142 - val_loss: 4.0905\n",
            "108/108 - 6s - loss: 4.2362 - val_loss: 4.0903\n",
            "108/108 - 6s - loss: 4.1989 - val_loss: 4.0891\n",
            "108/108 - 6s - loss: 4.1914 - val_loss: 4.0898\n",
            "108/108 - 6s - loss: 4.2308 - val_loss: 4.0905\n",
            "108/108 - 6s - loss: 4.1498 - val_loss: 4.0909\n",
            "108/108 - 6s - loss: 4.1606 - val_loss: 4.0890\n",
            "108/108 - 6s - loss: 4.2147 - val_loss: 4.0896\n",
            "108/108 - 6s - loss: 4.2064 - val_loss: 4.0900\n",
            "108/108 - 6s - loss: 4.1901 - val_loss: 4.0905\n",
            "108/108 - 6s - loss: 4.1795 - val_loss: 4.0916\n",
            "108/108 - 6s - loss: 4.2289 - val_loss: 4.0904\n",
            "108/108 - 6s - loss: 4.2687 - val_loss: 4.0904\n",
            "108/108 - 6s - loss: 4.2267 - val_loss: 4.0912\n",
            "108/108 - 6s - loss: 4.2033 - val_loss: 4.0910\n",
            "108/108 - 6s - loss: 4.1632 - val_loss: 4.0901\n",
            "108/108 - 6s - loss: 4.1600 - val_loss: 4.0903\n",
            "108/108 - 6s - loss: 4.1887 - val_loss: 4.0901\n",
            "108/108 - 6s - loss: 4.2092 - val_loss: 4.0898\n",
            "108/108 - 6s - loss: 4.1916 - val_loss: 4.0893\n",
            "108/108 - 6s - loss: 4.1191 - val_loss: 4.0887\n",
            "108/108 - 6s - loss: 4.2069 - val_loss: 4.0884\n",
            "108/108 - 6s - loss: 4.1750 - val_loss: 4.0888\n",
            "108/108 - 6s - loss: 4.1959 - val_loss: 4.0892\n",
            "108/108 - 6s - loss: 4.1990 - val_loss: 4.0894\n",
            "108/108 - 6s - loss: 4.5632 - val_loss: 4.5047\n",
            "108/108 - 6s - loss: 4.3718 - val_loss: 4.3216\n",
            "108/108 - 6s - loss: 4.2598 - val_loss: 4.2885\n",
            "108/108 - 6s - loss: 4.3178 - val_loss: 4.2621\n",
            "108/108 - 6s - loss: 4.3407 - val_loss: 4.2420\n",
            "108/108 - 6s - loss: 4.2757 - val_loss: 4.2248\n",
            "108/108 - 6s - loss: 4.3039 - val_loss: 4.2076\n",
            "108/108 - 6s - loss: 4.3074 - val_loss: 4.1961\n",
            "108/108 - 6s - loss: 4.2787 - val_loss: 4.1851\n",
            "108/108 - 6s - loss: 4.2779 - val_loss: 4.1764\n",
            "108/108 - 6s - loss: 4.2325 - val_loss: 4.1684\n",
            "108/108 - 6s - loss: 4.2854 - val_loss: 4.1605\n",
            "108/108 - 6s - loss: 4.2448 - val_loss: 4.1529\n",
            "108/108 - 6s - loss: 4.2180 - val_loss: 4.1466\n",
            "108/108 - 6s - loss: 4.2181 - val_loss: 4.1414\n",
            "108/108 - 6s - loss: 4.2239 - val_loss: 4.1358\n",
            "108/108 - 6s - loss: 4.2569 - val_loss: 4.1318\n",
            "108/108 - 6s - loss: 4.2456 - val_loss: 4.1260\n",
            "108/108 - 6s - loss: 4.2596 - val_loss: 4.1222\n",
            "108/108 - 6s - loss: 4.2044 - val_loss: 4.1198\n",
            "108/108 - 6s - loss: 4.2788 - val_loss: 4.1165\n",
            "108/108 - 6s - loss: 4.2205 - val_loss: 4.1130\n",
            "108/108 - 6s - loss: 4.2102 - val_loss: 4.1107\n",
            "108/108 - 6s - loss: 4.2728 - val_loss: 4.1092\n",
            "108/108 - 6s - loss: 4.2204 - val_loss: 4.1081\n",
            "108/108 - 6s - loss: 4.2577 - val_loss: 4.1070\n",
            "108/108 - 6s - loss: 4.2522 - val_loss: 4.1044\n",
            "108/108 - 6s - loss: 4.2019 - val_loss: 4.1033\n",
            "108/108 - 6s - loss: 4.1554 - val_loss: 4.1023\n",
            "108/108 - 6s - loss: 4.2130 - val_loss: 4.1021\n",
            "108/108 - 6s - loss: 4.2489 - val_loss: 4.1026\n",
            "108/108 - 6s - loss: 4.2378 - val_loss: 4.1027\n",
            "108/108 - 6s - loss: 4.2138 - val_loss: 4.1022\n",
            "108/108 - 6s - loss: 4.1747 - val_loss: 4.1013\n",
            "108/108 - 6s - loss: 4.2314 - val_loss: 4.1007\n",
            "108/108 - 6s - loss: 4.1964 - val_loss: 4.0989\n",
            "108/108 - 6s - loss: 4.3011 - val_loss: 4.0992\n",
            "108/108 - 6s - loss: 4.2895 - val_loss: 4.0970\n",
            "108/108 - 6s - loss: 4.3153 - val_loss: 4.0993\n",
            "108/108 - 6s - loss: 4.2408 - val_loss: 4.0930\n",
            "108/108 - 6s - loss: 4.2162 - val_loss: 4.0899\n",
            "108/108 - 6s - loss: 4.1404 - val_loss: 4.0932\n",
            "108/108 - 6s - loss: 4.2528 - val_loss: 4.0902\n",
            "108/108 - 6s - loss: 4.1901 - val_loss: 4.0868\n",
            "108/108 - 6s - loss: 4.2489 - val_loss: 4.0801\n",
            "108/108 - 6s - loss: 4.2545 - val_loss: 4.0714\n",
            "108/108 - 6s - loss: 4.1592 - val_loss: 3.9129\n",
            "108/108 - 6s - loss: 4.2081 - val_loss: 4.0605\n",
            "108/108 - 6s - loss: 4.2534 - val_loss: 4.0627\n",
            "108/108 - 6s - loss: 4.1999 - val_loss: 4.0477\n",
            "108/108 - 6s - loss: 4.1527 - val_loss: 3.8612\n",
            "108/108 - 6s - loss: 4.0735 - val_loss: 3.8278\n",
            "108/108 - 6s - loss: 3.9999 - val_loss: 3.8379\n",
            "108/108 - 6s - loss: 3.9728 - val_loss: 3.7647\n",
            "108/108 - 6s - loss: 4.0134 - val_loss: 3.7142\n",
            "108/108 - 6s - loss: 3.8253 - val_loss: 3.7148\n",
            "108/108 - 6s - loss: 3.8126 - val_loss: 3.4682\n",
            "108/108 - 6s - loss: 3.6865 - val_loss: 3.4239\n",
            "108/108 - 6s - loss: 3.6239 - val_loss: 3.3182\n",
            "108/108 - 6s - loss: 3.6497 - val_loss: 3.2568\n",
            "108/108 - 6s - loss: 3.5776 - val_loss: 3.2284\n",
            "108/108 - 6s - loss: 3.5322 - val_loss: 3.2332\n",
            "108/108 - 6s - loss: 3.5432 - val_loss: 3.2618\n",
            "108/108 - 6s - loss: 3.4703 - val_loss: 3.2093\n",
            "108/108 - 6s - loss: 3.4970 - val_loss: 3.1795\n",
            "108/108 - 6s - loss: 3.5134 - val_loss: 3.1933\n",
            "108/108 - 6s - loss: 3.4825 - val_loss: 3.2120\n",
            "108/108 - 6s - loss: 3.4345 - val_loss: 3.1751\n",
            "108/108 - 6s - loss: 3.3853 - val_loss: 3.1379\n",
            "108/108 - 6s - loss: 3.3681 - val_loss: 3.1383\n",
            "108/108 - 6s - loss: 3.3447 - val_loss: 3.0490\n",
            "108/108 - 6s - loss: 3.3188 - val_loss: 3.0280\n",
            "108/108 - 6s - loss: 3.3227 - val_loss: 2.9669\n",
            "108/108 - 6s - loss: 3.2708 - val_loss: 2.9656\n",
            "108/108 - 6s - loss: 3.3788 - val_loss: 2.9386\n",
            "108/108 - 6s - loss: 3.3097 - val_loss: 2.9501\n",
            "108/108 - 6s - loss: 3.2261 - val_loss: 2.9059\n",
            "108/108 - 6s - loss: 3.3414 - val_loss: 2.8965\n",
            "108/108 - 6s - loss: 3.2777 - val_loss: 2.9285\n",
            "108/108 - 6s - loss: 3.2794 - val_loss: 2.9014\n",
            "108/108 - 6s - loss: 3.2805 - val_loss: 2.8956\n",
            "108/108 - 6s - loss: 3.2921 - val_loss: 2.8587\n",
            "108/108 - 6s - loss: 3.2960 - val_loss: 2.8467\n",
            "108/108 - 6s - loss: 3.3187 - val_loss: 2.8647\n",
            "108/108 - 6s - loss: 3.3099 - val_loss: 2.8652\n",
            "108/108 - 6s - loss: 3.1854 - val_loss: 2.8177\n",
            "108/108 - 6s - loss: 3.3092 - val_loss: 2.8042\n",
            "108/108 - 6s - loss: 3.2989 - val_loss: 2.7927\n",
            "108/108 - 6s - loss: 3.2988 - val_loss: 2.8156\n",
            "108/108 - 6s - loss: 3.2494 - val_loss: 2.8262\n",
            "108/108 - 6s - loss: 3.2853 - val_loss: 2.8014\n",
            "108/108 - 6s - loss: 3.2080 - val_loss: 2.7848\n",
            "108/108 - 6s - loss: 3.1842 - val_loss: 2.7744\n",
            "108/108 - 6s - loss: 3.1867 - val_loss: 2.7692\n",
            "108/108 - 6s - loss: 3.2431 - val_loss: 2.7693\n",
            "108/108 - 6s - loss: 3.1989 - val_loss: 2.8842\n",
            "108/108 - 6s - loss: 3.1758 - val_loss: 2.8100\n",
            "108/108 - 6s - loss: 3.2682 - val_loss: 2.8144\n",
            "108/108 - 6s - loss: 3.2220 - val_loss: 2.7871\n",
            "108/108 - 6s - loss: 3.2004 - val_loss: 2.7570\n",
            "108/108 - 6s - loss: 3.1867 - val_loss: 2.7514\n",
            "108/108 - 6s - loss: 3.1845 - val_loss: 2.7812\n",
            "108/108 - 6s - loss: 3.2475 - val_loss: 2.8182\n",
            "108/108 - 6s - loss: 3.1490 - val_loss: 2.7488\n",
            "108/108 - 6s - loss: 3.1583 - val_loss: 2.7557\n",
            "108/108 - 6s - loss: 3.1237 - val_loss: 2.7666\n",
            "108/108 - 6s - loss: 3.1936 - val_loss: 2.7431\n",
            "108/108 - 6s - loss: 3.1843 - val_loss: 2.7894\n",
            "108/108 - 6s - loss: 3.2177 - val_loss: 2.7673\n",
            "108/108 - 6s - loss: 3.1677 - val_loss: 2.7799\n",
            "108/108 - 6s - loss: 3.2375 - val_loss: 2.7756\n",
            "108/108 - 6s - loss: 3.1192 - val_loss: 2.7880\n",
            "108/108 - 6s - loss: 3.2206 - val_loss: 2.7703\n",
            "108/108 - 6s - loss: 3.2027 - val_loss: 2.7139\n",
            "108/108 - 6s - loss: 3.2645 - val_loss: 2.7424\n",
            "108/108 - 6s - loss: 3.1396 - val_loss: 2.7596\n",
            "108/108 - 6s - loss: 3.1635 - val_loss: 2.7523\n",
            "108/108 - 6s - loss: 3.2283 - val_loss: 2.7407\n",
            "108/108 - 6s - loss: 3.1720 - val_loss: 2.7368\n",
            "108/108 - 6s - loss: 3.1490 - val_loss: 2.7571\n",
            "108/108 - 6s - loss: 3.1281 - val_loss: 2.7346\n",
            "108/108 - 6s - loss: 3.1863 - val_loss: 2.7260\n",
            "108/108 - 6s - loss: 3.1343 - val_loss: 2.7373\n",
            "108/108 - 6s - loss: 3.1727 - val_loss: 2.7426\n",
            "108/108 - 6s - loss: 3.1929 - val_loss: 2.7446\n",
            "108/108 - 6s - loss: 3.1379 - val_loss: 2.7754\n",
            "108/108 - 6s - loss: 3.1331 - val_loss: 2.7121\n",
            "108/108 - 6s - loss: 3.1131 - val_loss: 2.7245\n",
            "108/108 - 6s - loss: 3.1716 - val_loss: 2.6965\n",
            "108/108 - 6s - loss: 3.1000 - val_loss: 2.7423\n",
            "108/108 - 6s - loss: 3.1283 - val_loss: 2.6928\n",
            "108/108 - 6s - loss: 3.1321 - val_loss: 2.7577\n",
            "108/108 - 6s - loss: 3.1390 - val_loss: 2.6993\n",
            "108/108 - 6s - loss: 3.1136 - val_loss: 2.7533\n",
            "108/108 - 6s - loss: 3.1330 - val_loss: 2.7309\n",
            "108/108 - 6s - loss: 3.1488 - val_loss: 2.7014\n",
            "108/108 - 6s - loss: 3.1359 - val_loss: 2.7027\n",
            "108/108 - 6s - loss: 3.1327 - val_loss: 2.7544\n",
            "108/108 - 6s - loss: 3.0964 - val_loss: 2.6670\n",
            "108/108 - 6s - loss: 3.1404 - val_loss: 2.7243\n",
            "108/108 - 6s - loss: 3.1511 - val_loss: 2.6998\n",
            "108/108 - 6s - loss: 3.1734 - val_loss: 2.6913\n",
            "108/108 - 6s - loss: 3.1113 - val_loss: 2.6717\n",
            "108/108 - 6s - loss: 3.1204 - val_loss: 2.7115\n",
            "108/108 - 6s - loss: 3.0531 - val_loss: 2.7062\n",
            "108/108 - 6s - loss: 3.0507 - val_loss: 2.6857\n",
            "108/108 - 6s - loss: 3.1011 - val_loss: 2.7008\n",
            "108/108 - 6s - loss: 3.0760 - val_loss: 2.6934\n",
            "108/108 - 6s - loss: 3.1126 - val_loss: 2.7336\n",
            "108/108 - 6s - loss: 3.0195 - val_loss: 2.6607\n",
            "108/108 - 6s - loss: 3.0650 - val_loss: 2.6722\n",
            "108/108 - 6s - loss: 3.0630 - val_loss: 2.6651\n",
            "108/108 - 6s - loss: 3.1061 - val_loss: 2.6622\n",
            "108/108 - 6s - loss: 3.1035 - val_loss: 2.6736\n",
            "108/108 - 6s - loss: 3.0862 - val_loss: 2.6703\n",
            "108/108 - 6s - loss: 3.0507 - val_loss: 2.6537\n",
            "108/108 - 6s - loss: 3.0910 - val_loss: 2.6533\n",
            "108/108 - 6s - loss: 3.1112 - val_loss: 2.6542\n",
            "108/108 - 6s - loss: 3.0755 - val_loss: 2.6624\n",
            "108/108 - 6s - loss: 3.0524 - val_loss: 2.6915\n",
            "108/108 - 6s - loss: 3.0436 - val_loss: 2.6805\n",
            "108/108 - 6s - loss: 3.0735 - val_loss: 2.6954\n",
            "108/108 - 6s - loss: 3.0478 - val_loss: 2.7076\n",
            "108/108 - 6s - loss: 2.9972 - val_loss: 2.7643\n",
            "108/108 - 6s - loss: 3.0411 - val_loss: 2.6435\n",
            "108/108 - 6s - loss: 3.0707 - val_loss: 2.6941\n",
            "108/108 - 6s - loss: 3.0057 - val_loss: 2.7006\n",
            "108/108 - 6s - loss: 3.0573 - val_loss: 2.6674\n",
            "108/108 - 6s - loss: 3.0806 - val_loss: 2.6797\n",
            "108/108 - 6s - loss: 3.0090 - val_loss: 2.6660\n",
            "108/108 - 6s - loss: 3.0380 - val_loss: 2.7392\n",
            "108/108 - 6s - loss: 3.0196 - val_loss: 2.7015\n",
            "108/108 - 6s - loss: 3.0139 - val_loss: 2.6661\n",
            "108/108 - 6s - loss: 2.9796 - val_loss: 2.7016\n",
            "108/108 - 6s - loss: 3.0493 - val_loss: 2.7407\n",
            "108/108 - 6s - loss: 2.9937 - val_loss: 2.6714\n",
            "108/108 - 6s - loss: 3.0612 - val_loss: 2.6914\n",
            "108/108 - 6s - loss: 2.9693 - val_loss: 2.6637\n",
            "108/108 - 6s - loss: 2.9523 - val_loss: 2.6558\n",
            "108/108 - 6s - loss: 3.0155 - val_loss: 2.7242\n",
            "108/108 - 6s - loss: 3.0201 - val_loss: 2.6559\n",
            "108/108 - 6s - loss: 3.0156 - val_loss: 2.6635\n",
            "108/108 - 6s - loss: 2.9565 - val_loss: 2.6922\n",
            "108/108 - 6s - loss: 3.0535 - val_loss: 2.6874\n",
            "108/108 - 6s - loss: 2.9791 - val_loss: 2.7412\n",
            "108/108 - 6s - loss: 2.9811 - val_loss: 2.6904\n",
            "108/108 - 6s - loss: 2.9589 - val_loss: 2.6957\n",
            "108/108 - 6s - loss: 3.0189 - val_loss: 2.7054\n",
            "108/108 - 6s - loss: 2.9886 - val_loss: 2.7038\n",
            "108/108 - 6s - loss: 2.9615 - val_loss: 2.6749\n",
            "108/108 - 6s - loss: 2.9436 - val_loss: 2.6624\n",
            "108/108 - 6s - loss: 2.9707 - val_loss: 2.6718\n",
            "108/108 - 6s - loss: 2.9400 - val_loss: 2.6655\n",
            "108/108 - 6s - loss: 2.9717 - val_loss: 2.6582\n",
            "108/108 - 6s - loss: 2.9446 - val_loss: 2.6988\n",
            "108/108 - 6s - loss: 3.0207 - val_loss: 2.6874\n",
            "108/108 - 6s - loss: 2.9569 - val_loss: 2.6545\n",
            "108/108 - 6s - loss: 2.9984 - val_loss: 2.7328\n",
            "108/108 - 6s - loss: 2.9372 - val_loss: 2.6575\n",
            "108/108 - 6s - loss: 2.9436 - val_loss: 2.6420\n",
            "108/108 - 6s - loss: 2.8949 - val_loss: 2.6549\n",
            "108/108 - 6s - loss: 2.9240 - val_loss: 2.6750\n",
            "108/108 - 6s - loss: 2.9783 - val_loss: 2.6698\n",
            "108/108 - 6s - loss: 2.9458 - val_loss: 2.6456\n",
            "108/108 - 6s - loss: 2.9842 - val_loss: 2.6994\n",
            "108/108 - 6s - loss: 2.9563 - val_loss: 2.6820\n",
            "108/108 - 6s - loss: 2.9811 - val_loss: 2.7116\n",
            "108/108 - 6s - loss: 2.9356 - val_loss: 2.6402\n",
            "108/108 - 6s - loss: 2.9460 - val_loss: 2.6337\n",
            "108/108 - 6s - loss: 2.8895 - val_loss: 2.6672\n",
            "108/108 - 6s - loss: 2.9376 - val_loss: 2.6112\n",
            "108/108 - 6s - loss: 2.9784 - val_loss: 2.6488\n",
            "108/108 - 6s - loss: 2.9176 - val_loss: 2.6542\n",
            "108/108 - 6s - loss: 2.9161 - val_loss: 2.6628\n",
            "108/108 - 6s - loss: 2.9454 - val_loss: 2.6326\n",
            "108/108 - 6s - loss: 2.9842 - val_loss: 2.6044\n",
            "108/108 - 6s - loss: 2.8377 - val_loss: 2.6991\n",
            "108/108 - 6s - loss: 2.8879 - val_loss: 2.6750\n",
            "108/108 - 6s - loss: 2.8504 - val_loss: 2.6373\n",
            "108/108 - 6s - loss: 2.8983 - val_loss: 2.6569\n",
            "108/108 - 6s - loss: 2.8936 - val_loss: 2.6514\n",
            "108/108 - 6s - loss: 2.9055 - val_loss: 2.6254\n",
            "108/108 - 6s - loss: 2.9280 - val_loss: 2.6440\n",
            "108/108 - 6s - loss: 2.9257 - val_loss: 2.6502\n",
            "108/108 - 6s - loss: 2.9988 - val_loss: 2.6426\n",
            "108/108 - 6s - loss: 2.9145 - val_loss: 2.6635\n",
            "108/108 - 6s - loss: 2.8268 - val_loss: 2.6360\n",
            "108/108 - 6s - loss: 2.9046 - val_loss: 2.6323\n",
            "108/108 - 6s - loss: 2.8898 - val_loss: 2.6226\n",
            "108/108 - 6s - loss: 2.8905 - val_loss: 2.6503\n",
            "108/108 - 6s - loss: 2.9389 - val_loss: 2.6043\n",
            "108/108 - 6s - loss: 2.8866 - val_loss: 2.6144\n",
            "108/108 - 6s - loss: 2.8875 - val_loss: 2.6255\n",
            "108/108 - 6s - loss: 2.9541 - val_loss: 2.6250\n",
            "108/108 - 6s - loss: 2.9325 - val_loss: 2.6064\n",
            "108/108 - 6s - loss: 2.9417 - val_loss: 2.6603\n",
            "108/108 - 6s - loss: 2.8876 - val_loss: 2.6547\n",
            "108/108 - 6s - loss: 2.9041 - val_loss: 2.6754\n",
            "108/108 - 6s - loss: 2.8593 - val_loss: 2.5956\n",
            "108/108 - 6s - loss: 2.9423 - val_loss: 2.6113\n",
            "108/108 - 6s - loss: 2.8719 - val_loss: 2.6084\n",
            "108/108 - 6s - loss: 2.8300 - val_loss: 2.6221\n",
            "108/108 - 6s - loss: 2.9009 - val_loss: 2.6344\n",
            "108/108 - 6s - loss: 2.8281 - val_loss: 2.6396\n",
            "108/108 - 6s - loss: 2.8520 - val_loss: 2.6632\n",
            "108/108 - 6s - loss: 2.8404 - val_loss: 2.6305\n",
            "108/108 - 6s - loss: 2.8571 - val_loss: 2.6595\n",
            "108/108 - 6s - loss: 2.8998 - val_loss: 2.6087\n",
            "108/108 - 6s - loss: 2.9068 - val_loss: 2.6325\n",
            "108/108 - 6s - loss: 2.8631 - val_loss: 2.6309\n",
            "108/108 - 6s - loss: 2.8427 - val_loss: 2.6529\n",
            "108/108 - 6s - loss: 2.8531 - val_loss: 2.6225\n",
            "108/108 - 6s - loss: 2.8790 - val_loss: 2.6509\n",
            "108/108 - 6s - loss: 2.8275 - val_loss: 2.6544\n",
            "108/108 - 6s - loss: 2.8566 - val_loss: 2.6989\n",
            "108/108 - 6s - loss: 2.8539 - val_loss: 2.6249\n",
            "108/108 - 6s - loss: 2.8861 - val_loss: 2.6269\n",
            "108/108 - 6s - loss: 2.8375 - val_loss: 2.6766\n",
            "108/108 - 6s - loss: 2.8662 - val_loss: 2.6712\n",
            "108/108 - 6s - loss: 2.8186 - val_loss: 2.6519\n",
            "108/108 - 6s - loss: 2.8961 - val_loss: 2.6688\n",
            "108/108 - 6s - loss: 2.9048 - val_loss: 2.6474\n",
            "108/108 - 6s - loss: 2.8268 - val_loss: 2.6326\n",
            "108/108 - 6s - loss: 2.8514 - val_loss: 2.7353\n",
            "108/108 - 6s - loss: 2.8424 - val_loss: 2.6977\n",
            "108/108 - 6s - loss: 2.8355 - val_loss: 2.5888\n",
            "108/108 - 6s - loss: 2.8748 - val_loss: 2.6725\n",
            "108/108 - 6s - loss: 2.8761 - val_loss: 2.6943\n",
            "108/108 - 6s - loss: 2.8192 - val_loss: 2.6787\n",
            "108/108 - 6s - loss: 2.8507 - val_loss: 2.6194\n",
            "108/108 - 6s - loss: 2.8534 - val_loss: 2.7127\n",
            "108/108 - 6s - loss: 2.8315 - val_loss: 2.6262\n",
            "108/108 - 6s - loss: 2.8350 - val_loss: 2.6741\n",
            "108/108 - 6s - loss: 2.8086 - val_loss: 2.6146\n",
            "108/108 - 6s - loss: 2.7951 - val_loss: 2.6506\n",
            "108/108 - 6s - loss: 2.8028 - val_loss: 2.6663\n",
            "108/108 - 6s - loss: 2.8158 - val_loss: 2.7228\n",
            "108/108 - 6s - loss: 2.8496 - val_loss: 2.6678\n",
            "108/108 - 6s - loss: 2.8555 - val_loss: 2.6285\n",
            "108/108 - 6s - loss: 2.8101 - val_loss: 2.6931\n",
            "108/108 - 6s - loss: 2.8558 - val_loss: 2.6799\n",
            "108/108 - 6s - loss: 2.8442 - val_loss: 2.5580\n",
            "108/108 - 6s - loss: 2.8882 - val_loss: 2.6088\n",
            "108/108 - 6s - loss: 2.8564 - val_loss: 2.6265\n",
            "108/108 - 6s - loss: 2.9361 - val_loss: 2.7267\n",
            "108/108 - 6s - loss: 2.8437 - val_loss: 2.6550\n",
            "108/108 - 6s - loss: 2.8587 - val_loss: 2.6377\n",
            "108/108 - 6s - loss: 2.8343 - val_loss: 2.7385\n",
            "108/108 - 6s - loss: 2.8480 - val_loss: 2.6573\n",
            "108/108 - 6s - loss: 2.8102 - val_loss: 2.6656\n",
            "108/108 - 6s - loss: 2.8122 - val_loss: 2.6526\n",
            "108/108 - 6s - loss: 2.7883 - val_loss: 2.6623\n",
            "108/108 - 6s - loss: 2.8370 - val_loss: 2.6595\n",
            "108/108 - 6s - loss: 2.8320 - val_loss: 2.6452\n",
            "108/108 - 6s - loss: 2.8211 - val_loss: 2.7275\n",
            "108/108 - 6s - loss: 2.8352 - val_loss: 2.6690\n",
            "108/108 - 6s - loss: 2.7905 - val_loss: 2.6623\n",
            "108/108 - 6s - loss: 2.8278 - val_loss: 2.6525\n",
            "108/108 - 6s - loss: 2.8422 - val_loss: 2.6980\n",
            "108/108 - 6s - loss: 2.7794 - val_loss: 2.6906\n",
            "108/108 - 6s - loss: 2.7992 - val_loss: 2.6580\n",
            "0.7000000000000001\n",
            "108/108 - 13s - loss: 12.2061 - val_loss: 12.2148\n",
            "108/108 - 6s - loss: 11.5132 - val_loss: 11.6148\n",
            "108/108 - 6s - loss: 10.9754 - val_loss: 11.1088\n",
            "108/108 - 6s - loss: 10.4735 - val_loss: 10.6420\n",
            "108/108 - 6s - loss: 10.0367 - val_loss: 10.2094\n",
            "108/108 - 6s - loss: 9.6206 - val_loss: 9.8041\n",
            "108/108 - 6s - loss: 9.2468 - val_loss: 9.4224\n",
            "108/108 - 6s - loss: 8.8829 - val_loss: 9.0612\n",
            "108/108 - 6s - loss: 8.4890 - val_loss: 8.7141\n",
            "108/108 - 6s - loss: 8.2047 - val_loss: 8.3882\n",
            "108/108 - 6s - loss: 7.8603 - val_loss: 8.0746\n",
            "108/108 - 6s - loss: 7.5879 - val_loss: 7.7799\n",
            "108/108 - 6s - loss: 7.3072 - val_loss: 7.4985\n",
            "108/108 - 6s - loss: 7.0292 - val_loss: 7.2325\n",
            "108/108 - 6s - loss: 6.7955 - val_loss: 6.9790\n",
            "108/108 - 6s - loss: 6.5990 - val_loss: 6.7406\n",
            "108/108 - 6s - loss: 6.3417 - val_loss: 6.5117\n",
            "108/108 - 6s - loss: 6.1852 - val_loss: 6.2970\n",
            "108/108 - 6s - loss: 5.9169 - val_loss: 6.0913\n",
            "108/108 - 6s - loss: 5.7836 - val_loss: 5.8974\n",
            "108/108 - 6s - loss: 5.6004 - val_loss: 5.7141\n",
            "108/108 - 6s - loss: 5.4733 - val_loss: 5.5428\n",
            "108/108 - 6s - loss: 5.3518 - val_loss: 5.3829\n",
            "108/108 - 6s - loss: 5.1711 - val_loss: 5.2339\n",
            "108/108 - 6s - loss: 5.0312 - val_loss: 5.0941\n",
            "108/108 - 6s - loss: 4.9750 - val_loss: 4.9651\n",
            "108/108 - 6s - loss: 4.8557 - val_loss: 4.8488\n",
            "108/108 - 6s - loss: 4.7565 - val_loss: 4.7398\n",
            "108/108 - 6s - loss: 4.5959 - val_loss: 4.6389\n",
            "108/108 - 6s - loss: 4.5653 - val_loss: 4.5482\n",
            "108/108 - 6s - loss: 4.5222 - val_loss: 4.4657\n",
            "108/108 - 6s - loss: 4.3840 - val_loss: 4.3881\n",
            "108/108 - 6s - loss: 4.3679 - val_loss: 4.3177\n",
            "108/108 - 6s - loss: 4.2793 - val_loss: 4.2508\n",
            "108/108 - 6s - loss: 4.2937 - val_loss: 4.1909\n",
            "108/108 - 6s - loss: 4.2823 - val_loss: 4.1359\n",
            "108/108 - 6s - loss: 4.2137 - val_loss: 4.0850\n",
            "108/108 - 6s - loss: 4.1571 - val_loss: 4.0400\n",
            "108/108 - 6s - loss: 4.1007 - val_loss: 3.9996\n",
            "108/108 - 6s - loss: 4.1372 - val_loss: 3.9621\n",
            "108/108 - 6s - loss: 4.0356 - val_loss: 3.9294\n",
            "108/108 - 6s - loss: 3.9938 - val_loss: 3.8978\n",
            "108/108 - 6s - loss: 3.9289 - val_loss: 3.8669\n",
            "108/108 - 6s - loss: 3.9606 - val_loss: 3.8388\n",
            "108/108 - 6s - loss: 3.9943 - val_loss: 3.8144\n",
            "108/108 - 6s - loss: 3.9090 - val_loss: 3.7917\n",
            "108/108 - 6s - loss: 3.9092 - val_loss: 3.7699\n",
            "108/108 - 6s - loss: 3.9260 - val_loss: 3.7503\n",
            "108/108 - 6s - loss: 3.8859 - val_loss: 3.7330\n",
            "108/108 - 6s - loss: 3.8023 - val_loss: 3.7199\n",
            "108/108 - 6s - loss: 3.8672 - val_loss: 3.7057\n",
            "108/108 - 6s - loss: 3.9370 - val_loss: 3.6937\n",
            "108/108 - 6s - loss: 3.8486 - val_loss: 3.6835\n",
            "108/108 - 6s - loss: 3.8324 - val_loss: 3.6734\n",
            "108/108 - 6s - loss: 3.8177 - val_loss: 3.6653\n",
            "108/108 - 6s - loss: 3.8033 - val_loss: 3.6567\n",
            "108/108 - 6s - loss: 3.8779 - val_loss: 3.6489\n",
            "108/108 - 6s - loss: 3.8306 - val_loss: 3.6429\n",
            "108/108 - 6s - loss: 3.8729 - val_loss: 3.6382\n",
            "108/108 - 6s - loss: 3.8045 - val_loss: 3.6329\n",
            "108/108 - 6s - loss: 3.8118 - val_loss: 3.6290\n",
            "108/108 - 6s - loss: 3.8566 - val_loss: 3.6275\n",
            "108/108 - 6s - loss: 3.8133 - val_loss: 3.6248\n",
            "108/108 - 6s - loss: 3.8070 - val_loss: 3.6211\n",
            "108/108 - 6s - loss: 3.8358 - val_loss: 3.6200\n",
            "108/108 - 6s - loss: 3.8462 - val_loss: 3.6185\n",
            "108/108 - 6s - loss: 3.7689 - val_loss: 3.6164\n",
            "108/108 - 6s - loss: 3.8560 - val_loss: 3.6136\n",
            "108/108 - 6s - loss: 3.8822 - val_loss: 3.6114\n",
            "108/108 - 6s - loss: 3.8353 - val_loss: 3.6088\n",
            "108/108 - 6s - loss: 3.7586 - val_loss: 3.6079\n",
            "108/108 - 6s - loss: 3.7780 - val_loss: 3.6054\n",
            "108/108 - 6s - loss: 3.8197 - val_loss: 3.6054\n",
            "108/108 - 6s - loss: 3.8611 - val_loss: 3.6068\n",
            "108/108 - 6s - loss: 3.8365 - val_loss: 3.6058\n",
            "108/108 - 6s - loss: 3.7567 - val_loss: 3.6052\n",
            "108/108 - 6s - loss: 3.7765 - val_loss: 3.6052\n",
            "108/108 - 6s - loss: 3.7705 - val_loss: 3.6044\n",
            "108/108 - 6s - loss: 3.8161 - val_loss: 3.6028\n",
            "108/108 - 6s - loss: 3.8095 - val_loss: 3.6032\n",
            "108/108 - 6s - loss: 3.8121 - val_loss: 3.6033\n",
            "108/108 - 6s - loss: 3.8059 - val_loss: 3.6038\n",
            "108/108 - 6s - loss: 3.8439 - val_loss: 3.6038\n",
            "108/108 - 6s - loss: 3.8348 - val_loss: 3.6023\n",
            "108/108 - 6s - loss: 3.8023 - val_loss: 3.6013\n",
            "108/108 - 6s - loss: 3.7919 - val_loss: 3.6009\n",
            "108/108 - 6s - loss: 3.7119 - val_loss: 3.6033\n",
            "108/108 - 6s - loss: 3.8028 - val_loss: 3.6036\n",
            "108/108 - 6s - loss: 3.8669 - val_loss: 3.6029\n",
            "108/108 - 6s - loss: 3.8105 - val_loss: 3.6015\n",
            "108/108 - 6s - loss: 3.8535 - val_loss: 3.5998\n",
            "108/108 - 6s - loss: 3.8639 - val_loss: 3.5981\n",
            "108/108 - 6s - loss: 3.8863 - val_loss: 3.6266\n",
            "108/108 - 6s - loss: 4.2886 - val_loss: 3.9739\n",
            "108/108 - 6s - loss: 4.0313 - val_loss: 3.9119\n",
            "108/108 - 6s - loss: 3.9701 - val_loss: 3.8645\n",
            "108/108 - 6s - loss: 4.0767 - val_loss: 3.8255\n",
            "108/108 - 6s - loss: 3.9803 - val_loss: 3.7962\n",
            "108/108 - 6s - loss: 4.0302 - val_loss: 3.7717\n",
            "108/108 - 6s - loss: 3.8551 - val_loss: 3.7473\n",
            "108/108 - 6s - loss: 3.9047 - val_loss: 3.7268\n",
            "108/108 - 6s - loss: 3.8913 - val_loss: 3.7108\n",
            "108/108 - 6s - loss: 3.9238 - val_loss: 3.6956\n",
            "108/108 - 6s - loss: 3.8995 - val_loss: 3.6846\n",
            "108/108 - 6s - loss: 3.8718 - val_loss: 3.6740\n",
            "108/108 - 6s - loss: 3.8761 - val_loss: 3.6655\n",
            "108/108 - 6s - loss: 3.9950 - val_loss: 3.6596\n",
            "108/108 - 6s - loss: 3.8577 - val_loss: 3.6509\n",
            "108/108 - 6s - loss: 3.9144 - val_loss: 3.6465\n",
            "108/108 - 6s - loss: 3.9081 - val_loss: 3.6406\n",
            "108/108 - 6s - loss: 3.8647 - val_loss: 3.6359\n",
            "108/108 - 6s - loss: 3.9105 - val_loss: 3.6328\n",
            "108/108 - 6s - loss: 3.8498 - val_loss: 3.6273\n",
            "108/108 - 6s - loss: 3.8565 - val_loss: 3.6222\n",
            "108/108 - 6s - loss: 3.8941 - val_loss: 3.6185\n",
            "108/108 - 6s - loss: 3.8943 - val_loss: 3.6151\n",
            "108/108 - 6s - loss: 3.9093 - val_loss: 3.6146\n",
            "108/108 - 6s - loss: 3.8981 - val_loss: 3.6116\n",
            "108/108 - 6s - loss: 3.8483 - val_loss: 3.6101\n",
            "108/108 - 6s - loss: 3.8283 - val_loss: 3.6089\n",
            "108/108 - 6s - loss: 3.8356 - val_loss: 3.6062\n",
            "108/108 - 6s - loss: 3.8284 - val_loss: 3.6060\n",
            "108/108 - 6s - loss: 3.9008 - val_loss: 3.6057\n",
            "108/108 - 6s - loss: 3.8465 - val_loss: 3.6047\n",
            "108/108 - 6s - loss: 3.8941 - val_loss: 3.6038\n",
            "108/108 - 6s - loss: 3.8919 - val_loss: 3.6022\n",
            "108/108 - 6s - loss: 3.8468 - val_loss: 3.6009\n",
            "108/108 - 6s - loss: 3.8800 - val_loss: 3.6011\n",
            "108/108 - 6s - loss: 3.8584 - val_loss: 3.6008\n",
            "108/108 - 6s - loss: 3.8592 - val_loss: 3.6030\n",
            "108/108 - 6s - loss: 3.9070 - val_loss: 3.6021\n",
            "108/108 - 6s - loss: 3.8235 - val_loss: 3.6007\n",
            "108/108 - 6s - loss: 3.8051 - val_loss: 3.5998\n",
            "108/108 - 6s - loss: 3.8536 - val_loss: 3.6002\n",
            "108/108 - 6s - loss: 3.8231 - val_loss: 3.5996\n",
            "108/108 - 6s - loss: 3.9230 - val_loss: 3.5992\n",
            "108/108 - 6s - loss: 3.8588 - val_loss: 3.5988\n",
            "108/108 - 6s - loss: 3.8404 - val_loss: 3.5982\n",
            "108/108 - 6s - loss: 3.8893 - val_loss: 3.5994\n",
            "108/108 - 6s - loss: 3.9394 - val_loss: 3.5994\n",
            "108/108 - 6s - loss: 3.8043 - val_loss: 3.5995\n",
            "108/108 - 6s - loss: 3.8891 - val_loss: 3.5995\n",
            "108/108 - 6s - loss: 3.8458 - val_loss: 3.6018\n",
            "108/108 - 6s - loss: 3.8400 - val_loss: 3.6017\n",
            "108/108 - 6s - loss: 3.8586 - val_loss: 3.6020\n",
            "108/108 - 6s - loss: 3.9186 - val_loss: 3.6014\n",
            "108/108 - 6s - loss: 3.8153 - val_loss: 3.5937\n",
            "108/108 - 6s - loss: 3.8782 - val_loss: 3.5858\n",
            "108/108 - 6s - loss: 3.8593 - val_loss: 3.5906\n",
            "108/108 - 6s - loss: 3.8478 - val_loss: 3.5807\n",
            "108/108 - 6s - loss: 3.8427 - val_loss: 3.5744\n",
            "108/108 - 6s - loss: 3.7107 - val_loss: 3.3387\n",
            "108/108 - 6s - loss: 3.5663 - val_loss: 3.3394\n",
            "108/108 - 6s - loss: 3.6121 - val_loss: 3.3780\n",
            "108/108 - 6s - loss: 3.5392 - val_loss: 3.1312\n",
            "108/108 - 6s - loss: 3.5844 - val_loss: 3.1758\n",
            "108/108 - 6s - loss: 3.4757 - val_loss: 3.0209\n",
            "108/108 - 6s - loss: 3.5660 - val_loss: 3.1142\n",
            "108/108 - 6s - loss: 3.4816 - val_loss: 2.9801\n",
            "108/108 - 6s - loss: 3.3555 - val_loss: 3.0593\n",
            "108/108 - 6s - loss: 3.3864 - val_loss: 3.1044\n",
            "108/108 - 6s - loss: 3.5213 - val_loss: 3.0080\n",
            "108/108 - 6s - loss: 3.2891 - val_loss: 2.8429\n",
            "108/108 - 6s - loss: 3.3777 - val_loss: 2.9323\n",
            "108/108 - 6s - loss: 3.1910 - val_loss: 2.7694\n",
            "108/108 - 6s - loss: 3.1275 - val_loss: 2.7957\n",
            "108/108 - 6s - loss: 3.1608 - val_loss: 2.8097\n",
            "108/108 - 6s - loss: 3.1958 - val_loss: 2.5917\n",
            "108/108 - 6s - loss: 3.0420 - val_loss: 2.5586\n",
            "108/108 - 6s - loss: 3.0884 - val_loss: 2.5255\n",
            "108/108 - 6s - loss: 3.0006 - val_loss: 2.5167\n",
            "108/108 - 6s - loss: 2.9611 - val_loss: 2.5350\n",
            "108/108 - 6s - loss: 3.0025 - val_loss: 2.5126\n",
            "108/108 - 6s - loss: 2.9745 - val_loss: 2.4339\n",
            "108/108 - 6s - loss: 3.0374 - val_loss: 2.4650\n",
            "108/108 - 6s - loss: 2.9958 - val_loss: 2.4240\n",
            "108/108 - 6s - loss: 3.0075 - val_loss: 2.4085\n",
            "108/108 - 6s - loss: 2.9102 - val_loss: 2.4440\n",
            "108/108 - 6s - loss: 2.9678 - val_loss: 2.4902\n",
            "108/108 - 6s - loss: 3.0004 - val_loss: 2.4256\n",
            "108/108 - 6s - loss: 2.9112 - val_loss: 2.4243\n",
            "108/108 - 6s - loss: 2.9512 - val_loss: 2.4354\n",
            "108/108 - 6s - loss: 2.9614 - val_loss: 2.3976\n",
            "108/108 - 6s - loss: 2.8816 - val_loss: 2.4249\n",
            "108/108 - 6s - loss: 2.9729 - val_loss: 2.4011\n",
            "108/108 - 6s - loss: 2.8665 - val_loss: 2.3567\n",
            "108/108 - 6s - loss: 2.8994 - val_loss: 2.3933\n",
            "108/108 - 6s - loss: 2.9812 - val_loss: 2.3839\n",
            "108/108 - 6s - loss: 2.8255 - val_loss: 2.3779\n",
            "108/108 - 6s - loss: 2.8769 - val_loss: 2.4606\n",
            "108/108 - 6s - loss: 2.8588 - val_loss: 2.3452\n",
            "108/108 - 6s - loss: 2.8525 - val_loss: 2.3353\n",
            "108/108 - 6s - loss: 2.8864 - val_loss: 2.3394\n",
            "108/108 - 6s - loss: 2.8447 - val_loss: 2.3766\n",
            "108/108 - 6s - loss: 2.8083 - val_loss: 2.3552\n",
            "108/108 - 6s - loss: 2.7539 - val_loss: 2.3123\n",
            "108/108 - 6s - loss: 2.8600 - val_loss: 2.3212\n",
            "108/108 - 6s - loss: 2.8595 - val_loss: 2.3028\n",
            "108/108 - 6s - loss: 2.7974 - val_loss: 2.3135\n",
            "108/108 - 6s - loss: 2.8553 - val_loss: 2.4020\n",
            "108/108 - 6s - loss: 2.7761 - val_loss: 2.3301\n",
            "108/108 - 6s - loss: 2.8030 - val_loss: 2.3217\n",
            "108/108 - 6s - loss: 2.8616 - val_loss: 2.3124\n",
            "108/108 - 6s - loss: 2.7754 - val_loss: 2.3115\n",
            "108/108 - 6s - loss: 2.8034 - val_loss: 2.2964\n",
            "108/108 - 6s - loss: 2.7884 - val_loss: 2.2816\n",
            "108/108 - 6s - loss: 2.8422 - val_loss: 2.2633\n",
            "108/108 - 6s - loss: 2.7588 - val_loss: 2.2626\n",
            "108/108 - 6s - loss: 2.7738 - val_loss: 2.2705\n",
            "108/108 - 6s - loss: 2.7372 - val_loss: 2.3039\n",
            "108/108 - 6s - loss: 2.7468 - val_loss: 2.2694\n",
            "108/108 - 6s - loss: 2.7301 - val_loss: 2.2990\n",
            "108/108 - 6s - loss: 2.7473 - val_loss: 2.2562\n",
            "108/108 - 6s - loss: 2.7234 - val_loss: 2.2480\n",
            "108/108 - 6s - loss: 2.7380 - val_loss: 2.2554\n",
            "108/108 - 6s - loss: 2.7800 - val_loss: 2.2968\n",
            "108/108 - 6s - loss: 2.7361 - val_loss: 2.2344\n",
            "108/108 - 6s - loss: 2.7502 - val_loss: 2.2461\n",
            "108/108 - 6s - loss: 2.7122 - val_loss: 2.2352\n",
            "108/108 - 6s - loss: 2.7612 - val_loss: 2.2331\n",
            "108/108 - 6s - loss: 2.7407 - val_loss: 2.2653\n",
            "108/108 - 6s - loss: 2.7523 - val_loss: 2.2097\n",
            "108/108 - 6s - loss: 2.7891 - val_loss: 2.3112\n",
            "108/108 - 6s - loss: 2.7184 - val_loss: 2.3215\n",
            "108/108 - 6s - loss: 2.6595 - val_loss: 2.3243\n",
            "108/108 - 6s - loss: 2.7466 - val_loss: 2.2256\n",
            "108/108 - 6s - loss: 2.7538 - val_loss: 2.2883\n",
            "108/108 - 6s - loss: 2.6778 - val_loss: 2.2460\n",
            "108/108 - 6s - loss: 2.7667 - val_loss: 2.2483\n",
            "108/108 - 6s - loss: 2.7660 - val_loss: 2.2308\n",
            "108/108 - 6s - loss: 2.7586 - val_loss: 2.2485\n",
            "108/108 - 6s - loss: 2.7490 - val_loss: 2.2109\n",
            "108/108 - 6s - loss: 2.7176 - val_loss: 2.2005\n",
            "108/108 - 6s - loss: 2.7415 - val_loss: 2.2034\n",
            "108/108 - 6s - loss: 2.7041 - val_loss: 2.2011\n",
            "108/108 - 6s - loss: 2.7014 - val_loss: 2.1855\n",
            "108/108 - 6s - loss: 2.6968 - val_loss: 2.1949\n",
            "108/108 - 6s - loss: 2.6601 - val_loss: 2.1855\n",
            "108/108 - 6s - loss: 2.6630 - val_loss: 2.2393\n",
            "108/108 - 6s - loss: 2.6706 - val_loss: 2.2345\n",
            "108/108 - 6s - loss: 2.7418 - val_loss: 2.1675\n",
            "108/108 - 6s - loss: 2.6650 - val_loss: 2.1810\n",
            "108/108 - 6s - loss: 2.6852 - val_loss: 2.1500\n",
            "108/108 - 6s - loss: 2.6845 - val_loss: 2.2056\n",
            "108/108 - 6s - loss: 2.6937 - val_loss: 2.1916\n",
            "108/108 - 6s - loss: 2.7245 - val_loss: 2.2202\n",
            "108/108 - 6s - loss: 2.6968 - val_loss: 2.1902\n",
            "108/108 - 6s - loss: 2.6839 - val_loss: 2.1649\n",
            "108/108 - 6s - loss: 2.6985 - val_loss: 2.2529\n",
            "108/108 - 6s - loss: 2.6345 - val_loss: 2.1677\n",
            "108/108 - 6s - loss: 2.6387 - val_loss: 2.1786\n",
            "108/108 - 6s - loss: 2.6538 - val_loss: 2.2004\n",
            "108/108 - 6s - loss: 2.6986 - val_loss: 2.1815\n",
            "108/108 - 6s - loss: 2.7473 - val_loss: 2.2122\n",
            "108/108 - 6s - loss: 2.7028 - val_loss: 2.1951\n",
            "108/108 - 6s - loss: 2.6742 - val_loss: 2.2382\n",
            "108/108 - 6s - loss: 2.7043 - val_loss: 2.1673\n",
            "108/108 - 6s - loss: 2.6890 - val_loss: 2.2218\n",
            "108/108 - 6s - loss: 2.6906 - val_loss: 2.1691\n",
            "108/108 - 6s - loss: 2.6691 - val_loss: 2.1515\n",
            "108/108 - 6s - loss: 2.5991 - val_loss: 2.1824\n",
            "108/108 - 6s - loss: 2.6332 - val_loss: 2.2151\n",
            "108/108 - 6s - loss: 2.6269 - val_loss: 2.1850\n",
            "108/108 - 6s - loss: 2.6624 - val_loss: 2.1543\n",
            "108/108 - 6s - loss: 2.6820 - val_loss: 2.2286\n",
            "108/108 - 6s - loss: 2.6658 - val_loss: 2.1952\n",
            "108/108 - 6s - loss: 2.5935 - val_loss: 2.1667\n",
            "108/108 - 6s - loss: 2.6898 - val_loss: 2.1317\n",
            "108/108 - 6s - loss: 2.6432 - val_loss: 2.1639\n",
            "108/108 - 6s - loss: 2.6690 - val_loss: 2.1713\n",
            "108/108 - 6s - loss: 2.7175 - val_loss: 2.1735\n",
            "108/108 - 6s - loss: 2.6412 - val_loss: 2.1860\n",
            "108/108 - 6s - loss: 2.6154 - val_loss: 2.1588\n",
            "108/108 - 6s - loss: 2.6888 - val_loss: 2.1881\n",
            "108/108 - 6s - loss: 2.6539 - val_loss: 2.1679\n",
            "108/108 - 6s - loss: 2.6816 - val_loss: 2.1875\n",
            "108/108 - 6s - loss: 2.6928 - val_loss: 2.1483\n",
            "108/108 - 6s - loss: 2.6064 - val_loss: 2.1405\n",
            "108/108 - 6s - loss: 2.6618 - val_loss: 2.1227\n",
            "108/108 - 6s - loss: 2.6822 - val_loss: 2.2417\n",
            "108/108 - 6s - loss: 2.6976 - val_loss: 2.1842\n",
            "108/108 - 6s - loss: 2.6356 - val_loss: 2.1818\n",
            "108/108 - 6s - loss: 2.6189 - val_loss: 2.1532\n",
            "108/108 - 6s - loss: 2.6193 - val_loss: 2.1911\n",
            "108/108 - 6s - loss: 2.5995 - val_loss: 2.2269\n",
            "108/108 - 6s - loss: 2.5721 - val_loss: 2.1941\n",
            "108/108 - 6s - loss: 2.6380 - val_loss: 2.1707\n",
            "108/108 - 6s - loss: 2.6393 - val_loss: 2.1698\n",
            "108/108 - 6s - loss: 2.5971 - val_loss: 2.2155\n",
            "108/108 - 6s - loss: 2.5700 - val_loss: 2.1777\n",
            "108/108 - 6s - loss: 2.6290 - val_loss: 2.2313\n",
            "108/108 - 6s - loss: 2.6510 - val_loss: 2.1353\n",
            "108/108 - 6s - loss: 2.5994 - val_loss: 2.1552\n",
            "108/108 - 6s - loss: 2.6011 - val_loss: 2.1426\n",
            "108/108 - 6s - loss: 2.5964 - val_loss: 2.2858\n",
            "108/108 - 6s - loss: 2.6410 - val_loss: 2.1834\n",
            "108/108 - 6s - loss: 2.6080 - val_loss: 2.1520\n",
            "108/108 - 6s - loss: 2.6018 - val_loss: 2.1769\n",
            "108/108 - 6s - loss: 2.6379 - val_loss: 2.1613\n",
            "108/108 - 6s - loss: 2.6343 - val_loss: 2.2169\n",
            "108/108 - 6s - loss: 2.6245 - val_loss: 2.2318\n",
            "108/108 - 6s - loss: 2.6176 - val_loss: 2.2423\n",
            "108/108 - 6s - loss: 2.6022 - val_loss: 2.1637\n",
            "108/108 - 6s - loss: 2.6000 - val_loss: 2.1997\n",
            "108/108 - 6s - loss: 2.5706 - val_loss: 2.2087\n",
            "108/108 - 6s - loss: 2.6223 - val_loss: 2.2039\n",
            "108/108 - 6s - loss: 2.5455 - val_loss: 2.2108\n",
            "108/108 - 6s - loss: 2.5941 - val_loss: 2.1713\n",
            "108/108 - 6s - loss: 2.5950 - val_loss: 2.1702\n",
            "108/108 - 6s - loss: 2.6112 - val_loss: 2.1493\n",
            "108/108 - 6s - loss: 2.6031 - val_loss: 2.3249\n",
            "108/108 - 6s - loss: 2.5465 - val_loss: 2.1931\n",
            "108/108 - 6s - loss: 2.6171 - val_loss: 2.1740\n",
            "108/108 - 6s - loss: 2.6100 - val_loss: 2.1727\n",
            "108/108 - 6s - loss: 2.6186 - val_loss: 2.1588\n",
            "108/108 - 6s - loss: 2.5789 - val_loss: 2.1470\n",
            "108/108 - 6s - loss: 2.5668 - val_loss: 2.1653\n",
            "108/108 - 6s - loss: 2.5636 - val_loss: 2.1932\n",
            "108/108 - 6s - loss: 2.6690 - val_loss: 2.1344\n",
            "108/108 - 6s - loss: 2.5735 - val_loss: 2.1581\n",
            "108/108 - 6s - loss: 2.5705 - val_loss: 2.1609\n",
            "108/108 - 6s - loss: 2.6129 - val_loss: 2.1429\n",
            "108/108 - 6s - loss: 2.5452 - val_loss: 2.1367\n",
            "108/108 - 6s - loss: 2.5748 - val_loss: 2.1787\n",
            "108/108 - 6s - loss: 2.6320 - val_loss: 2.1331\n",
            "108/108 - 6s - loss: 2.5797 - val_loss: 2.1521\n",
            "108/108 - 6s - loss: 2.5491 - val_loss: 2.1669\n",
            "108/108 - 6s - loss: 2.5807 - val_loss: 2.1032\n",
            "108/108 - 6s - loss: 2.5457 - val_loss: 2.1181\n",
            "108/108 - 6s - loss: 2.5794 - val_loss: 2.1822\n",
            "108/108 - 6s - loss: 2.5514 - val_loss: 2.1292\n",
            "108/108 - 6s - loss: 2.5705 - val_loss: 2.1378\n",
            "108/108 - 6s - loss: 2.5727 - val_loss: 2.1638\n",
            "108/108 - 6s - loss: 2.5849 - val_loss: 2.1113\n",
            "108/108 - 6s - loss: 2.4981 - val_loss: 2.1641\n",
            "108/108 - 6s - loss: 2.5654 - val_loss: 2.1929\n",
            "108/108 - 6s - loss: 2.5903 - val_loss: 2.1631\n",
            "108/108 - 6s - loss: 2.5776 - val_loss: 2.1538\n",
            "108/108 - 6s - loss: 2.5693 - val_loss: 2.0976\n",
            "108/108 - 6s - loss: 2.5598 - val_loss: 2.1672\n",
            "108/108 - 6s - loss: 2.5187 - val_loss: 2.1593\n",
            "108/108 - 6s - loss: 2.5296 - val_loss: 2.1294\n",
            "108/108 - 6s - loss: 2.6210 - val_loss: 2.1486\n",
            "108/108 - 6s - loss: 2.5380 - val_loss: 2.1118\n",
            "108/108 - 6s - loss: 2.5591 - val_loss: 2.1458\n",
            "108/108 - 6s - loss: 2.5876 - val_loss: 2.1426\n",
            "108/108 - 6s - loss: 2.5506 - val_loss: 2.1436\n",
            "108/108 - 6s - loss: 2.5582 - val_loss: 2.0999\n",
            "108/108 - 6s - loss: 2.5435 - val_loss: 2.1112\n",
            "108/108 - 6s - loss: 2.5293 - val_loss: 2.1767\n",
            "108/108 - 6s - loss: 2.5524 - val_loss: 2.1597\n",
            "108/108 - 6s - loss: 2.5779 - val_loss: 2.1412\n",
            "108/108 - 6s - loss: 2.5192 - val_loss: 2.1965\n",
            "108/108 - 6s - loss: 2.5148 - val_loss: 2.1502\n",
            "108/108 - 6s - loss: 2.5319 - val_loss: 2.1924\n",
            "108/108 - 6s - loss: 2.5460 - val_loss: 2.1265\n",
            "108/108 - 6s - loss: 2.5029 - val_loss: 2.1372\n",
            "108/108 - 6s - loss: 2.5383 - val_loss: 2.1971\n",
            "108/108 - 6s - loss: 2.5251 - val_loss: 2.1765\n",
            "108/108 - 6s - loss: 2.5641 - val_loss: 2.1264\n",
            "108/108 - 6s - loss: 2.5537 - val_loss: 2.1479\n",
            "108/108 - 6s - loss: 2.5507 - val_loss: 2.1674\n",
            "108/108 - 6s - loss: 2.5391 - val_loss: 2.1481\n",
            "108/108 - 6s - loss: 2.5020 - val_loss: 2.1403\n",
            "108/108 - 6s - loss: 2.5362 - val_loss: 2.1164\n",
            "108/108 - 6s - loss: 2.5633 - val_loss: 2.1765\n",
            "108/108 - 6s - loss: 2.5043 - val_loss: 2.1370\n",
            "108/108 - 6s - loss: 2.6055 - val_loss: 2.1676\n",
            "108/108 - 6s - loss: 2.5479 - val_loss: 2.0970\n",
            "108/108 - 6s - loss: 2.5180 - val_loss: 2.1629\n",
            "108/108 - 6s - loss: 2.5135 - val_loss: 2.1114\n",
            "108/108 - 6s - loss: 2.4952 - val_loss: 2.1461\n",
            "108/108 - 6s - loss: 2.5320 - val_loss: 2.1707\n",
            "108/108 - 6s - loss: 2.5446 - val_loss: 2.1391\n",
            "108/108 - 6s - loss: 2.4967 - val_loss: 2.1119\n",
            "108/108 - 6s - loss: 2.5248 - val_loss: 2.1711\n",
            "108/108 - 6s - loss: 2.5801 - val_loss: 2.0928\n",
            "108/108 - 6s - loss: 2.4758 - val_loss: 2.0930\n",
            "108/108 - 6s - loss: 2.5057 - val_loss: 2.1070\n",
            "108/108 - 6s - loss: 2.5008 - val_loss: 2.1103\n",
            "108/108 - 6s - loss: 2.5094 - val_loss: 2.1274\n",
            "108/108 - 6s - loss: 2.4827 - val_loss: 2.0999\n",
            "108/108 - 6s - loss: 2.4432 - val_loss: 2.1308\n",
            "108/108 - 6s - loss: 2.5214 - val_loss: 2.0971\n",
            "108/108 - 6s - loss: 2.4925 - val_loss: 2.0904\n",
            "108/108 - 6s - loss: 2.5263 - val_loss: 2.1163\n",
            "108/108 - 6s - loss: 2.4922 - val_loss: 2.0751\n",
            "108/108 - 6s - loss: 2.5020 - val_loss: 2.1781\n",
            "108/108 - 6s - loss: 2.5607 - val_loss: 2.1333\n",
            "108/108 - 6s - loss: 2.4939 - val_loss: 2.1077\n",
            "108/108 - 6s - loss: 2.5167 - val_loss: 2.1278\n",
            "108/108 - 6s - loss: 2.4793 - val_loss: 2.1090\n",
            "108/108 - 6s - loss: 2.5078 - val_loss: 2.1168\n",
            "108/108 - 6s - loss: 2.4617 - val_loss: 2.1417\n",
            "108/108 - 6s - loss: 2.4979 - val_loss: 2.1381\n",
            "108/108 - 6s - loss: 2.4878 - val_loss: 2.1958\n",
            "108/108 - 6s - loss: 2.4873 - val_loss: 2.1089\n",
            "108/108 - 6s - loss: 2.4751 - val_loss: 2.1576\n",
            "108/108 - 6s - loss: 2.4656 - val_loss: 2.1644\n",
            "108/108 - 6s - loss: 2.4934 - val_loss: 2.1413\n",
            "108/108 - 6s - loss: 2.4755 - val_loss: 2.1876\n",
            "108/108 - 6s - loss: 2.5432 - val_loss: 2.1690\n",
            "108/108 - 6s - loss: 2.4972 - val_loss: 2.1200\n",
            "108/108 - 6s - loss: 2.5059 - val_loss: 2.1759\n",
            "108/108 - 6s - loss: 2.4552 - val_loss: 2.1404\n",
            "108/108 - 6s - loss: 2.5246 - val_loss: 2.0815\n",
            "108/108 - 6s - loss: 2.4998 - val_loss: 2.1536\n",
            "108/108 - 6s - loss: 2.4264 - val_loss: 2.1577\n",
            "108/108 - 6s - loss: 2.5134 - val_loss: 2.1772\n",
            "108/108 - 6s - loss: 2.4591 - val_loss: 2.1408\n",
            "108/108 - 6s - loss: 2.4841 - val_loss: 2.1562\n",
            "108/108 - 6s - loss: 2.5101 - val_loss: 2.1362\n",
            "108/108 - 6s - loss: 2.5333 - val_loss: 2.1121\n",
            "108/108 - 6s - loss: 2.5083 - val_loss: 2.0933\n",
            "108/108 - 6s - loss: 2.4706 - val_loss: 2.0720\n",
            "108/108 - 6s - loss: 2.4578 - val_loss: 2.1006\n",
            "108/108 - 6s - loss: 2.4715 - val_loss: 2.1692\n",
            "108/108 - 6s - loss: 2.4936 - val_loss: 2.1534\n",
            "108/108 - 6s - loss: 2.4733 - val_loss: 2.1395\n",
            "108/108 - 6s - loss: 2.4243 - val_loss: 2.1505\n",
            "108/108 - 6s - loss: 2.4533 - val_loss: 2.2062\n",
            "108/108 - 6s - loss: 2.4645 - val_loss: 2.2196\n",
            "108/108 - 6s - loss: 2.4458 - val_loss: 2.1635\n",
            "108/108 - 6s - loss: 2.4415 - val_loss: 2.1796\n",
            "108/108 - 6s - loss: 2.4898 - val_loss: 2.2283\n",
            "108/108 - 6s - loss: 2.4880 - val_loss: 2.1588\n",
            "108/108 - 6s - loss: 2.4765 - val_loss: 2.1164\n",
            "108/108 - 6s - loss: 2.4659 - val_loss: 2.0832\n",
            "108/108 - 6s - loss: 2.4193 - val_loss: 2.1097\n",
            "108/108 - 6s - loss: 2.4175 - val_loss: 2.1424\n",
            "108/108 - 6s - loss: 2.4604 - val_loss: 2.2280\n",
            "108/108 - 6s - loss: 2.4627 - val_loss: 2.1671\n",
            "108/108 - 6s - loss: 2.4178 - val_loss: 2.1276\n",
            "108/108 - 6s - loss: 2.4349 - val_loss: 2.1269\n",
            "108/108 - 6s - loss: 2.4160 - val_loss: 2.2072\n",
            "108/108 - 6s - loss: 2.4923 - val_loss: 2.1857\n",
            "108/108 - 6s - loss: 2.4355 - val_loss: 2.1369\n",
            "108/108 - 6s - loss: 2.4345 - val_loss: 2.0882\n",
            "108/108 - 6s - loss: 2.4353 - val_loss: 2.1273\n",
            "108/108 - 6s - loss: 2.4523 - val_loss: 2.1617\n",
            "108/108 - 6s - loss: 2.3790 - val_loss: 2.1096\n",
            "108/108 - 6s - loss: 2.4987 - val_loss: 2.0821\n",
            "108/108 - 6s - loss: 2.4391 - val_loss: 2.1203\n",
            "108/108 - 6s - loss: 2.4942 - val_loss: 2.1336\n",
            "108/108 - 6s - loss: 2.3891 - val_loss: 2.1260\n",
            "108/108 - 6s - loss: 2.4239 - val_loss: 2.1377\n",
            "108/108 - 6s - loss: 2.4164 - val_loss: 2.1639\n",
            "108/108 - 6s - loss: 2.4494 - val_loss: 2.1445\n",
            "108/108 - 6s - loss: 2.4287 - val_loss: 2.0956\n",
            "108/108 - 6s - loss: 2.4417 - val_loss: 2.1328\n",
            "108/108 - 6s - loss: 2.3998 - val_loss: 2.1372\n",
            "108/108 - 6s - loss: 2.3638 - val_loss: 2.1251\n",
            "108/108 - 6s - loss: 2.4699 - val_loss: 2.1735\n",
            "108/108 - 6s - loss: 2.4461 - val_loss: 2.1914\n",
            "108/108 - 6s - loss: 2.4511 - val_loss: 2.1311\n",
            "108/108 - 6s - loss: 2.4516 - val_loss: 2.1443\n",
            "108/108 - 6s - loss: 2.4323 - val_loss: 2.0905\n",
            "108/108 - 6s - loss: 2.4199 - val_loss: 2.1428\n",
            "108/108 - 6s - loss: 2.4264 - val_loss: 2.1807\n",
            "108/108 - 6s - loss: 2.4371 - val_loss: 2.1493\n",
            "108/108 - 6s - loss: 2.3810 - val_loss: 2.0693\n",
            "108/108 - 6s - loss: 2.4762 - val_loss: 2.1752\n",
            "108/108 - 6s - loss: 2.4363 - val_loss: 2.1653\n",
            "108/108 - 6s - loss: 2.3801 - val_loss: 2.0726\n",
            "108/108 - 6s - loss: 2.4523 - val_loss: 2.1418\n",
            "108/108 - 6s - loss: 2.4377 - val_loss: 2.1061\n",
            "108/108 - 6s - loss: 2.4207 - val_loss: 2.1562\n",
            "108/108 - 6s - loss: 2.4204 - val_loss: 2.1279\n",
            "108/108 - 6s - loss: 2.4298 - val_loss: 2.0734\n",
            "108/108 - 6s - loss: 2.3876 - val_loss: 2.1480\n",
            "108/108 - 6s - loss: 2.4130 - val_loss: 2.1844\n",
            "108/108 - 6s - loss: 2.4032 - val_loss: 2.1932\n",
            "108/108 - 6s - loss: 2.3669 - val_loss: 2.1374\n",
            "108/108 - 6s - loss: 2.4311 - val_loss: 2.1494\n",
            "108/108 - 6s - loss: 2.4131 - val_loss: 2.1971\n",
            "108/108 - 6s - loss: 2.4456 - val_loss: 2.1163\n",
            "108/108 - 6s - loss: 2.4306 - val_loss: 2.1035\n",
            "108/108 - 6s - loss: 2.4528 - val_loss: 2.1094\n",
            "108/108 - 6s - loss: 2.4213 - val_loss: 2.1201\n",
            "108/108 - 6s - loss: 2.4063 - val_loss: 2.0578\n",
            "108/108 - 6s - loss: 2.3887 - val_loss: 2.0999\n",
            "108/108 - 6s - loss: 2.4421 - val_loss: 2.1360\n",
            "108/108 - 6s - loss: 2.4436 - val_loss: 2.1518\n",
            "108/108 - 6s - loss: 2.3548 - val_loss: 2.2162\n",
            "108/108 - 6s - loss: 2.3879 - val_loss: 2.0975\n",
            "108/108 - 6s - loss: 2.4139 - val_loss: 2.1389\n",
            "108/108 - 6s - loss: 2.3838 - val_loss: 2.1302\n",
            "108/108 - 6s - loss: 2.4133 - val_loss: 2.1074\n",
            "108/108 - 6s - loss: 2.3893 - val_loss: 2.0748\n",
            "108/108 - 6s - loss: 2.3668 - val_loss: 2.1804\n",
            "108/108 - 6s - loss: 2.3850 - val_loss: 2.1781\n",
            "108/108 - 6s - loss: 2.3507 - val_loss: 2.1781\n",
            "108/108 - 6s - loss: 2.3539 - val_loss: 2.1675\n",
            "108/108 - 6s - loss: 2.3596 - val_loss: 2.1518\n",
            "108/108 - 6s - loss: 2.3575 - val_loss: 2.1437\n",
            "108/108 - 6s - loss: 2.4105 - val_loss: 2.2134\n",
            "108/108 - 6s - loss: 2.4594 - val_loss: 2.1434\n",
            "108/108 - 6s - loss: 2.4095 - val_loss: 2.2362\n",
            "108/108 - 6s - loss: 2.3819 - val_loss: 2.0724\n",
            "108/108 - 6s - loss: 2.3653 - val_loss: 2.0947\n",
            "0.8\n",
            "108/108 - 13s - loss: 13.8953 - val_loss: 13.9359\n",
            "108/108 - 6s - loss: 13.1572 - val_loss: 13.3034\n",
            "108/108 - 6s - loss: 12.5530 - val_loss: 12.6947\n",
            "108/108 - 6s - loss: 11.9315 - val_loss: 12.1030\n",
            "108/108 - 6s - loss: 11.3724 - val_loss: 11.5672\n",
            "108/108 - 6s - loss: 10.8564 - val_loss: 11.0696\n",
            "108/108 - 6s - loss: 10.4212 - val_loss: 10.6014\n",
            "108/108 - 6s - loss: 9.9757 - val_loss: 10.1572\n",
            "108/108 - 6s - loss: 9.5523 - val_loss: 9.7342\n",
            "108/108 - 6s - loss: 9.1283 - val_loss: 9.3320\n",
            "108/108 - 6s - loss: 8.7846 - val_loss: 8.9473\n",
            "108/108 - 6s - loss: 8.3914 - val_loss: 8.5791\n",
            "108/108 - 6s - loss: 8.0775 - val_loss: 8.2304\n",
            "108/108 - 6s - loss: 7.7130 - val_loss: 7.8950\n",
            "108/108 - 6s - loss: 7.4388 - val_loss: 7.5773\n",
            "108/108 - 6s - loss: 7.0882 - val_loss: 7.2735\n",
            "108/108 - 6s - loss: 6.8609 - val_loss: 6.9833\n",
            "108/108 - 6s - loss: 6.6183 - val_loss: 6.7067\n",
            "108/108 - 6s - loss: 6.3258 - val_loss: 6.4436\n",
            "108/108 - 6s - loss: 6.0885 - val_loss: 6.1928\n",
            "108/108 - 6s - loss: 5.8725 - val_loss: 5.9547\n",
            "108/108 - 6s - loss: 5.6964 - val_loss: 5.7283\n",
            "108/108 - 6s - loss: 5.4868 - val_loss: 5.5149\n",
            "108/108 - 6s - loss: 5.2838 - val_loss: 5.3139\n",
            "108/108 - 6s - loss: 5.0690 - val_loss: 5.1230\n",
            "108/108 - 6s - loss: 4.9407 - val_loss: 4.9443\n",
            "108/108 - 6s - loss: 4.7358 - val_loss: 4.7776\n",
            "108/108 - 6s - loss: 4.6348 - val_loss: 4.6234\n",
            "108/108 - 6s - loss: 4.4904 - val_loss: 4.4798\n",
            "108/108 - 6s - loss: 4.3615 - val_loss: 4.3473\n",
            "108/108 - 6s - loss: 4.2311 - val_loss: 4.2233\n",
            "108/108 - 6s - loss: 4.2157 - val_loss: 4.1104\n",
            "108/108 - 6s - loss: 4.0469 - val_loss: 4.0031\n",
            "108/108 - 6s - loss: 3.9614 - val_loss: 3.9035\n",
            "108/108 - 6s - loss: 3.9043 - val_loss: 3.8119\n",
            "108/108 - 6s - loss: 3.8166 - val_loss: 3.7269\n",
            "108/108 - 6s - loss: 3.6357 - val_loss: 3.6468\n",
            "108/108 - 6s - loss: 3.5979 - val_loss: 3.5728\n",
            "108/108 - 6s - loss: 3.5821 - val_loss: 3.5035\n",
            "108/108 - 6s - loss: 3.5139 - val_loss: 3.4393\n",
            "108/108 - 6s - loss: 3.4808 - val_loss: 3.3783\n",
            "108/108 - 6s - loss: 3.4411 - val_loss: 3.3240\n",
            "108/108 - 6s - loss: 3.3697 - val_loss: 3.2733\n",
            "108/108 - 6s - loss: 3.3543 - val_loss: 3.2247\n",
            "108/108 - 6s - loss: 3.3459 - val_loss: 3.1792\n",
            "108/108 - 6s - loss: 3.2799 - val_loss: 3.1358\n",
            "108/108 - 6s - loss: 3.2664 - val_loss: 3.0986\n",
            "108/108 - 6s - loss: 3.2305 - val_loss: 3.0654\n",
            "108/108 - 6s - loss: 3.2278 - val_loss: 3.0353\n",
            "108/108 - 6s - loss: 3.2413 - val_loss: 3.0075\n",
            "108/108 - 6s - loss: 3.1581 - val_loss: 2.9808\n",
            "108/108 - 6s - loss: 3.2297 - val_loss: 2.9559\n",
            "108/108 - 6s - loss: 3.1271 - val_loss: 2.9340\n",
            "108/108 - 6s - loss: 3.0866 - val_loss: 2.9148\n",
            "108/108 - 6s - loss: 3.1738 - val_loss: 2.8988\n",
            "108/108 - 6s - loss: 3.0990 - val_loss: 2.8837\n",
            "108/108 - 6s - loss: 3.1829 - val_loss: 2.8698\n",
            "108/108 - 6s - loss: 3.1536 - val_loss: 2.8569\n",
            "108/108 - 6s - loss: 3.0802 - val_loss: 2.8464\n",
            "108/108 - 6s - loss: 3.1663 - val_loss: 2.8367\n",
            "108/108 - 6s - loss: 3.0320 - val_loss: 2.8288\n",
            "108/108 - 6s - loss: 2.9938 - val_loss: 2.8204\n",
            "108/108 - 6s - loss: 3.0429 - val_loss: 2.8119\n",
            "108/108 - 6s - loss: 3.0878 - val_loss: 2.8032\n",
            "108/108 - 6s - loss: 3.1168 - val_loss: 2.7973\n",
            "108/108 - 6s - loss: 3.0926 - val_loss: 2.7937\n",
            "108/108 - 6s - loss: 3.1788 - val_loss: 2.7903\n",
            "108/108 - 6s - loss: 3.0362 - val_loss: 2.7865\n",
            "108/108 - 6s - loss: 3.1227 - val_loss: 2.7831\n",
            "108/108 - 6s - loss: 3.0486 - val_loss: 2.7794\n",
            "108/108 - 6s - loss: 3.0870 - val_loss: 2.7760\n",
            "108/108 - 6s - loss: 3.0889 - val_loss: 2.7740\n",
            "108/108 - 6s - loss: 3.0432 - val_loss: 2.7743\n",
            "108/108 - 6s - loss: 3.0225 - val_loss: 2.7732\n",
            "108/108 - 6s - loss: 3.1469 - val_loss: 2.7708\n",
            "108/108 - 6s - loss: 3.0714 - val_loss: 2.7698\n",
            "108/108 - 6s - loss: 3.0733 - val_loss: 2.7695\n",
            "108/108 - 6s - loss: 3.0426 - val_loss: 2.7689\n",
            "108/108 - 6s - loss: 3.1324 - val_loss: 2.7674\n",
            "108/108 - 6s - loss: 3.0985 - val_loss: 2.7668\n",
            "108/108 - 6s - loss: 3.0996 - val_loss: 2.7644\n",
            "108/108 - 6s - loss: 3.0587 - val_loss: 2.7619\n",
            "108/108 - 6s - loss: 3.0532 - val_loss: 2.7607\n",
            "108/108 - 6s - loss: 3.0534 - val_loss: 2.7603\n",
            "108/108 - 6s - loss: 3.0741 - val_loss: 2.7593\n",
            "108/108 - 6s - loss: 3.0743 - val_loss: 2.7592\n",
            "108/108 - 6s - loss: 3.0317 - val_loss: 2.7597\n",
            "108/108 - 6s - loss: 3.0567 - val_loss: 2.7589\n",
            "108/108 - 6s - loss: 3.1157 - val_loss: 2.7573\n",
            "108/108 - 6s - loss: 3.1021 - val_loss: 2.7571\n",
            "108/108 - 6s - loss: 3.0600 - val_loss: 2.7580\n",
            "108/108 - 6s - loss: 3.0616 - val_loss: 2.7585\n",
            "108/108 - 6s - loss: 3.0968 - val_loss: 2.7591\n",
            "108/108 - 6s - loss: 3.0986 - val_loss: 2.7600\n",
            "108/108 - 6s - loss: 3.0995 - val_loss: 2.7592\n",
            "108/108 - 6s - loss: 3.0677 - val_loss: 2.7591\n",
            "108/108 - 6s - loss: 3.1140 - val_loss: 2.7589\n",
            "108/108 - 6s - loss: 3.0522 - val_loss: 2.7599\n",
            "108/108 - 6s - loss: 3.1232 - val_loss: 2.7604\n",
            "108/108 - 6s - loss: 3.1573 - val_loss: 2.7588\n",
            "108/108 - 6s - loss: 3.0283 - val_loss: 2.7584\n",
            "108/108 - 6s - loss: 3.1082 - val_loss: 2.7597\n",
            "108/108 - 6s - loss: 3.0707 - val_loss: 2.7580\n",
            "108/108 - 6s - loss: 3.1225 - val_loss: 2.7577\n",
            "108/108 - 6s - loss: 3.0699 - val_loss: 2.7578\n",
            "108/108 - 6s - loss: 3.0496 - val_loss: 2.7585\n",
            "108/108 - 6s - loss: 3.0206 - val_loss: 2.7585\n",
            "108/108 - 6s - loss: 3.0615 - val_loss: 2.7575\n",
            "108/108 - 6s - loss: 3.0085 - val_loss: 2.7586\n",
            "108/108 - 6s - loss: 3.1039 - val_loss: 2.7586\n",
            "108/108 - 6s - loss: 3.1082 - val_loss: 2.7586\n",
            "108/108 - 6s - loss: 3.0197 - val_loss: 2.7605\n",
            "108/108 - 6s - loss: 3.0897 - val_loss: 2.7615\n",
            "108/108 - 6s - loss: 3.0481 - val_loss: 2.7607\n",
            "108/108 - 6s - loss: 3.0616 - val_loss: 2.7592\n",
            "108/108 - 6s - loss: 3.0884 - val_loss: 2.7581\n",
            "108/108 - 6s - loss: 3.0927 - val_loss: 2.7589\n",
            "108/108 - 6s - loss: 3.0728 - val_loss: 2.7585\n",
            "108/108 - 6s - loss: 3.1299 - val_loss: 2.7590\n",
            "108/108 - 6s - loss: 3.0460 - val_loss: 2.7595\n",
            "108/108 - 6s - loss: 3.1024 - val_loss: 2.7594\n",
            "108/108 - 6s - loss: 3.0731 - val_loss: 2.7578\n",
            "108/108 - 6s - loss: 3.0359 - val_loss: 2.7575\n",
            "108/108 - 6s - loss: 3.0976 - val_loss: 2.7562\n",
            "108/108 - 6s - loss: 3.0069 - val_loss: 2.7582\n",
            "108/108 - 6s - loss: 3.0138 - val_loss: 2.7567\n",
            "108/108 - 6s - loss: 3.1324 - val_loss: 2.7569\n",
            "108/108 - 6s - loss: 3.0540 - val_loss: 2.7582\n",
            "108/108 - 6s - loss: 3.0661 - val_loss: 2.7591\n",
            "108/108 - 6s - loss: 3.0479 - val_loss: 2.7590\n",
            "108/108 - 6s - loss: 3.0859 - val_loss: 2.7592\n",
            "108/108 - 6s - loss: 3.0719 - val_loss: 2.7582\n",
            "108/108 - 6s - loss: 3.0506 - val_loss: 2.7575\n",
            "108/108 - 6s - loss: 3.0701 - val_loss: 2.7583\n",
            "108/108 - 6s - loss: 3.1095 - val_loss: 2.7580\n",
            "108/108 - 6s - loss: 3.1064 - val_loss: 2.7575\n",
            "108/108 - 6s - loss: 3.0695 - val_loss: 2.7571\n",
            "108/108 - 6s - loss: 3.1529 - val_loss: 2.7561\n",
            "108/108 - 6s - loss: 3.1264 - val_loss: 2.7552\n",
            "108/108 - 6s - loss: 3.0574 - val_loss: 2.7556\n",
            "108/108 - 6s - loss: 3.1012 - val_loss: 2.7559\n",
            "108/108 - 6s - loss: 3.0560 - val_loss: 2.7576\n",
            "108/108 - 6s - loss: 3.0433 - val_loss: 2.7571\n",
            "108/108 - 6s - loss: 3.0641 - val_loss: 2.7572\n",
            "108/108 - 6s - loss: 3.0952 - val_loss: 2.7559\n",
            "108/108 - 6s - loss: 3.0530 - val_loss: 2.7554\n",
            "108/108 - 6s - loss: 3.0610 - val_loss: 2.7577\n",
            "108/108 - 6s - loss: 3.1033 - val_loss: 2.7562\n",
            "108/108 - 6s - loss: 3.1009 - val_loss: 2.7560\n",
            "108/108 - 6s - loss: 3.0381 - val_loss: 2.7568\n",
            "108/108 - 6s - loss: 3.0813 - val_loss: 2.7564\n",
            "108/108 - 6s - loss: 3.1124 - val_loss: 2.7575\n",
            "108/108 - 6s - loss: 3.0200 - val_loss: 2.7580\n",
            "108/108 - 6s - loss: 2.9984 - val_loss: 2.7594\n",
            "108/108 - 6s - loss: 3.4555 - val_loss: 3.6236\n",
            "108/108 - 6s - loss: 3.4188 - val_loss: 3.1802\n",
            "108/108 - 6s - loss: 3.3654 - val_loss: 3.1021\n",
            "108/108 - 6s - loss: 3.2077 - val_loss: 3.0434\n",
            "108/108 - 6s - loss: 3.2248 - val_loss: 2.9937\n",
            "108/108 - 6s - loss: 3.1911 - val_loss: 2.9585\n",
            "108/108 - 6s - loss: 3.2709 - val_loss: 2.9283\n",
            "108/108 - 6s - loss: 3.1689 - val_loss: 2.9033\n",
            "108/108 - 6s - loss: 3.2047 - val_loss: 2.8830\n",
            "108/108 - 6s - loss: 3.1515 - val_loss: 2.8650\n",
            "108/108 - 6s - loss: 3.2097 - val_loss: 2.8464\n",
            "108/108 - 6s - loss: 3.1847 - val_loss: 2.8331\n",
            "108/108 - 6s - loss: 3.1529 - val_loss: 2.8252\n",
            "108/108 - 6s - loss: 3.0928 - val_loss: 2.8148\n",
            "108/108 - 6s - loss: 3.1129 - val_loss: 2.7551\n",
            "108/108 - 6s - loss: 3.1304 - val_loss: 2.7516\n",
            "108/108 - 6s - loss: 3.1227 - val_loss: 2.7520\n",
            "108/108 - 6s - loss: 3.1214 - val_loss: 2.7540\n",
            "108/108 - 6s - loss: 3.1890 - val_loss: 2.7547\n",
            "108/108 - 6s - loss: 3.1304 - val_loss: 2.7558\n",
            "108/108 - 6s - loss: 3.1829 - val_loss: 2.7565\n",
            "108/108 - 6s - loss: 3.1459 - val_loss: 2.7571\n",
            "108/108 - 6s - loss: 3.1104 - val_loss: 2.7579\n",
            "108/108 - 6s - loss: 3.0658 - val_loss: 2.7580\n",
            "108/108 - 6s - loss: 3.0498 - val_loss: 2.7585\n",
            "108/108 - 6s - loss: 3.0962 - val_loss: 2.7587\n",
            "108/108 - 6s - loss: 3.0731 - val_loss: 2.7577\n",
            "108/108 - 6s - loss: 3.1037 - val_loss: 2.7575\n",
            "108/108 - 6s - loss: 3.1036 - val_loss: 2.7580\n",
            "108/108 - 6s - loss: 3.0642 - val_loss: 2.7582\n",
            "108/108 - 6s - loss: 3.0893 - val_loss: 2.7586\n",
            "108/108 - 6s - loss: 3.1271 - val_loss: 2.7587\n",
            "108/108 - 6s - loss: 3.1501 - val_loss: 2.7601\n",
            "108/108 - 6s - loss: 3.0744 - val_loss: 2.7606\n",
            "108/108 - 6s - loss: 3.0818 - val_loss: 2.7597\n",
            "108/108 - 6s - loss: 3.0531 - val_loss: 2.7610\n",
            "108/108 - 6s - loss: 3.0634 - val_loss: 2.7612\n",
            "108/108 - 6s - loss: 3.1399 - val_loss: 2.7612\n",
            "108/108 - 6s - loss: 3.0655 - val_loss: 2.7628\n",
            "108/108 - 6s - loss: 3.1551 - val_loss: 2.7621\n",
            "108/108 - 6s - loss: 3.1012 - val_loss: 2.7610\n",
            "108/108 - 6s - loss: 3.0769 - val_loss: 2.7610\n",
            "108/108 - 6s - loss: 3.1199 - val_loss: 2.7603\n",
            "108/108 - 6s - loss: 3.0870 - val_loss: 2.7606\n",
            "108/108 - 6s - loss: 3.1062 - val_loss: 2.7602\n",
            "108/108 - 6s - loss: 3.1155 - val_loss: 2.7600\n",
            "108/108 - 6s - loss: 3.1673 - val_loss: 2.7605\n",
            "108/108 - 6s - loss: 3.0535 - val_loss: 2.7619\n",
            "108/108 - 6s - loss: 3.1840 - val_loss: 2.7614\n",
            "108/108 - 6s - loss: 3.0970 - val_loss: 2.7617\n",
            "108/108 - 6s - loss: 3.1057 - val_loss: 2.7604\n",
            "108/108 - 6s - loss: 3.0316 - val_loss: 2.7605\n",
            "108/108 - 6s - loss: 3.0403 - val_loss: 2.7615\n",
            "108/108 - 6s - loss: 3.0883 - val_loss: 2.7603\n",
            "108/108 - 6s - loss: 3.1257 - val_loss: 2.7592\n",
            "108/108 - 6s - loss: 3.0505 - val_loss: 2.7598\n",
            "108/108 - 6s - loss: 3.1393 - val_loss: 2.7591\n",
            "108/108 - 6s - loss: 3.1431 - val_loss: 2.7577\n",
            "108/108 - 6s - loss: 3.0480 - val_loss: 2.7548\n",
            "108/108 - 6s - loss: 2.9296 - val_loss: 2.7549\n",
            "108/108 - 6s - loss: 3.0600 - val_loss: 2.7572\n",
            "108/108 - 6s - loss: 3.0716 - val_loss: 2.7597\n",
            "108/108 - 6s - loss: 3.0699 - val_loss: 2.7628\n",
            "108/108 - 6s - loss: 3.1421 - val_loss: 2.7577\n",
            "108/108 - 6s - loss: 3.0394 - val_loss: 2.7559\n",
            "108/108 - 6s - loss: 3.1386 - val_loss: 2.9617\n",
            "108/108 - 6s - loss: 3.0917 - val_loss: 2.8306\n",
            "108/108 - 6s - loss: 3.0892 - val_loss: 2.5221\n",
            "108/108 - 6s - loss: 2.7691 - val_loss: 2.4032\n",
            "108/108 - 6s - loss: 2.9502 - val_loss: 2.3067\n",
            "108/108 - 6s - loss: 2.6891 - val_loss: 2.3210\n",
            "108/108 - 6s - loss: 2.7037 - val_loss: 2.1044\n",
            "108/108 - 6s - loss: 2.4748 - val_loss: 2.0069\n",
            "108/108 - 6s - loss: 2.4606 - val_loss: 1.9958\n",
            "108/108 - 6s - loss: 2.4644 - val_loss: 1.9667\n",
            "108/108 - 6s - loss: 2.4253 - val_loss: 1.9824\n",
            "108/108 - 6s - loss: 2.4419 - val_loss: 1.9583\n",
            "108/108 - 6s - loss: 2.4191 - val_loss: 1.9916\n",
            "108/108 - 6s - loss: 2.3995 - val_loss: 1.9795\n",
            "108/108 - 6s - loss: 2.4007 - val_loss: 1.9709\n",
            "108/108 - 6s - loss: 2.4280 - val_loss: 2.0373\n",
            "108/108 - 6s - loss: 2.4046 - val_loss: 1.9239\n",
            "108/108 - 6s - loss: 2.3035 - val_loss: 1.9536\n",
            "108/108 - 6s - loss: 2.2597 - val_loss: 1.9513\n",
            "108/108 - 6s - loss: 2.3094 - val_loss: 1.8375\n",
            "108/108 - 6s - loss: 2.2542 - val_loss: 1.8677\n",
            "108/108 - 6s - loss: 2.3450 - val_loss: 1.8834\n",
            "108/108 - 6s - loss: 2.3354 - val_loss: 1.8450\n",
            "108/108 - 6s - loss: 2.2851 - val_loss: 1.9264\n",
            "108/108 - 6s - loss: 2.2975 - val_loss: 1.8102\n",
            "108/108 - 6s - loss: 2.2689 - val_loss: 1.8235\n",
            "108/108 - 6s - loss: 2.2231 - val_loss: 1.8108\n",
            "108/108 - 6s - loss: 2.2068 - val_loss: 1.8412\n",
            "108/108 - 6s - loss: 2.2516 - val_loss: 1.8127\n",
            "108/108 - 6s - loss: 2.1927 - val_loss: 1.8783\n",
            "108/108 - 6s - loss: 2.2048 - val_loss: 1.9305\n",
            "108/108 - 6s - loss: 2.2380 - val_loss: 1.9099\n",
            "108/108 - 6s - loss: 2.2547 - val_loss: 1.8569\n",
            "108/108 - 6s - loss: 2.2592 - val_loss: 1.8562\n",
            "108/108 - 6s - loss: 2.2210 - val_loss: 1.7994\n",
            "108/108 - 6s - loss: 2.2056 - val_loss: 1.8412\n",
            "108/108 - 6s - loss: 2.1980 - val_loss: 1.7554\n",
            "108/108 - 6s - loss: 2.2112 - val_loss: 1.7850\n",
            "108/108 - 6s - loss: 2.2588 - val_loss: 1.8371\n",
            "108/108 - 6s - loss: 2.1493 - val_loss: 1.8347\n",
            "108/108 - 6s - loss: 2.1488 - val_loss: 1.9169\n",
            "108/108 - 6s - loss: 2.2032 - val_loss: 1.7687\n",
            "108/108 - 6s - loss: 2.1412 - val_loss: 1.7782\n",
            "108/108 - 6s - loss: 2.2242 - val_loss: 1.7996\n",
            "108/108 - 6s - loss: 2.2034 - val_loss: 1.7394\n",
            "108/108 - 6s - loss: 2.1674 - val_loss: 1.8988\n",
            "108/108 - 6s - loss: 2.2743 - val_loss: 1.7667\n",
            "108/108 - 6s - loss: 2.2098 - val_loss: 1.7923\n",
            "108/108 - 6s - loss: 2.1845 - val_loss: 1.7505\n",
            "108/108 - 6s - loss: 2.1901 - val_loss: 1.7474\n",
            "108/108 - 6s - loss: 2.1709 - val_loss: 1.7590\n",
            "108/108 - 6s - loss: 2.1517 - val_loss: 1.7528\n",
            "108/108 - 6s - loss: 2.1468 - val_loss: 1.7528\n",
            "108/108 - 6s - loss: 2.1593 - val_loss: 1.7629\n",
            "108/108 - 6s - loss: 2.2391 - val_loss: 1.7225\n",
            "108/108 - 6s - loss: 2.1572 - val_loss: 1.6700\n",
            "108/108 - 6s - loss: 2.1570 - val_loss: 1.6876\n",
            "108/108 - 6s - loss: 2.2100 - val_loss: 1.6687\n",
            "108/108 - 6s - loss: 2.1544 - val_loss: 1.7484\n",
            "108/108 - 6s - loss: 2.1734 - val_loss: 1.7133\n",
            "108/108 - 6s - loss: 2.1257 - val_loss: 1.6831\n",
            "108/108 - 6s - loss: 2.1643 - val_loss: 1.7610\n",
            "108/108 - 6s - loss: 2.1266 - val_loss: 1.6795\n",
            "108/108 - 6s - loss: 2.1490 - val_loss: 1.6952\n",
            "108/108 - 6s - loss: 2.1334 - val_loss: 1.7751\n",
            "108/108 - 6s - loss: 2.1379 - val_loss: 1.6620\n",
            "108/108 - 6s - loss: 2.1841 - val_loss: 1.7210\n",
            "108/108 - 6s - loss: 2.1673 - val_loss: 1.6606\n",
            "108/108 - 6s - loss: 2.1480 - val_loss: 1.7597\n",
            "108/108 - 6s - loss: 2.0930 - val_loss: 1.7236\n",
            "108/108 - 6s - loss: 2.1132 - val_loss: 1.7080\n",
            "108/108 - 6s - loss: 2.1031 - val_loss: 1.7425\n",
            "108/108 - 6s - loss: 2.1413 - val_loss: 1.7240\n",
            "108/108 - 6s - loss: 2.0908 - val_loss: 1.6261\n",
            "108/108 - 6s - loss: 2.1299 - val_loss: 1.7646\n",
            "108/108 - 6s - loss: 2.0714 - val_loss: 1.6189\n",
            "108/108 - 6s - loss: 2.1311 - val_loss: 1.6533\n",
            "108/108 - 6s - loss: 2.1261 - val_loss: 1.7006\n",
            "108/108 - 6s - loss: 2.1295 - val_loss: 1.6402\n",
            "108/108 - 6s - loss: 2.1021 - val_loss: 1.6847\n",
            "108/108 - 6s - loss: 2.0958 - val_loss: 1.6790\n",
            "108/108 - 6s - loss: 2.0905 - val_loss: 1.6680\n",
            "108/108 - 6s - loss: 2.1301 - val_loss: 1.6552\n",
            "108/108 - 6s - loss: 2.0797 - val_loss: 1.6192\n",
            "108/108 - 6s - loss: 2.0209 - val_loss: 1.6987\n",
            "108/108 - 6s - loss: 2.1344 - val_loss: 1.6438\n",
            "108/108 - 6s - loss: 2.0530 - val_loss: 1.6045\n",
            "108/108 - 6s - loss: 2.0451 - val_loss: 1.6554\n",
            "108/108 - 6s - loss: 2.0814 - val_loss: 1.5909\n",
            "108/108 - 6s - loss: 2.0758 - val_loss: 1.5988\n",
            "108/108 - 6s - loss: 2.0613 - val_loss: 1.6105\n",
            "108/108 - 6s - loss: 2.0798 - val_loss: 1.6252\n",
            "108/108 - 6s - loss: 1.9930 - val_loss: 1.6325\n",
            "108/108 - 6s - loss: 2.0459 - val_loss: 1.6417\n",
            "108/108 - 6s - loss: 2.0640 - val_loss: 1.6494\n",
            "108/108 - 6s - loss: 2.0365 - val_loss: 1.6413\n",
            "108/108 - 6s - loss: 2.1281 - val_loss: 1.6813\n",
            "108/108 - 6s - loss: 2.1050 - val_loss: 1.6158\n",
            "108/108 - 6s - loss: 2.0554 - val_loss: 1.5965\n",
            "108/108 - 6s - loss: 2.1008 - val_loss: 1.6197\n",
            "108/108 - 6s - loss: 2.0483 - val_loss: 1.5943\n",
            "108/108 - 6s - loss: 2.0517 - val_loss: 1.5718\n",
            "108/108 - 6s - loss: 2.0082 - val_loss: 1.5621\n",
            "108/108 - 6s - loss: 2.0248 - val_loss: 1.6264\n",
            "108/108 - 6s - loss: 2.0512 - val_loss: 1.6230\n",
            "108/108 - 6s - loss: 2.0872 - val_loss: 1.6639\n",
            "108/108 - 6s - loss: 2.0092 - val_loss: 1.5997\n",
            "108/108 - 6s - loss: 2.0511 - val_loss: 1.6595\n",
            "108/108 - 6s - loss: 2.0202 - val_loss: 1.6672\n",
            "108/108 - 6s - loss: 2.0475 - val_loss: 1.5965\n",
            "108/108 - 6s - loss: 2.0098 - val_loss: 1.5790\n",
            "108/108 - 6s - loss: 2.0165 - val_loss: 1.5708\n",
            "108/108 - 6s - loss: 2.0698 - val_loss: 1.5753\n",
            "108/108 - 6s - loss: 2.0089 - val_loss: 1.6189\n",
            "108/108 - 6s - loss: 2.0382 - val_loss: 1.5361\n",
            "108/108 - 6s - loss: 1.9832 - val_loss: 1.6373\n",
            "108/108 - 6s - loss: 2.0409 - val_loss: 1.5759\n",
            "108/108 - 6s - loss: 2.0524 - val_loss: 1.5819\n",
            "108/108 - 6s - loss: 1.9982 - val_loss: 1.6113\n",
            "108/108 - 6s - loss: 2.0520 - val_loss: 1.6233\n",
            "108/108 - 6s - loss: 2.0124 - val_loss: 1.6212\n",
            "108/108 - 6s - loss: 2.0147 - val_loss: 1.5456\n",
            "108/108 - 6s - loss: 2.0379 - val_loss: 1.6469\n",
            "108/108 - 6s - loss: 2.0216 - val_loss: 1.5894\n",
            "108/108 - 6s - loss: 2.0009 - val_loss: 1.5920\n",
            "108/108 - 6s - loss: 2.0187 - val_loss: 1.6471\n",
            "108/108 - 6s - loss: 2.0466 - val_loss: 1.5877\n",
            "108/108 - 6s - loss: 2.0615 - val_loss: 1.6196\n",
            "108/108 - 6s - loss: 2.0467 - val_loss: 1.5983\n",
            "108/108 - 6s - loss: 1.9936 - val_loss: 1.6952\n",
            "108/108 - 6s - loss: 2.0923 - val_loss: 1.5625\n",
            "108/108 - 6s - loss: 1.9693 - val_loss: 1.5795\n",
            "108/108 - 6s - loss: 2.0320 - val_loss: 1.5508\n",
            "108/108 - 6s - loss: 2.0084 - val_loss: 1.5884\n",
            "108/108 - 6s - loss: 2.0095 - val_loss: 1.5721\n",
            "108/108 - 6s - loss: 2.0723 - val_loss: 1.5857\n",
            "108/108 - 6s - loss: 1.9919 - val_loss: 1.5717\n",
            "108/108 - 6s - loss: 2.0501 - val_loss: 1.6369\n",
            "108/108 - 6s - loss: 2.0330 - val_loss: 1.5679\n",
            "108/108 - 6s - loss: 2.0401 - val_loss: 1.5303\n",
            "108/108 - 6s - loss: 2.0192 - val_loss: 1.5675\n",
            "108/108 - 6s - loss: 1.9804 - val_loss: 1.5879\n",
            "108/108 - 6s - loss: 1.9602 - val_loss: 1.5718\n",
            "108/108 - 6s - loss: 2.0205 - val_loss: 1.5967\n",
            "108/108 - 6s - loss: 2.0264 - val_loss: 1.5550\n",
            "108/108 - 6s - loss: 2.0421 - val_loss: 1.5610\n",
            "108/108 - 6s - loss: 2.0081 - val_loss: 1.5624\n",
            "108/108 - 6s - loss: 1.9970 - val_loss: 1.6236\n",
            "108/108 - 6s - loss: 2.0552 - val_loss: 1.5657\n",
            "108/108 - 6s - loss: 2.0533 - val_loss: 1.5716\n",
            "108/108 - 6s - loss: 2.0353 - val_loss: 1.5925\n",
            "108/108 - 6s - loss: 2.0155 - val_loss: 1.5960\n",
            "108/108 - 6s - loss: 2.0208 - val_loss: 1.5590\n",
            "108/108 - 6s - loss: 2.0088 - val_loss: 1.5805\n",
            "108/108 - 6s - loss: 1.9889 - val_loss: 1.5426\n",
            "108/108 - 6s - loss: 2.0118 - val_loss: 1.5955\n",
            "108/108 - 6s - loss: 2.0718 - val_loss: 1.6339\n",
            "108/108 - 6s - loss: 2.0411 - val_loss: 1.5229\n",
            "108/108 - 6s - loss: 1.9434 - val_loss: 1.5530\n",
            "108/108 - 6s - loss: 2.0054 - val_loss: 1.5676\n",
            "108/108 - 6s - loss: 2.0457 - val_loss: 1.6098\n",
            "108/108 - 6s - loss: 2.0500 - val_loss: 1.5238\n",
            "108/108 - 6s - loss: 2.0035 - val_loss: 1.5337\n",
            "108/108 - 6s - loss: 2.0179 - val_loss: 1.5715\n",
            "108/108 - 6s - loss: 2.0458 - val_loss: 1.5327\n",
            "108/108 - 6s - loss: 2.0331 - val_loss: 1.5724\n",
            "108/108 - 6s - loss: 2.0295 - val_loss: 1.5741\n",
            "108/108 - 6s - loss: 1.9931 - val_loss: 1.5734\n",
            "108/108 - 6s - loss: 2.0371 - val_loss: 1.6677\n",
            "108/108 - 6s - loss: 2.0542 - val_loss: 1.5859\n",
            "108/108 - 6s - loss: 2.0304 - val_loss: 1.6061\n",
            "108/108 - 6s - loss: 1.9797 - val_loss: 1.5395\n",
            "108/108 - 6s - loss: 2.0264 - val_loss: 1.5690\n",
            "108/108 - 6s - loss: 2.0033 - val_loss: 1.5549\n",
            "108/108 - 6s - loss: 1.9741 - val_loss: 1.5989\n",
            "108/108 - 6s - loss: 1.9896 - val_loss: 1.5596\n",
            "108/108 - 6s - loss: 2.0137 - val_loss: 1.5167\n",
            "108/108 - 6s - loss: 1.9850 - val_loss: 1.5034\n",
            "108/108 - 6s - loss: 1.9953 - val_loss: 1.5112\n",
            "108/108 - 6s - loss: 1.9962 - val_loss: 1.5593\n",
            "108/108 - 6s - loss: 1.9910 - val_loss: 1.5673\n",
            "108/108 - 6s - loss: 1.9679 - val_loss: 1.5699\n",
            "108/108 - 6s - loss: 2.0130 - val_loss: 1.5096\n",
            "108/108 - 6s - loss: 2.0097 - val_loss: 1.5349\n",
            "108/108 - 6s - loss: 1.9431 - val_loss: 1.4920\n",
            "108/108 - 6s - loss: 1.9618 - val_loss: 1.5406\n",
            "108/108 - 6s - loss: 1.9654 - val_loss: 1.5577\n",
            "108/108 - 6s - loss: 2.0481 - val_loss: 1.5703\n",
            "108/108 - 6s - loss: 1.9663 - val_loss: 1.5283\n",
            "108/108 - 6s - loss: 2.0136 - val_loss: 1.5235\n",
            "108/108 - 6s - loss: 1.9450 - val_loss: 1.5203\n",
            "108/108 - 6s - loss: 1.9875 - val_loss: 1.5149\n",
            "108/108 - 6s - loss: 1.9929 - val_loss: 1.5472\n",
            "108/108 - 6s - loss: 2.0568 - val_loss: 1.5555\n",
            "108/108 - 6s - loss: 2.0376 - val_loss: 1.6052\n",
            "108/108 - 6s - loss: 1.9674 - val_loss: 1.5620\n",
            "108/108 - 6s - loss: 1.9938 - val_loss: 1.5705\n",
            "108/108 - 6s - loss: 1.9451 - val_loss: 1.5531\n",
            "108/108 - 6s - loss: 1.9901 - val_loss: 1.6320\n",
            "108/108 - 6s - loss: 1.9568 - val_loss: 1.5977\n",
            "108/108 - 6s - loss: 2.0204 - val_loss: 1.6076\n",
            "108/108 - 6s - loss: 1.9839 - val_loss: 1.5102\n",
            "108/108 - 6s - loss: 1.9962 - val_loss: 1.5312\n",
            "108/108 - 6s - loss: 1.9901 - val_loss: 1.5061\n",
            "108/108 - 6s - loss: 1.9891 - val_loss: 1.5950\n",
            "108/108 - 6s - loss: 2.0429 - val_loss: 1.5417\n",
            "108/108 - 6s - loss: 1.9637 - val_loss: 1.5479\n",
            "108/108 - 6s - loss: 1.9718 - val_loss: 1.5985\n",
            "108/108 - 6s - loss: 1.9823 - val_loss: 1.5241\n",
            "108/108 - 6s - loss: 1.9896 - val_loss: 1.5624\n",
            "108/108 - 6s - loss: 1.9841 - val_loss: 1.5037\n",
            "108/108 - 6s - loss: 2.0240 - val_loss: 1.5382\n",
            "108/108 - 6s - loss: 1.9630 - val_loss: 1.4831\n",
            "108/108 - 6s - loss: 1.9827 - val_loss: 1.6016\n",
            "108/108 - 6s - loss: 1.9788 - val_loss: 1.5667\n",
            "108/108 - 6s - loss: 2.0018 - val_loss: 1.5829\n",
            "108/108 - 6s - loss: 1.9744 - val_loss: 1.5570\n",
            "108/108 - 6s - loss: 1.9707 - val_loss: 1.5143\n",
            "108/108 - 6s - loss: 1.9878 - val_loss: 1.6154\n",
            "108/108 - 6s - loss: 2.0025 - val_loss: 1.5808\n",
            "108/108 - 6s - loss: 2.0093 - val_loss: 1.5829\n",
            "108/108 - 6s - loss: 1.9878 - val_loss: 1.5663\n",
            "108/108 - 6s - loss: 1.9654 - val_loss: 1.5646\n",
            "108/108 - 6s - loss: 1.9439 - val_loss: 1.5141\n",
            "108/108 - 6s - loss: 1.9422 - val_loss: 1.5442\n",
            "108/108 - 6s - loss: 1.9398 - val_loss: 1.5277\n",
            "108/108 - 6s - loss: 1.9803 - val_loss: 1.5854\n",
            "108/108 - 6s - loss: 1.9948 - val_loss: 1.5531\n",
            "108/108 - 6s - loss: 1.9848 - val_loss: 1.5389\n",
            "108/108 - 6s - loss: 1.9279 - val_loss: 1.5568\n",
            "108/108 - 6s - loss: 1.9804 - val_loss: 1.5368\n",
            "108/108 - 6s - loss: 1.9314 - val_loss: 1.5703\n",
            "108/108 - 6s - loss: 1.9331 - val_loss: 1.5000\n",
            "108/108 - 6s - loss: 1.9878 - val_loss: 1.5791\n",
            "108/108 - 6s - loss: 1.9230 - val_loss: 1.5966\n",
            "108/108 - 6s - loss: 1.9444 - val_loss: 1.5712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBP1-6BZSAoc",
        "outputId": "21b4eb55-99dc-49c7-8a3e-da48e738eb73"
      },
      "source": [
        "n_batch = 8\r\n",
        "early_stopping = EarlyStopping(monitor='val_loss',patience = 20)\r\n",
        "\r\n",
        "for l, q in enumerate(np.arange(0.1, 1, 0.1)):\r\n",
        "  print(q)\r\n",
        "  model = Sequential()\r\n",
        "  for i in range(2):\r\n",
        "      model.add(LSTM(64, batch_input_shape=(8, 336, 8), stateful=True, return_sequences=True))\r\n",
        "      model.add(Dropout(0.3))\r\n",
        "  model.add(LSTM(64, batch_input_shape=(8, 336, 8), stateful=True))\r\n",
        "  model.add(Dropout(0.3))\r\n",
        "  model.add(Dense(96))\r\n",
        "  model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "  for j in range(500):\r\n",
        "    # model.fit(train_x, train_y, epochs=1, batch_size=1, shuffle=False, callbacks=[custom_hist], validation_data=(val_x, val_y))\r\n",
        "    model.fit(train_x, train_y, epochs=1, batch_size=n_batch, shuffle=False, validation_data=(val_x, val_y), callbacks=[early_stopping], verbose=2)\r\n",
        "    model.reset_states()\r\n",
        "  weights = model.get_weights()\r\n",
        "  \r\n",
        "  single_item_model = Sequential()\r\n",
        "  for i in range(2):\r\n",
        "      single_item_model.add(LSTM(64, batch_input_shape=(1, 336, 8), stateful=True, return_sequences=True))\r\n",
        "      single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(LSTM(64, batch_input_shape=(1, 336, 8), stateful=True))\r\n",
        "  single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(Dense(96))\r\n",
        "  single_item_model.set_weights(weights)\r\n",
        "  single_item_model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "  \r\n",
        "  predictions = []\r\n",
        "  for k in range(81):\r\n",
        "    prediction = single_item_model.predict(np.array([test[k]]), batch_size=1)\r\n",
        "    predictions.append(sun_rise(np.array([test[k]]),prediction))\r\n",
        "  predictions = np.reshape(np.concatenate(np.array(predictions), axis=0),(81*96))\r\n",
        "  submission.iloc[:,l+1] = predictions\r\n",
        "submission.to_csv(f'submission.csv_2', index=False)\r\n",
        "print('finish')"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1\n",
            "108/108 - 8s - loss: 1.7575 - val_loss: 1.5311\n",
            "108/108 - 4s - loss: 1.6007 - val_loss: 1.4540\n",
            "108/108 - 4s - loss: 1.5604 - val_loss: 1.4362\n",
            "108/108 - 4s - loss: 1.5455 - val_loss: 1.4266\n",
            "108/108 - 4s - loss: 1.5393 - val_loss: 1.4232\n",
            "108/108 - 4s - loss: 1.5271 - val_loss: 1.4174\n",
            "108/108 - 4s - loss: 1.5268 - val_loss: 1.4139\n",
            "108/108 - 4s - loss: 1.5212 - val_loss: 1.4105\n",
            "108/108 - 4s - loss: 1.5130 - val_loss: 1.4087\n",
            "108/108 - 4s - loss: 1.5121 - val_loss: 1.4067\n",
            "108/108 - 4s - loss: 1.5113 - val_loss: 1.4058\n",
            "108/108 - 4s - loss: 1.5082 - val_loss: 1.4018\n",
            "108/108 - 4s - loss: 1.5086 - val_loss: 1.4019\n",
            "108/108 - 4s - loss: 1.5039 - val_loss: 1.3970\n",
            "108/108 - 4s - loss: 1.5027 - val_loss: 1.3951\n",
            "108/108 - 4s - loss: 1.4984 - val_loss: 1.3882\n",
            "108/108 - 4s - loss: 1.4916 - val_loss: 1.3919\n",
            "108/108 - 4s - loss: 1.4904 - val_loss: 1.3898\n",
            "108/108 - 4s - loss: 1.4837 - val_loss: 1.3887\n",
            "108/108 - 4s - loss: 1.4831 - val_loss: 1.3875\n",
            "108/108 - 4s - loss: 1.4850 - val_loss: 1.3876\n",
            "108/108 - 4s - loss: 1.4792 - val_loss: 1.3870\n",
            "108/108 - 4s - loss: 1.4800 - val_loss: 1.3847\n",
            "108/108 - 4s - loss: 1.4752 - val_loss: 1.3794\n",
            "108/108 - 4s - loss: 1.4759 - val_loss: 1.3758\n",
            "108/108 - 4s - loss: 1.4734 - val_loss: 1.3754\n",
            "108/108 - 4s - loss: 1.4715 - val_loss: 1.3739\n",
            "108/108 - 4s - loss: 1.4670 - val_loss: 1.3773\n",
            "108/108 - 4s - loss: 1.4631 - val_loss: 1.3762\n",
            "108/108 - 4s - loss: 1.4629 - val_loss: 1.3747\n",
            "108/108 - 4s - loss: 1.4662 - val_loss: 1.3722\n",
            "108/108 - 4s - loss: 1.4624 - val_loss: 1.3778\n",
            "108/108 - 4s - loss: 1.4688 - val_loss: 1.3739\n",
            "108/108 - 4s - loss: 1.4522 - val_loss: 1.3697\n",
            "108/108 - 4s - loss: 1.4535 - val_loss: 1.3682\n",
            "108/108 - 4s - loss: 1.4456 - val_loss: 1.3649\n",
            "108/108 - 4s - loss: 1.4493 - val_loss: 1.3651\n",
            "108/108 - 4s - loss: 1.4508 - val_loss: 1.3594\n",
            "108/108 - 4s - loss: 1.4445 - val_loss: 1.3674\n",
            "108/108 - 4s - loss: 1.4461 - val_loss: 1.3707\n",
            "108/108 - 4s - loss: 1.4389 - val_loss: 1.3504\n",
            "108/108 - 4s - loss: 1.4358 - val_loss: 1.3525\n",
            "108/108 - 4s - loss: 1.4304 - val_loss: 1.3490\n",
            "108/108 - 4s - loss: 1.4221 - val_loss: 1.3432\n",
            "108/108 - 4s - loss: 1.4222 - val_loss: 1.3552\n",
            "108/108 - 4s - loss: 1.4304 - val_loss: 1.3496\n",
            "108/108 - 4s - loss: 1.4136 - val_loss: 1.3491\n",
            "108/108 - 4s - loss: 1.4039 - val_loss: 1.3451\n",
            "108/108 - 4s - loss: 1.4057 - val_loss: 1.3444\n",
            "108/108 - 4s - loss: 1.4016 - val_loss: 1.3587\n",
            "108/108 - 4s - loss: 1.4030 - val_loss: 1.3623\n",
            "108/108 - 4s - loss: 1.3930 - val_loss: 1.3563\n",
            "108/108 - 4s - loss: 1.3910 - val_loss: 1.3585\n",
            "108/108 - 4s - loss: 1.3908 - val_loss: 1.3560\n",
            "108/108 - 4s - loss: 1.3901 - val_loss: 1.3441\n",
            "108/108 - 4s - loss: 1.3796 - val_loss: 1.3345\n",
            "108/108 - 4s - loss: 1.3809 - val_loss: 1.3388\n",
            "108/108 - 4s - loss: 1.3743 - val_loss: 1.3437\n",
            "108/108 - 4s - loss: 1.3757 - val_loss: 1.3317\n",
            "108/108 - 4s - loss: 1.3666 - val_loss: 1.3451\n",
            "108/108 - 4s - loss: 1.3639 - val_loss: 1.3505\n",
            "108/108 - 4s - loss: 1.3625 - val_loss: 1.3575\n",
            "108/108 - 4s - loss: 1.3600 - val_loss: 1.3603\n",
            "108/108 - 4s - loss: 1.3550 - val_loss: 1.3538\n",
            "108/108 - 4s - loss: 1.3549 - val_loss: 1.3405\n",
            "108/108 - 4s - loss: 1.3478 - val_loss: 1.3618\n",
            "108/108 - 4s - loss: 1.3579 - val_loss: 1.3906\n",
            "108/108 - 4s - loss: 1.3503 - val_loss: 1.3665\n",
            "108/108 - 4s - loss: 1.3356 - val_loss: 1.3702\n",
            "108/108 - 4s - loss: 1.3416 - val_loss: 1.3831\n",
            "108/108 - 4s - loss: 1.3432 - val_loss: 1.3677\n",
            "108/108 - 4s - loss: 1.3348 - val_loss: 1.3964\n",
            "108/108 - 4s - loss: 1.3332 - val_loss: 1.3959\n",
            "108/108 - 4s - loss: 1.3273 - val_loss: 1.3876\n",
            "108/108 - 4s - loss: 1.3410 - val_loss: 1.3670\n",
            "108/108 - 4s - loss: 1.3297 - val_loss: 1.3906\n",
            "108/108 - 4s - loss: 1.3135 - val_loss: 1.3743\n",
            "108/108 - 4s - loss: 1.3293 - val_loss: 1.3947\n",
            "108/108 - 4s - loss: 1.3212 - val_loss: 1.3845\n",
            "108/108 - 4s - loss: 1.3230 - val_loss: 1.3827\n",
            "108/108 - 4s - loss: 1.3240 - val_loss: 1.3763\n",
            "108/108 - 4s - loss: 1.3387 - val_loss: 1.3814\n",
            "108/108 - 4s - loss: 1.3157 - val_loss: 1.3780\n",
            "108/108 - 4s - loss: 1.3064 - val_loss: 1.3814\n",
            "108/108 - 4s - loss: 1.3151 - val_loss: 1.3614\n",
            "108/108 - 4s - loss: 1.3037 - val_loss: 1.3699\n",
            "108/108 - 4s - loss: 1.2945 - val_loss: 1.4123\n",
            "108/108 - 4s - loss: 1.2975 - val_loss: 1.4546\n",
            "108/108 - 4s - loss: 1.3100 - val_loss: 1.4285\n",
            "108/108 - 4s - loss: 1.2967 - val_loss: 1.4163\n",
            "108/108 - 4s - loss: 1.3178 - val_loss: 1.4407\n",
            "108/108 - 4s - loss: 1.3044 - val_loss: 1.4105\n",
            "108/108 - 4s - loss: 1.3005 - val_loss: 1.4119\n",
            "108/108 - 4s - loss: 1.2932 - val_loss: 1.4442\n",
            "108/108 - 4s - loss: 1.2968 - val_loss: 1.4197\n",
            "108/108 - 4s - loss: 1.2940 - val_loss: 1.3995\n",
            "108/108 - 4s - loss: 1.2924 - val_loss: 1.4176\n",
            "108/108 - 4s - loss: 1.2912 - val_loss: 1.4312\n",
            "108/108 - 4s - loss: 1.2864 - val_loss: 1.4799\n",
            "108/108 - 4s - loss: 1.2822 - val_loss: 1.4383\n",
            "108/108 - 4s - loss: 1.2814 - val_loss: 1.4039\n",
            "108/108 - 4s - loss: 1.2735 - val_loss: 1.4459\n",
            "108/108 - 4s - loss: 1.2807 - val_loss: 1.4357\n",
            "108/108 - 4s - loss: 1.2618 - val_loss: 1.4105\n",
            "108/108 - 4s - loss: 1.2611 - val_loss: 1.4336\n",
            "108/108 - 4s - loss: 1.2625 - val_loss: 1.4160\n",
            "108/108 - 4s - loss: 1.2774 - val_loss: 1.4486\n",
            "108/108 - 4s - loss: 1.2514 - val_loss: 1.4407\n",
            "108/108 - 4s - loss: 1.2591 - val_loss: 1.4448\n",
            "108/108 - 4s - loss: 1.2551 - val_loss: 1.4397\n",
            "108/108 - 4s - loss: 1.2500 - val_loss: 1.4226\n",
            "108/108 - 4s - loss: 1.2510 - val_loss: 1.4286\n",
            "108/108 - 4s - loss: 1.2571 - val_loss: 1.3703\n",
            "108/108 - 4s - loss: 1.2760 - val_loss: 1.3916\n",
            "108/108 - 4s - loss: 1.2535 - val_loss: 1.4269\n",
            "108/108 - 4s - loss: 1.2507 - val_loss: 1.3647\n",
            "108/108 - 4s - loss: 1.2452 - val_loss: 1.4038\n",
            "108/108 - 4s - loss: 1.2441 - val_loss: 1.4312\n",
            "108/108 - 4s - loss: 1.2369 - val_loss: 1.4283\n",
            "108/108 - 4s - loss: 1.2431 - val_loss: 1.4575\n",
            "108/108 - 4s - loss: 1.2392 - val_loss: 1.3920\n",
            "108/108 - 4s - loss: 1.2612 - val_loss: 1.4365\n",
            "108/108 - 4s - loss: 1.2422 - val_loss: 1.4581\n",
            "108/108 - 4s - loss: 1.2338 - val_loss: 1.4137\n",
            "108/108 - 4s - loss: 1.2358 - val_loss: 1.4382\n",
            "108/108 - 4s - loss: 1.2209 - val_loss: 1.4498\n",
            "108/108 - 4s - loss: 1.2288 - val_loss: 1.4370\n",
            "108/108 - 4s - loss: 1.2189 - val_loss: 1.4611\n",
            "108/108 - 4s - loss: 1.2238 - val_loss: 1.4110\n",
            "108/108 - 4s - loss: 1.2260 - val_loss: 1.4429\n",
            "108/108 - 4s - loss: 1.2193 - val_loss: 1.4453\n",
            "108/108 - 4s - loss: 1.2148 - val_loss: 1.4829\n",
            "108/108 - 4s - loss: 1.2258 - val_loss: 1.4533\n",
            "108/108 - 4s - loss: 1.2356 - val_loss: 1.4693\n",
            "108/108 - 4s - loss: 1.2199 - val_loss: 1.4645\n",
            "108/108 - 4s - loss: 1.2187 - val_loss: 1.4664\n",
            "108/108 - 4s - loss: 1.2123 - val_loss: 1.4812\n",
            "108/108 - 4s - loss: 1.2286 - val_loss: 1.4788\n",
            "108/108 - 4s - loss: 1.2374 - val_loss: 1.4259\n",
            "108/108 - 4s - loss: 1.2205 - val_loss: 1.4860\n",
            "108/108 - 4s - loss: 1.2210 - val_loss: 1.4866\n",
            "108/108 - 4s - loss: 1.2113 - val_loss: 1.4141\n",
            "108/108 - 4s - loss: 1.1927 - val_loss: 1.4427\n",
            "108/108 - 4s - loss: 1.2111 - val_loss: 1.4399\n",
            "108/108 - 4s - loss: 1.2146 - val_loss: 1.4500\n",
            "108/108 - 4s - loss: 1.1990 - val_loss: 1.4406\n",
            "108/108 - 4s - loss: 1.2124 - val_loss: 1.4713\n",
            "108/108 - 4s - loss: 1.2053 - val_loss: 1.4500\n",
            "108/108 - 4s - loss: 1.1871 - val_loss: 1.4502\n",
            "108/108 - 4s - loss: 1.1896 - val_loss: 1.5015\n",
            "108/108 - 4s - loss: 1.1765 - val_loss: 1.4165\n",
            "108/108 - 4s - loss: 1.1950 - val_loss: 1.4449\n",
            "108/108 - 4s - loss: 1.1926 - val_loss: 1.4501\n",
            "108/108 - 4s - loss: 1.1943 - val_loss: 1.4226\n",
            "108/108 - 4s - loss: 1.1999 - val_loss: 1.4519\n",
            "108/108 - 4s - loss: 1.1838 - val_loss: 1.4428\n",
            "108/108 - 4s - loss: 1.1859 - val_loss: 1.4785\n",
            "108/108 - 4s - loss: 1.1836 - val_loss: 1.4585\n",
            "108/108 - 4s - loss: 1.1825 - val_loss: 1.4847\n",
            "108/108 - 4s - loss: 1.2015 - val_loss: 1.4691\n",
            "108/108 - 4s - loss: 1.1856 - val_loss: 1.4542\n",
            "108/108 - 4s - loss: 1.1861 - val_loss: 1.4502\n",
            "108/108 - 4s - loss: 1.1897 - val_loss: 1.4411\n",
            "108/108 - 4s - loss: 1.1840 - val_loss: 1.4742\n",
            "108/108 - 4s - loss: 1.1803 - val_loss: 1.4373\n",
            "108/108 - 4s - loss: 1.1724 - val_loss: 1.4691\n",
            "108/108 - 4s - loss: 1.1723 - val_loss: 1.4673\n",
            "108/108 - 4s - loss: 1.1872 - val_loss: 1.3888\n",
            "108/108 - 4s - loss: 1.1708 - val_loss: 1.4546\n",
            "108/108 - 4s - loss: 1.1861 - val_loss: 1.4449\n",
            "108/108 - 4s - loss: 1.1592 - val_loss: 1.4618\n",
            "108/108 - 4s - loss: 1.1737 - val_loss: 1.3866\n",
            "108/108 - 4s - loss: 1.1967 - val_loss: 1.3493\n",
            "108/108 - 4s - loss: 1.2093 - val_loss: 1.4309\n",
            "108/108 - 4s - loss: 1.1838 - val_loss: 1.4557\n",
            "108/108 - 4s - loss: 1.1719 - val_loss: 1.4241\n",
            "108/108 - 4s - loss: 1.2061 - val_loss: 1.4169\n",
            "108/108 - 4s - loss: 1.1731 - val_loss: 1.4709\n",
            "108/108 - 4s - loss: 1.1749 - val_loss: 1.4592\n",
            "108/108 - 4s - loss: 1.1878 - val_loss: 1.4311\n",
            "108/108 - 4s - loss: 1.1709 - val_loss: 1.4705\n",
            "108/108 - 4s - loss: 1.1615 - val_loss: 1.4318\n",
            "108/108 - 4s - loss: 1.1861 - val_loss: 1.4372\n",
            "108/108 - 4s - loss: 1.1580 - val_loss: 1.4561\n",
            "108/108 - 4s - loss: 1.1567 - val_loss: 1.4271\n",
            "108/108 - 4s - loss: 1.1457 - val_loss: 1.4266\n",
            "108/108 - 4s - loss: 1.1554 - val_loss: 1.4351\n",
            "108/108 - 4s - loss: 1.2241 - val_loss: 1.4473\n",
            "108/108 - 4s - loss: 1.1756 - val_loss: 1.4266\n",
            "108/108 - 4s - loss: 1.1729 - val_loss: 1.4383\n",
            "108/108 - 4s - loss: 1.1488 - val_loss: 1.3986\n",
            "108/108 - 4s - loss: 1.1688 - val_loss: 1.4167\n",
            "108/108 - 4s - loss: 1.1774 - val_loss: 1.4570\n",
            "108/108 - 4s - loss: 1.1593 - val_loss: 1.4467\n",
            "108/108 - 4s - loss: 1.1650 - val_loss: 1.4085\n",
            "108/108 - 4s - loss: 1.1488 - val_loss: 1.4378\n",
            "108/108 - 4s - loss: 1.1435 - val_loss: 1.4460\n",
            "108/108 - 4s - loss: 1.1346 - val_loss: 1.4322\n",
            "108/108 - 4s - loss: 1.1497 - val_loss: 1.4335\n",
            "108/108 - 4s - loss: 1.1989 - val_loss: 1.3931\n",
            "108/108 - 4s - loss: 1.1646 - val_loss: 1.4130\n",
            "108/108 - 4s - loss: 1.1471 - val_loss: 1.4207\n",
            "108/108 - 4s - loss: 1.1357 - val_loss: 1.3676\n",
            "108/108 - 4s - loss: 1.1467 - val_loss: 1.4355\n",
            "108/108 - 4s - loss: 1.1405 - val_loss: 1.4050\n",
            "108/108 - 4s - loss: 1.1865 - val_loss: 1.4355\n",
            "108/108 - 4s - loss: 1.1741 - val_loss: 1.4220\n",
            "108/108 - 4s - loss: 1.1562 - val_loss: 1.4999\n",
            "108/108 - 4s - loss: 1.1473 - val_loss: 1.3920\n",
            "108/108 - 4s - loss: 1.1461 - val_loss: 1.4062\n",
            "108/108 - 4s - loss: 1.1411 - val_loss: 1.4061\n",
            "108/108 - 4s - loss: 1.1513 - val_loss: 1.4590\n",
            "108/108 - 4s - loss: 1.1630 - val_loss: 1.4833\n",
            "108/108 - 4s - loss: 1.1914 - val_loss: 1.4941\n",
            "108/108 - 4s - loss: 1.1701 - val_loss: 1.4993\n",
            "108/108 - 4s - loss: 1.1480 - val_loss: 1.4582\n",
            "108/108 - 4s - loss: 1.1388 - val_loss: 1.4954\n",
            "108/108 - 4s - loss: 1.1331 - val_loss: 1.4517\n",
            "108/108 - 4s - loss: 1.1497 - val_loss: 1.3642\n",
            "108/108 - 4s - loss: 1.1610 - val_loss: 1.4542\n",
            "108/108 - 4s - loss: 1.1798 - val_loss: 1.4181\n",
            "108/108 - 4s - loss: 1.1921 - val_loss: 1.3912\n",
            "108/108 - 4s - loss: 1.1672 - val_loss: 1.4367\n",
            "108/108 - 4s - loss: 1.1359 - val_loss: 1.4414\n",
            "108/108 - 4s - loss: 1.1274 - val_loss: 1.4133\n",
            "108/108 - 4s - loss: 1.1170 - val_loss: 1.4519\n",
            "108/108 - 4s - loss: 1.1388 - val_loss: 1.4368\n",
            "108/108 - 4s - loss: 1.1265 - val_loss: 1.4950\n",
            "108/108 - 4s - loss: 1.1213 - val_loss: 1.4860\n",
            "108/108 - 4s - loss: 1.1146 - val_loss: 1.4355\n",
            "108/108 - 4s - loss: 1.1212 - val_loss: 1.4596\n",
            "108/108 - 4s - loss: 1.1296 - val_loss: 1.4688\n",
            "108/108 - 4s - loss: 1.1139 - val_loss: 1.4472\n",
            "108/108 - 4s - loss: 1.1297 - val_loss: 1.4741\n",
            "108/108 - 4s - loss: 1.1529 - val_loss: 1.4559\n",
            "108/108 - 4s - loss: 1.1512 - val_loss: 1.5061\n",
            "108/108 - 4s - loss: 1.1248 - val_loss: 1.4844\n",
            "108/108 - 4s - loss: 1.1296 - val_loss: 1.5218\n",
            "108/108 - 4s - loss: 1.1144 - val_loss: 1.4672\n",
            "108/108 - 4s - loss: 1.1190 - val_loss: 1.4378\n",
            "108/108 - 4s - loss: 1.1052 - val_loss: 1.4671\n",
            "108/108 - 4s - loss: 1.0995 - val_loss: 1.5024\n",
            "108/108 - 4s - loss: 1.1042 - val_loss: 1.4702\n",
            "108/108 - 4s - loss: 1.1051 - val_loss: 1.5026\n",
            "108/108 - 4s - loss: 1.1001 - val_loss: 1.5073\n",
            "108/108 - 4s - loss: 1.0946 - val_loss: 1.4212\n",
            "108/108 - 4s - loss: 1.1018 - val_loss: 1.4114\n",
            "108/108 - 3s - loss: 1.1366 - val_loss: 1.4399\n",
            "108/108 - 4s - loss: 1.1053 - val_loss: 1.4144\n",
            "108/108 - 4s - loss: 1.1118 - val_loss: 1.4867\n",
            "108/108 - 4s - loss: 1.1111 - val_loss: 1.4982\n",
            "108/108 - 4s - loss: 1.1053 - val_loss: 1.4343\n",
            "108/108 - 4s - loss: 1.1099 - val_loss: 1.4765\n",
            "108/108 - 4s - loss: 1.1070 - val_loss: 1.4370\n",
            "108/108 - 4s - loss: 1.1092 - val_loss: 1.3962\n",
            "108/108 - 4s - loss: 1.0890 - val_loss: 1.4352\n",
            "108/108 - 4s - loss: 1.1051 - val_loss: 1.4644\n",
            "108/108 - 4s - loss: 1.1146 - val_loss: 1.3996\n",
            "108/108 - 4s - loss: 1.0821 - val_loss: 1.4147\n",
            "108/108 - 4s - loss: 1.0900 - val_loss: 1.4399\n",
            "108/108 - 4s - loss: 1.0930 - val_loss: 1.4115\n",
            "108/108 - 4s - loss: 1.0835 - val_loss: 1.4311\n",
            "108/108 - 4s - loss: 1.0823 - val_loss: 1.5265\n",
            "108/108 - 4s - loss: 1.0834 - val_loss: 1.4290\n",
            "108/108 - 4s - loss: 1.1035 - val_loss: 1.4126\n",
            "108/108 - 4s - loss: 1.1075 - val_loss: 1.3973\n",
            "108/108 - 4s - loss: 1.1154 - val_loss: 1.4788\n",
            "108/108 - 4s - loss: 1.1204 - val_loss: 1.4063\n",
            "108/108 - 4s - loss: 1.1141 - val_loss: 1.4801\n",
            "108/108 - 4s - loss: 1.0989 - val_loss: 1.4924\n",
            "108/108 - 4s - loss: 1.1115 - val_loss: 1.4441\n",
            "108/108 - 4s - loss: 1.0939 - val_loss: 1.4524\n",
            "108/108 - 4s - loss: 1.0900 - val_loss: 1.5390\n",
            "108/108 - 4s - loss: 1.0871 - val_loss: 1.4236\n",
            "108/108 - 4s - loss: 1.0791 - val_loss: 1.4151\n",
            "108/108 - 4s - loss: 1.0833 - val_loss: 1.4847\n",
            "108/108 - 4s - loss: 1.0806 - val_loss: 1.4862\n",
            "108/108 - 4s - loss: 1.0849 - val_loss: 1.4862\n",
            "108/108 - 4s - loss: 1.0820 - val_loss: 1.5237\n",
            "108/108 - 4s - loss: 1.0930 - val_loss: 1.5347\n",
            "108/108 - 4s - loss: 1.1447 - val_loss: 1.4348\n",
            "108/108 - 4s - loss: 1.1292 - val_loss: 1.4340\n",
            "108/108 - 4s - loss: 1.1049 - val_loss: 1.4677\n",
            "108/108 - 4s - loss: 1.1048 - val_loss: 1.4432\n",
            "108/108 - 4s - loss: 1.0850 - val_loss: 1.4551\n",
            "108/108 - 3s - loss: 1.0813 - val_loss: 1.4509\n",
            "108/108 - 3s - loss: 1.0854 - val_loss: 1.4105\n",
            "108/108 - 4s - loss: 1.0970 - val_loss: 1.4655\n",
            "108/108 - 4s - loss: 1.0843 - val_loss: 1.5470\n",
            "108/108 - 4s - loss: 1.0877 - val_loss: 1.4953\n",
            "108/108 - 4s - loss: 1.0758 - val_loss: 1.4923\n",
            "108/108 - 4s - loss: 1.0720 - val_loss: 1.4693\n",
            "108/108 - 4s - loss: 1.0779 - val_loss: 1.4785\n",
            "108/108 - 4s - loss: 1.0647 - val_loss: 1.4200\n",
            "108/108 - 4s - loss: 1.0628 - val_loss: 1.4587\n",
            "108/108 - 4s - loss: 1.0714 - val_loss: 1.4557\n",
            "108/108 - 4s - loss: 1.0665 - val_loss: 1.4274\n",
            "108/108 - 4s - loss: 1.0722 - val_loss: 1.4430\n",
            "108/108 - 4s - loss: 1.0702 - val_loss: 1.4488\n",
            "108/108 - 4s - loss: 1.0692 - val_loss: 1.4066\n",
            "108/108 - 4s - loss: 1.0754 - val_loss: 1.5412\n",
            "108/108 - 4s - loss: 1.0725 - val_loss: 1.4771\n",
            "108/108 - 4s - loss: 1.0734 - val_loss: 1.5418\n",
            "108/108 - 4s - loss: 1.0760 - val_loss: 1.4806\n",
            "108/108 - 4s - loss: 1.0731 - val_loss: 1.4763\n",
            "108/108 - 4s - loss: 1.0647 - val_loss: 1.4849\n",
            "108/108 - 4s - loss: 1.0676 - val_loss: 1.4512\n",
            "108/108 - 4s - loss: 1.0769 - val_loss: 1.5192\n",
            "108/108 - 4s - loss: 1.0550 - val_loss: 1.4760\n",
            "108/108 - 4s - loss: 1.0462 - val_loss: 1.4407\n",
            "108/108 - 4s - loss: 1.0609 - val_loss: 1.4727\n",
            "108/108 - 4s - loss: 1.0818 - val_loss: 1.4324\n",
            "108/108 - 4s - loss: 1.0741 - val_loss: 1.4762\n",
            "108/108 - 4s - loss: 1.0649 - val_loss: 1.4737\n",
            "108/108 - 4s - loss: 1.0632 - val_loss: 1.4788\n",
            "108/108 - 4s - loss: 1.0450 - val_loss: 1.4161\n",
            "108/108 - 4s - loss: 1.0567 - val_loss: 1.4597\n",
            "108/108 - 4s - loss: 1.0691 - val_loss: 1.4044\n",
            "108/108 - 4s - loss: 1.0728 - val_loss: 1.4279\n",
            "108/108 - 4s - loss: 1.0924 - val_loss: 1.4423\n",
            "108/108 - 4s - loss: 1.0571 - val_loss: 1.3890\n",
            "108/108 - 4s - loss: 1.0649 - val_loss: 1.4227\n",
            "108/108 - 4s - loss: 1.0517 - val_loss: 1.5494\n",
            "108/108 - 4s - loss: 1.1189 - val_loss: 1.5160\n",
            "108/108 - 4s - loss: 1.0966 - val_loss: 1.4438\n",
            "108/108 - 4s - loss: 1.0811 - val_loss: 1.5181\n",
            "108/108 - 4s - loss: 1.0737 - val_loss: 1.4452\n",
            "108/108 - 4s - loss: 1.1051 - val_loss: 1.3889\n",
            "108/108 - 4s - loss: 1.0883 - val_loss: 1.4608\n",
            "108/108 - 4s - loss: 1.0662 - val_loss: 1.4982\n",
            "108/108 - 4s - loss: 1.0612 - val_loss: 1.4815\n",
            "108/108 - 4s - loss: 1.0587 - val_loss: 1.4434\n",
            "108/108 - 4s - loss: 1.0491 - val_loss: 1.4216\n",
            "108/108 - 4s - loss: 1.0600 - val_loss: 1.4375\n",
            "108/108 - 4s - loss: 1.0403 - val_loss: 1.4569\n",
            "108/108 - 4s - loss: 1.0553 - val_loss: 1.4231\n",
            "108/108 - 4s - loss: 1.0538 - val_loss: 1.3971\n",
            "108/108 - 4s - loss: 1.0470 - val_loss: 1.3701\n",
            "108/108 - 4s - loss: 1.0507 - val_loss: 1.3932\n",
            "108/108 - 4s - loss: 1.0442 - val_loss: 1.4172\n",
            "108/108 - 4s - loss: 1.0388 - val_loss: 1.4067\n",
            "108/108 - 4s - loss: 1.0246 - val_loss: 1.4315\n",
            "108/108 - 4s - loss: 1.0444 - val_loss: 1.4793\n",
            "108/108 - 4s - loss: 1.0410 - val_loss: 1.4284\n",
            "108/108 - 4s - loss: 1.0466 - val_loss: 1.4436\n",
            "108/108 - 4s - loss: 1.0461 - val_loss: 1.4431\n",
            "108/108 - 4s - loss: 1.0457 - val_loss: 1.4522\n",
            "108/108 - 4s - loss: 1.0582 - val_loss: 1.3877\n",
            "108/108 - 4s - loss: 1.0528 - val_loss: 1.4164\n",
            "108/108 - 4s - loss: 1.0498 - val_loss: 1.4684\n",
            "108/108 - 4s - loss: 1.0502 - val_loss: 1.4919\n",
            "108/108 - 4s - loss: 1.0467 - val_loss: 1.4733\n",
            "108/108 - 4s - loss: 1.0603 - val_loss: 1.4198\n",
            "108/108 - 4s - loss: 1.0651 - val_loss: 1.4650\n",
            "108/108 - 4s - loss: 1.0569 - val_loss: 1.4117\n",
            "108/108 - 4s - loss: 1.0538 - val_loss: 1.4421\n",
            "108/108 - 4s - loss: 1.0516 - val_loss: 1.5265\n",
            "108/108 - 4s - loss: 1.0399 - val_loss: 1.4855\n",
            "108/108 - 4s - loss: 1.0400 - val_loss: 1.4717\n",
            "108/108 - 4s - loss: 1.0395 - val_loss: 1.4839\n",
            "108/108 - 4s - loss: 1.0515 - val_loss: 1.4185\n",
            "108/108 - 4s - loss: 1.0549 - val_loss: 1.4454\n",
            "108/108 - 4s - loss: 1.0499 - val_loss: 1.3886\n",
            "108/108 - 4s - loss: 1.0284 - val_loss: 1.4666\n",
            "108/108 - 4s - loss: 1.0502 - val_loss: 1.4780\n",
            "108/108 - 4s - loss: 1.0512 - val_loss: 1.4943\n",
            "108/108 - 4s - loss: 1.0401 - val_loss: 1.4468\n",
            "108/108 - 4s - loss: 1.0369 - val_loss: 1.4811\n",
            "108/108 - 4s - loss: 1.0441 - val_loss: 1.4966\n",
            "108/108 - 4s - loss: 1.0348 - val_loss: 1.4949\n",
            "108/108 - 4s - loss: 1.0285 - val_loss: 1.4232\n",
            "108/108 - 4s - loss: 1.0343 - val_loss: 1.4576\n",
            "108/108 - 4s - loss: 1.0388 - val_loss: 1.4535\n",
            "108/108 - 4s - loss: 1.0389 - val_loss: 1.4505\n",
            "108/108 - 4s - loss: 1.0319 - val_loss: 1.4493\n",
            "108/108 - 4s - loss: 1.0181 - val_loss: 1.3975\n",
            "108/108 - 4s - loss: 1.0202 - val_loss: 1.4090\n",
            "108/108 - 4s - loss: 1.0275 - val_loss: 1.3696\n",
            "108/108 - 4s - loss: 1.0370 - val_loss: 1.4182\n",
            "108/108 - 4s - loss: 1.0657 - val_loss: 1.5179\n",
            "108/108 - 4s - loss: 1.0766 - val_loss: 1.4157\n",
            "108/108 - 4s - loss: 1.0842 - val_loss: 1.4735\n",
            "108/108 - 4s - loss: 1.1155 - val_loss: 1.4797\n",
            "108/108 - 4s - loss: 1.0985 - val_loss: 1.4450\n",
            "108/108 - 4s - loss: 1.0570 - val_loss: 1.4555\n",
            "108/108 - 4s - loss: 1.0587 - val_loss: 1.4930\n",
            "108/108 - 4s - loss: 1.0428 - val_loss: 1.4375\n",
            "108/108 - 4s - loss: 1.0447 - val_loss: 1.4363\n",
            "108/108 - 4s - loss: 1.0558 - val_loss: 1.4100\n",
            "108/108 - 4s - loss: 1.0494 - val_loss: 1.3956\n",
            "108/108 - 4s - loss: 1.0378 - val_loss: 1.3705\n",
            "108/108 - 4s - loss: 1.0331 - val_loss: 1.3920\n",
            "108/108 - 4s - loss: 1.0296 - val_loss: 1.3742\n",
            "108/108 - 4s - loss: 1.0323 - val_loss: 1.3934\n",
            "108/108 - 4s - loss: 1.0443 - val_loss: 1.4579\n",
            "108/108 - 4s - loss: 1.0675 - val_loss: 1.5006\n",
            "108/108 - 4s - loss: 1.0699 - val_loss: 1.4455\n",
            "108/108 - 4s - loss: 1.0474 - val_loss: 1.4419\n",
            "108/108 - 4s - loss: 1.0522 - val_loss: 1.4140\n",
            "108/108 - 4s - loss: 1.0655 - val_loss: 1.4002\n",
            "108/108 - 4s - loss: 1.0768 - val_loss: 1.4269\n",
            "108/108 - 4s - loss: 1.0667 - val_loss: 1.4397\n",
            "108/108 - 4s - loss: 1.0446 - val_loss: 1.4653\n",
            "108/108 - 4s - loss: 1.0281 - val_loss: 1.4283\n",
            "108/108 - 4s - loss: 1.0311 - val_loss: 1.4459\n",
            "108/108 - 4s - loss: 1.0753 - val_loss: 1.4238\n",
            "108/108 - 4s - loss: 1.0349 - val_loss: 1.5350\n",
            "108/108 - 4s - loss: 1.0251 - val_loss: 1.4477\n",
            "108/108 - 4s - loss: 1.0186 - val_loss: 1.4348\n",
            "108/108 - 4s - loss: 1.0141 - val_loss: 1.3829\n",
            "108/108 - 4s - loss: 1.0164 - val_loss: 1.4177\n",
            "108/108 - 4s - loss: 1.0248 - val_loss: 1.4430\n",
            "108/108 - 4s - loss: 1.0299 - val_loss: 1.3969\n",
            "108/108 - 4s - loss: 1.0239 - val_loss: 1.4554\n",
            "108/108 - 4s - loss: 1.0943 - val_loss: 1.3642\n",
            "108/108 - 4s - loss: 1.0286 - val_loss: 1.4492\n",
            "108/108 - 4s - loss: 1.0239 - val_loss: 1.3934\n",
            "108/108 - 4s - loss: 1.0124 - val_loss: 1.4044\n",
            "108/108 - 4s - loss: 1.0130 - val_loss: 1.4147\n",
            "108/108 - 4s - loss: 1.0172 - val_loss: 1.4421\n",
            "108/108 - 4s - loss: 1.0294 - val_loss: 1.4028\n",
            "108/108 - 4s - loss: 1.0352 - val_loss: 1.3443\n",
            "108/108 - 4s - loss: 1.0237 - val_loss: 1.4331\n",
            "108/108 - 4s - loss: 1.0226 - val_loss: 1.4364\n",
            "108/108 - 4s - loss: 1.0150 - val_loss: 1.4063\n",
            "108/108 - 4s - loss: 1.0072 - val_loss: 1.4202\n",
            "108/108 - 4s - loss: 1.0078 - val_loss: 1.4471\n",
            "108/108 - 4s - loss: 1.0296 - val_loss: 1.3755\n",
            "108/108 - 4s - loss: 1.0140 - val_loss: 1.4215\n",
            "108/108 - 4s - loss: 1.0134 - val_loss: 1.4187\n",
            "108/108 - 4s - loss: 1.0099 - val_loss: 1.3858\n",
            "108/108 - 4s - loss: 1.0232 - val_loss: 1.4629\n",
            "108/108 - 4s - loss: 1.0105 - val_loss: 1.4356\n",
            "108/108 - 4s - loss: 1.0087 - val_loss: 1.4599\n",
            "108/108 - 4s - loss: 1.0093 - val_loss: 1.5543\n",
            "108/108 - 4s - loss: 1.0158 - val_loss: 1.4598\n",
            "108/108 - 4s - loss: 0.9943 - val_loss: 1.4519\n",
            "108/108 - 4s - loss: 0.9964 - val_loss: 1.4336\n",
            "108/108 - 4s - loss: 1.0084 - val_loss: 1.4548\n",
            "108/108 - 4s - loss: 0.9980 - val_loss: 1.4896\n",
            "108/108 - 4s - loss: 1.0007 - val_loss: 1.4541\n",
            "108/108 - 4s - loss: 1.0092 - val_loss: 1.4637\n",
            "108/108 - 4s - loss: 1.0106 - val_loss: 1.4357\n",
            "108/108 - 4s - loss: 1.0194 - val_loss: 1.5249\n",
            "108/108 - 4s - loss: 1.0019 - val_loss: 1.5079\n",
            "108/108 - 4s - loss: 1.0167 - val_loss: 1.4413\n",
            "108/108 - 4s - loss: 0.9989 - val_loss: 1.4541\n",
            "108/108 - 4s - loss: 1.0035 - val_loss: 1.4331\n",
            "108/108 - 4s - loss: 1.0013 - val_loss: 1.5010\n",
            "108/108 - 4s - loss: 1.0018 - val_loss: 1.5200\n",
            "108/108 - 4s - loss: 1.0064 - val_loss: 1.5006\n",
            "108/108 - 4s - loss: 1.0132 - val_loss: 1.5705\n",
            "108/108 - 4s - loss: 1.0156 - val_loss: 1.4487\n",
            "108/108 - 4s - loss: 1.0097 - val_loss: 1.4905\n",
            "108/108 - 4s - loss: 1.0246 - val_loss: 1.4072\n",
            "108/108 - 4s - loss: 0.9926 - val_loss: 1.4433\n",
            "108/108 - 4s - loss: 1.0029 - val_loss: 1.4258\n",
            "108/108 - 4s - loss: 1.0037 - val_loss: 1.4126\n",
            "108/108 - 4s - loss: 1.0023 - val_loss: 1.4572\n",
            "108/108 - 4s - loss: 1.0207 - val_loss: 1.4593\n",
            "108/108 - 4s - loss: 1.0145 - val_loss: 1.4825\n",
            "108/108 - 4s - loss: 1.0001 - val_loss: 1.5237\n",
            "108/108 - 4s - loss: 1.0005 - val_loss: 1.4838\n",
            "108/108 - 4s - loss: 1.0103 - val_loss: 1.4929\n",
            "108/108 - 4s - loss: 1.0053 - val_loss: 1.4575\n",
            "108/108 - 4s - loss: 0.9966 - val_loss: 1.4159\n",
            "108/108 - 4s - loss: 0.9965 - val_loss: 1.4526\n",
            "108/108 - 4s - loss: 0.9918 - val_loss: 1.4982\n",
            "108/108 - 4s - loss: 0.9904 - val_loss: 1.4200\n",
            "108/108 - 4s - loss: 0.9883 - val_loss: 1.4329\n",
            "108/108 - 4s - loss: 0.9869 - val_loss: 1.4687\n",
            "108/108 - 4s - loss: 0.9924 - val_loss: 1.4579\n",
            "108/108 - 4s - loss: 0.9795 - val_loss: 1.4094\n",
            "108/108 - 4s - loss: 0.9958 - val_loss: 1.3805\n",
            "108/108 - 4s - loss: 1.0262 - val_loss: 1.4204\n",
            "108/108 - 4s - loss: 1.0390 - val_loss: 1.4271\n",
            "108/108 - 4s - loss: 1.0437 - val_loss: 1.4490\n",
            "108/108 - 4s - loss: 0.9893 - val_loss: 1.4348\n",
            "108/108 - 4s - loss: 0.9944 - val_loss: 1.4390\n",
            "108/108 - 4s - loss: 0.9985 - val_loss: 1.3950\n",
            "108/108 - 4s - loss: 0.9901 - val_loss: 1.4263\n",
            "108/108 - 4s - loss: 1.0035 - val_loss: 1.4443\n",
            "108/108 - 4s - loss: 1.0092 - val_loss: 1.4644\n",
            "108/108 - 4s - loss: 0.9984 - val_loss: 1.4114\n",
            "108/108 - 4s - loss: 1.0016 - val_loss: 1.5623\n",
            "108/108 - 4s - loss: 0.9986 - val_loss: 1.4884\n",
            "108/108 - 4s - loss: 1.0052 - val_loss: 1.5229\n",
            "108/108 - 4s - loss: 0.9943 - val_loss: 1.5163\n",
            "108/108 - 4s - loss: 0.9859 - val_loss: 1.4375\n",
            "108/108 - 4s - loss: 0.9858 - val_loss: 1.4402\n",
            "108/108 - 4s - loss: 0.9974 - val_loss: 1.4146\n",
            "108/108 - 4s - loss: 0.9796 - val_loss: 1.5114\n",
            "108/108 - 4s - loss: 0.9910 - val_loss: 1.4154\n",
            "108/108 - 4s - loss: 0.9797 - val_loss: 1.4945\n",
            "108/108 - 4s - loss: 0.9858 - val_loss: 1.4704\n",
            "108/108 - 4s - loss: 0.9827 - val_loss: 1.4967\n",
            "108/108 - 4s - loss: 0.9785 - val_loss: 1.4820\n",
            "108/108 - 4s - loss: 0.9814 - val_loss: 1.4917\n",
            "108/108 - 4s - loss: 1.0635 - val_loss: 1.5039\n",
            "108/108 - 4s - loss: 1.0095 - val_loss: 1.5090\n",
            "0.2\n",
            "108/108 - 7s - loss: 3.4354 - val_loss: 2.9111\n",
            "108/108 - 4s - loss: 3.0063 - val_loss: 2.6581\n",
            "108/108 - 4s - loss: 2.8186 - val_loss: 2.5417\n",
            "108/108 - 4s - loss: 2.7309 - val_loss: 2.4833\n",
            "108/108 - 4s - loss: 2.6793 - val_loss: 2.4536\n",
            "108/108 - 4s - loss: 2.6501 - val_loss: 2.4359\n",
            "108/108 - 4s - loss: 2.6288 - val_loss: 2.4249\n",
            "108/108 - 4s - loss: 2.6099 - val_loss: 2.4188\n",
            "108/108 - 4s - loss: 2.6185 - val_loss: 2.4166\n",
            "108/108 - 4s - loss: 2.6143 - val_loss: 2.4152\n",
            "108/108 - 4s - loss: 2.6108 - val_loss: 2.4149\n",
            "108/108 - 4s - loss: 2.5953 - val_loss: 2.4153\n",
            "108/108 - 4s - loss: 2.6025 - val_loss: 2.4151\n",
            "108/108 - 4s - loss: 2.6006 - val_loss: 2.4160\n",
            "108/108 - 4s - loss: 2.5967 - val_loss: 2.4153\n",
            "108/108 - 4s - loss: 2.6035 - val_loss: 2.4146\n",
            "108/108 - 4s - loss: 2.6036 - val_loss: 2.4145\n",
            "108/108 - 4s - loss: 2.6077 - val_loss: 2.4135\n",
            "108/108 - 4s - loss: 2.6094 - val_loss: 2.4118\n",
            "108/108 - 4s - loss: 2.5979 - val_loss: 2.4137\n",
            "108/108 - 4s - loss: 2.6073 - val_loss: 2.4072\n",
            "108/108 - 4s - loss: 2.5995 - val_loss: 2.3920\n",
            "108/108 - 4s - loss: 2.5809 - val_loss: 2.4051\n",
            "108/108 - 4s - loss: 2.5648 - val_loss: 2.3677\n",
            "108/108 - 4s - loss: 2.5470 - val_loss: 2.3594\n",
            "108/108 - 3s - loss: 2.5335 - val_loss: 2.3499\n",
            "108/108 - 4s - loss: 2.5102 - val_loss: 2.3450\n",
            "108/108 - 4s - loss: 2.5103 - val_loss: 2.3305\n",
            "108/108 - 4s - loss: 2.4960 - val_loss: 2.3364\n",
            "108/108 - 4s - loss: 2.4815 - val_loss: 2.3087\n",
            "108/108 - 4s - loss: 2.4561 - val_loss: 2.3099\n",
            "108/108 - 4s - loss: 2.4639 - val_loss: 2.3105\n",
            "108/108 - 4s - loss: 2.4620 - val_loss: 2.3092\n",
            "108/108 - 4s - loss: 2.4374 - val_loss: 2.2888\n",
            "108/108 - 4s - loss: 2.4231 - val_loss: 2.2842\n",
            "108/108 - 4s - loss: 2.4228 - val_loss: 2.2975\n",
            "108/108 - 4s - loss: 2.4122 - val_loss: 2.2959\n",
            "108/108 - 4s - loss: 2.4042 - val_loss: 2.2776\n",
            "108/108 - 4s - loss: 2.3985 - val_loss: 2.2779\n",
            "108/108 - 4s - loss: 2.3851 - val_loss: 2.2567\n",
            "108/108 - 4s - loss: 2.3863 - val_loss: 2.2572\n",
            "108/108 - 4s - loss: 2.3822 - val_loss: 2.2627\n",
            "108/108 - 4s - loss: 2.3530 - val_loss: 2.2604\n",
            "108/108 - 4s - loss: 2.3690 - val_loss: 2.2371\n",
            "108/108 - 4s - loss: 2.3444 - val_loss: 2.2439\n",
            "108/108 - 4s - loss: 2.3394 - val_loss: 2.2338\n",
            "108/108 - 4s - loss: 2.3303 - val_loss: 2.2306\n",
            "108/108 - 4s - loss: 2.3498 - val_loss: 2.2423\n",
            "108/108 - 4s - loss: 2.3143 - val_loss: 2.2298\n",
            "108/108 - 4s - loss: 2.3267 - val_loss: 2.2238\n",
            "108/108 - 4s - loss: 2.3203 - val_loss: 2.2244\n",
            "108/108 - 4s - loss: 2.2874 - val_loss: 2.2028\n",
            "108/108 - 4s - loss: 2.3058 - val_loss: 2.2236\n",
            "108/108 - 4s - loss: 2.2929 - val_loss: 2.2368\n",
            "108/108 - 4s - loss: 2.2688 - val_loss: 2.2183\n",
            "108/108 - 4s - loss: 2.2734 - val_loss: 2.2188\n",
            "108/108 - 4s - loss: 2.2931 - val_loss: 2.2261\n",
            "108/108 - 4s - loss: 2.2776 - val_loss: 2.2327\n",
            "108/108 - 4s - loss: 2.2628 - val_loss: 2.2141\n",
            "108/108 - 4s - loss: 2.2456 - val_loss: 2.2360\n",
            "108/108 - 4s - loss: 2.2449 - val_loss: 2.2135\n",
            "108/108 - 4s - loss: 2.2419 - val_loss: 2.2118\n",
            "108/108 - 4s - loss: 2.2392 - val_loss: 2.2422\n",
            "108/108 - 4s - loss: 2.2216 - val_loss: 2.2351\n",
            "108/108 - 4s - loss: 2.2223 - val_loss: 2.2264\n",
            "108/108 - 4s - loss: 2.2069 - val_loss: 2.2183\n",
            "108/108 - 4s - loss: 2.1997 - val_loss: 2.2453\n",
            "108/108 - 4s - loss: 2.2101 - val_loss: 2.2149\n",
            "108/108 - 4s - loss: 2.1919 - val_loss: 2.2252\n",
            "108/108 - 4s - loss: 2.1900 - val_loss: 2.2130\n",
            "108/108 - 4s - loss: 2.1815 - val_loss: 2.2292\n",
            "108/108 - 4s - loss: 2.1838 - val_loss: 2.2396\n",
            "108/108 - 4s - loss: 2.1671 - val_loss: 2.2321\n",
            "108/108 - 4s - loss: 2.1889 - val_loss: 2.2492\n",
            "108/108 - 4s - loss: 2.1724 - val_loss: 2.2437\n",
            "108/108 - 4s - loss: 2.1701 - val_loss: 2.2392\n",
            "108/108 - 4s - loss: 2.1546 - val_loss: 2.2380\n",
            "108/108 - 4s - loss: 2.1371 - val_loss: 2.2601\n",
            "108/108 - 4s - loss: 2.1554 - val_loss: 2.2454\n",
            "108/108 - 4s - loss: 2.1396 - val_loss: 2.2634\n",
            "108/108 - 4s - loss: 2.1321 - val_loss: 2.2574\n",
            "108/108 - 4s - loss: 2.1480 - val_loss: 2.2347\n",
            "108/108 - 4s - loss: 2.1275 - val_loss: 2.2374\n",
            "108/108 - 4s - loss: 2.1341 - val_loss: 2.2652\n",
            "108/108 - 4s - loss: 2.1250 - val_loss: 2.2305\n",
            "108/108 - 4s - loss: 2.1423 - val_loss: 2.2805\n",
            "108/108 - 4s - loss: 2.1331 - val_loss: 2.2310\n",
            "108/108 - 4s - loss: 2.1311 - val_loss: 2.2901\n",
            "108/108 - 4s - loss: 2.1044 - val_loss: 2.2294\n",
            "108/108 - 4s - loss: 2.1064 - val_loss: 2.2719\n",
            "108/108 - 4s - loss: 2.1155 - val_loss: 2.2810\n",
            "108/108 - 4s - loss: 2.0937 - val_loss: 2.2775\n",
            "108/108 - 4s - loss: 2.0891 - val_loss: 2.2773\n",
            "108/108 - 4s - loss: 2.0956 - val_loss: 2.2591\n",
            "108/108 - 4s - loss: 2.0996 - val_loss: 2.2550\n",
            "108/108 - 4s - loss: 2.0982 - val_loss: 2.2894\n",
            "108/108 - 4s - loss: 2.0691 - val_loss: 2.3008\n",
            "108/108 - 4s - loss: 2.0869 - val_loss: 2.3058\n",
            "108/108 - 4s - loss: 2.0670 - val_loss: 2.3199\n",
            "108/108 - 4s - loss: 2.0776 - val_loss: 2.3313\n",
            "108/108 - 4s - loss: 2.0522 - val_loss: 2.3454\n",
            "108/108 - 4s - loss: 2.0818 - val_loss: 2.3353\n",
            "108/108 - 4s - loss: 2.0742 - val_loss: 2.2976\n",
            "108/108 - 4s - loss: 2.0653 - val_loss: 2.3139\n",
            "108/108 - 4s - loss: 2.0791 - val_loss: 2.3008\n",
            "108/108 - 4s - loss: 2.0574 - val_loss: 2.2670\n",
            "108/108 - 4s - loss: 2.0507 - val_loss: 2.3255\n",
            "108/108 - 4s - loss: 2.0672 - val_loss: 2.2797\n",
            "108/108 - 4s - loss: 2.0595 - val_loss: 2.3204\n",
            "108/108 - 4s - loss: 2.0659 - val_loss: 2.2911\n",
            "108/108 - 4s - loss: 2.0851 - val_loss: 2.3327\n",
            "108/108 - 4s - loss: 2.0472 - val_loss: 2.3324\n",
            "108/108 - 4s - loss: 2.0310 - val_loss: 2.3155\n",
            "108/108 - 4s - loss: 2.0482 - val_loss: 2.2852\n",
            "108/108 - 4s - loss: 2.0433 - val_loss: 2.3161\n",
            "108/108 - 4s - loss: 2.0584 - val_loss: 2.2885\n",
            "108/108 - 4s - loss: 2.0568 - val_loss: 2.2717\n",
            "108/108 - 4s - loss: 2.0591 - val_loss: 2.3112\n",
            "108/108 - 4s - loss: 2.0767 - val_loss: 2.2872\n",
            "108/108 - 4s - loss: 2.0480 - val_loss: 2.2902\n",
            "108/108 - 4s - loss: 2.0314 - val_loss: 2.3054\n",
            "108/108 - 4s - loss: 2.0569 - val_loss: 2.3088\n",
            "108/108 - 4s - loss: 2.0431 - val_loss: 2.2721\n",
            "108/108 - 4s - loss: 2.0464 - val_loss: 2.2952\n",
            "108/108 - 4s - loss: 2.0364 - val_loss: 2.3141\n",
            "108/108 - 4s - loss: 2.0311 - val_loss: 2.3247\n",
            "108/108 - 4s - loss: 2.0437 - val_loss: 2.3157\n",
            "108/108 - 4s - loss: 2.0269 - val_loss: 2.2730\n",
            "108/108 - 4s - loss: 2.0708 - val_loss: 2.3098\n",
            "108/108 - 4s - loss: 2.0285 - val_loss: 2.3401\n",
            "108/108 - 4s - loss: 2.0320 - val_loss: 2.3740\n",
            "108/108 - 4s - loss: 2.0039 - val_loss: 2.2983\n",
            "108/108 - 4s - loss: 2.0028 - val_loss: 2.3563\n",
            "108/108 - 4s - loss: 2.0159 - val_loss: 2.3612\n",
            "108/108 - 4s - loss: 1.9904 - val_loss: 2.3512\n",
            "108/108 - 4s - loss: 2.0079 - val_loss: 2.2969\n",
            "108/108 - 4s - loss: 2.0062 - val_loss: 2.3636\n",
            "108/108 - 4s - loss: 2.0026 - val_loss: 2.3223\n",
            "108/108 - 4s - loss: 1.9930 - val_loss: 2.2985\n",
            "108/108 - 4s - loss: 1.9933 - val_loss: 2.3444\n",
            "108/108 - 4s - loss: 1.9795 - val_loss: 2.3816\n",
            "108/108 - 4s - loss: 2.0021 - val_loss: 2.3684\n",
            "108/108 - 4s - loss: 1.9847 - val_loss: 2.3921\n",
            "108/108 - 4s - loss: 1.9980 - val_loss: 2.3566\n",
            "108/108 - 4s - loss: 2.0008 - val_loss: 2.3500\n",
            "108/108 - 4s - loss: 1.9851 - val_loss: 2.3494\n",
            "108/108 - 4s - loss: 1.9675 - val_loss: 2.3762\n",
            "108/108 - 4s - loss: 1.9606 - val_loss: 2.3940\n",
            "108/108 - 4s - loss: 1.9805 - val_loss: 2.3860\n",
            "108/108 - 4s - loss: 1.9783 - val_loss: 2.3912\n",
            "108/108 - 4s - loss: 1.9642 - val_loss: 2.3240\n",
            "108/108 - 4s - loss: 1.9572 - val_loss: 2.3520\n",
            "108/108 - 4s - loss: 2.0194 - val_loss: 2.2993\n",
            "108/108 - 4s - loss: 1.9620 - val_loss: 2.3753\n",
            "108/108 - 4s - loss: 1.9277 - val_loss: 2.3475\n",
            "108/108 - 4s - loss: 1.9437 - val_loss: 2.3213\n",
            "108/108 - 4s - loss: 1.9678 - val_loss: 2.3653\n",
            "108/108 - 4s - loss: 1.9612 - val_loss: 2.3660\n",
            "108/108 - 4s - loss: 2.0112 - val_loss: 2.3490\n",
            "108/108 - 4s - loss: 2.0004 - val_loss: 2.3281\n",
            "108/108 - 4s - loss: 2.0475 - val_loss: 2.2978\n",
            "108/108 - 4s - loss: 1.9955 - val_loss: 2.3269\n",
            "108/108 - 4s - loss: 1.9929 - val_loss: 2.3204\n",
            "108/108 - 4s - loss: 1.9792 - val_loss: 2.3565\n",
            "108/108 - 4s - loss: 1.9565 - val_loss: 2.3069\n",
            "108/108 - 4s - loss: 1.9610 - val_loss: 2.3456\n",
            "108/108 - 4s - loss: 1.9366 - val_loss: 2.3365\n",
            "108/108 - 4s - loss: 1.9354 - val_loss: 2.3080\n",
            "108/108 - 4s - loss: 1.9638 - val_loss: 2.3180\n",
            "108/108 - 4s - loss: 1.9927 - val_loss: 2.3315\n",
            "108/108 - 4s - loss: 1.9991 - val_loss: 2.2579\n",
            "108/108 - 4s - loss: 2.0595 - val_loss: 2.2717\n",
            "108/108 - 4s - loss: 1.9853 - val_loss: 2.2944\n",
            "108/108 - 4s - loss: 1.9702 - val_loss: 2.3096\n",
            "108/108 - 4s - loss: 1.9895 - val_loss: 2.3574\n",
            "108/108 - 4s - loss: 1.9657 - val_loss: 2.3264\n",
            "108/108 - 4s - loss: 1.9736 - val_loss: 2.3696\n",
            "108/108 - 4s - loss: 1.9386 - val_loss: 2.3586\n",
            "108/108 - 4s - loss: 1.9537 - val_loss: 2.3607\n",
            "108/108 - 4s - loss: 2.0115 - val_loss: 2.4368\n",
            "108/108 - 4s - loss: 2.0076 - val_loss: 2.3598\n",
            "108/108 - 4s - loss: 1.9675 - val_loss: 2.3822\n",
            "108/108 - 4s - loss: 1.9439 - val_loss: 2.4082\n",
            "108/108 - 4s - loss: 1.9619 - val_loss: 2.3330\n",
            "108/108 - 4s - loss: 1.9507 - val_loss: 2.3441\n",
            "108/108 - 4s - loss: 1.9382 - val_loss: 2.3726\n",
            "108/108 - 4s - loss: 1.9238 - val_loss: 2.3666\n",
            "108/108 - 4s - loss: 1.9623 - val_loss: 2.3535\n",
            "108/108 - 4s - loss: 1.9325 - val_loss: 2.3628\n",
            "108/108 - 4s - loss: 1.9804 - val_loss: 2.3243\n",
            "108/108 - 4s - loss: 1.9320 - val_loss: 2.3887\n",
            "108/108 - 4s - loss: 1.9300 - val_loss: 2.3185\n",
            "108/108 - 4s - loss: 1.9567 - val_loss: 2.3336\n",
            "108/108 - 4s - loss: 1.9465 - val_loss: 2.3055\n",
            "108/108 - 4s - loss: 1.9148 - val_loss: 2.3283\n",
            "108/108 - 4s - loss: 1.9298 - val_loss: 2.3236\n",
            "108/108 - 4s - loss: 1.9498 - val_loss: 2.3294\n",
            "108/108 - 4s - loss: 1.9410 - val_loss: 2.3441\n",
            "108/108 - 4s - loss: 1.9509 - val_loss: 2.3518\n",
            "108/108 - 4s - loss: 1.9125 - val_loss: 2.3278\n",
            "108/108 - 4s - loss: 1.9347 - val_loss: 2.2845\n",
            "108/108 - 4s - loss: 1.9088 - val_loss: 2.3014\n",
            "108/108 - 4s - loss: 1.9182 - val_loss: 2.2776\n",
            "108/108 - 4s - loss: 1.8843 - val_loss: 2.2425\n",
            "108/108 - 4s - loss: 1.8717 - val_loss: 2.3151\n",
            "108/108 - 4s - loss: 1.8961 - val_loss: 2.2686\n",
            "108/108 - 4s - loss: 1.8914 - val_loss: 2.2979\n",
            "108/108 - 4s - loss: 1.8951 - val_loss: 2.3131\n",
            "108/108 - 4s - loss: 1.8884 - val_loss: 2.3324\n",
            "108/108 - 4s - loss: 1.9111 - val_loss: 2.2858\n",
            "108/108 - 4s - loss: 1.9199 - val_loss: 2.2758\n",
            "108/108 - 4s - loss: 1.8969 - val_loss: 2.3231\n",
            "108/108 - 4s - loss: 1.9081 - val_loss: 2.2784\n",
            "108/108 - 4s - loss: 1.9307 - val_loss: 2.2746\n",
            "108/108 - 4s - loss: 1.8806 - val_loss: 2.3320\n",
            "108/108 - 4s - loss: 1.8974 - val_loss: 2.2798\n",
            "108/108 - 4s - loss: 1.8875 - val_loss: 2.3918\n",
            "108/108 - 4s - loss: 1.8847 - val_loss: 2.3232\n",
            "108/108 - 4s - loss: 1.8925 - val_loss: 2.3656\n",
            "108/108 - 4s - loss: 1.9365 - val_loss: 2.3211\n",
            "108/108 - 4s - loss: 1.8964 - val_loss: 2.4619\n",
            "108/108 - 4s - loss: 1.9127 - val_loss: 2.3777\n",
            "108/108 - 4s - loss: 1.8977 - val_loss: 2.3614\n",
            "108/108 - 4s - loss: 1.8922 - val_loss: 2.3433\n",
            "108/108 - 4s - loss: 1.8850 - val_loss: 2.3906\n",
            "108/108 - 4s - loss: 1.8819 - val_loss: 2.3933\n",
            "108/108 - 4s - loss: 1.9113 - val_loss: 2.4016\n",
            "108/108 - 4s - loss: 1.8737 - val_loss: 2.3577\n",
            "108/108 - 4s - loss: 1.9049 - val_loss: 2.3217\n",
            "108/108 - 4s - loss: 1.8933 - val_loss: 2.3766\n",
            "108/108 - 4s - loss: 1.8876 - val_loss: 2.3719\n",
            "108/108 - 4s - loss: 1.8804 - val_loss: 2.4232\n",
            "108/108 - 4s - loss: 2.0149 - val_loss: 2.3289\n",
            "108/108 - 4s - loss: 1.9564 - val_loss: 2.4112\n",
            "108/108 - 4s - loss: 1.9173 - val_loss: 2.4856\n",
            "108/108 - 4s - loss: 1.8740 - val_loss: 2.4044\n",
            "108/108 - 4s - loss: 1.9084 - val_loss: 2.3943\n",
            "108/108 - 4s - loss: 1.8828 - val_loss: 2.3411\n",
            "108/108 - 4s - loss: 1.8762 - val_loss: 2.4118\n",
            "108/108 - 4s - loss: 1.8472 - val_loss: 2.3279\n",
            "108/108 - 4s - loss: 1.8675 - val_loss: 2.3825\n",
            "108/108 - 4s - loss: 1.8665 - val_loss: 2.3756\n",
            "108/108 - 4s - loss: 1.8911 - val_loss: 2.4609\n",
            "108/108 - 4s - loss: 1.8634 - val_loss: 2.4119\n",
            "108/108 - 4s - loss: 1.8619 - val_loss: 2.4033\n",
            "108/108 - 4s - loss: 1.8585 - val_loss: 2.3375\n",
            "108/108 - 4s - loss: 1.8615 - val_loss: 2.3998\n",
            "108/108 - 4s - loss: 1.8855 - val_loss: 2.3690\n",
            "108/108 - 4s - loss: 1.8781 - val_loss: 2.3747\n",
            "108/108 - 4s - loss: 1.8507 - val_loss: 2.2797\n",
            "108/108 - 4s - loss: 1.8637 - val_loss: 2.2865\n",
            "108/108 - 4s - loss: 1.8384 - val_loss: 2.3269\n",
            "108/108 - 4s - loss: 1.8455 - val_loss: 2.3076\n",
            "108/108 - 4s - loss: 1.8381 - val_loss: 2.3270\n",
            "108/108 - 4s - loss: 1.8531 - val_loss: 2.3969\n",
            "108/108 - 4s - loss: 1.8505 - val_loss: 2.3175\n",
            "108/108 - 4s - loss: 1.8519 - val_loss: 2.3014\n",
            "108/108 - 4s - loss: 1.8486 - val_loss: 2.4059\n",
            "108/108 - 4s - loss: 1.8404 - val_loss: 2.3421\n",
            "108/108 - 4s - loss: 1.8509 - val_loss: 2.3074\n",
            "108/108 - 4s - loss: 1.8394 - val_loss: 2.3063\n",
            "108/108 - 4s - loss: 1.8278 - val_loss: 2.2996\n",
            "108/108 - 4s - loss: 1.8293 - val_loss: 2.3645\n",
            "108/108 - 4s - loss: 1.8592 - val_loss: 2.4151\n",
            "108/108 - 4s - loss: 1.8354 - val_loss: 2.3068\n",
            "108/108 - 4s - loss: 1.8467 - val_loss: 2.3671\n",
            "108/108 - 4s - loss: 1.8255 - val_loss: 2.3243\n",
            "108/108 - 4s - loss: 1.8299 - val_loss: 2.3857\n",
            "108/108 - 4s - loss: 1.8367 - val_loss: 2.4636\n",
            "108/108 - 4s - loss: 1.8126 - val_loss: 2.3168\n",
            "108/108 - 4s - loss: 1.8244 - val_loss: 2.4901\n",
            "108/108 - 4s - loss: 1.8327 - val_loss: 2.3030\n",
            "108/108 - 4s - loss: 1.8306 - val_loss: 2.3990\n",
            "108/108 - 4s - loss: 1.8480 - val_loss: 2.3748\n",
            "108/108 - 4s - loss: 1.8168 - val_loss: 2.2638\n",
            "108/108 - 4s - loss: 1.8122 - val_loss: 2.3479\n",
            "108/108 - 4s - loss: 1.7869 - val_loss: 2.3414\n",
            "108/108 - 4s - loss: 1.8036 - val_loss: 2.2733\n",
            "108/108 - 4s - loss: 1.8325 - val_loss: 2.3710\n",
            "108/108 - 4s - loss: 1.8134 - val_loss: 2.4141\n",
            "108/108 - 4s - loss: 1.8230 - val_loss: 2.3182\n",
            "108/108 - 4s - loss: 1.8236 - val_loss: 2.3687\n",
            "108/108 - 4s - loss: 1.8495 - val_loss: 2.3448\n",
            "108/108 - 4s - loss: 1.8186 - val_loss: 2.2914\n",
            "108/108 - 4s - loss: 1.7986 - val_loss: 2.3759\n",
            "108/108 - 4s - loss: 1.8311 - val_loss: 2.3658\n",
            "108/108 - 4s - loss: 1.8325 - val_loss: 2.2553\n",
            "108/108 - 4s - loss: 1.8375 - val_loss: 2.3936\n",
            "108/108 - 4s - loss: 1.8252 - val_loss: 2.2878\n",
            "108/108 - 4s - loss: 1.8141 - val_loss: 2.3698\n",
            "108/108 - 4s - loss: 1.7983 - val_loss: 2.3393\n",
            "108/108 - 4s - loss: 1.7838 - val_loss: 2.3026\n",
            "108/108 - 4s - loss: 1.7776 - val_loss: 2.3745\n",
            "108/108 - 4s - loss: 1.7732 - val_loss: 2.3947\n",
            "108/108 - 4s - loss: 1.7812 - val_loss: 2.3736\n",
            "108/108 - 4s - loss: 1.7841 - val_loss: 2.3612\n",
            "108/108 - 4s - loss: 1.7828 - val_loss: 2.4301\n",
            "108/108 - 4s - loss: 1.7696 - val_loss: 2.3687\n",
            "108/108 - 4s - loss: 1.7842 - val_loss: 2.4080\n",
            "108/108 - 4s - loss: 1.7811 - val_loss: 2.3700\n",
            "108/108 - 4s - loss: 1.7934 - val_loss: 2.3311\n",
            "108/108 - 4s - loss: 1.7924 - val_loss: 2.4123\n",
            "108/108 - 4s - loss: 1.7748 - val_loss: 2.3995\n",
            "108/108 - 4s - loss: 1.7761 - val_loss: 2.4758\n",
            "108/108 - 4s - loss: 1.7718 - val_loss: 2.4091\n",
            "108/108 - 4s - loss: 1.7654 - val_loss: 2.3926\n",
            "108/108 - 4s - loss: 1.7775 - val_loss: 2.3913\n",
            "108/108 - 4s - loss: 1.7846 - val_loss: 2.4402\n",
            "108/108 - 4s - loss: 1.7801 - val_loss: 2.3306\n",
            "108/108 - 4s - loss: 1.7988 - val_loss: 2.3033\n",
            "108/108 - 4s - loss: 1.7802 - val_loss: 2.3747\n",
            "108/108 - 4s - loss: 1.7581 - val_loss: 2.3502\n",
            "108/108 - 4s - loss: 1.7730 - val_loss: 2.3322\n",
            "108/108 - 4s - loss: 1.7513 - val_loss: 2.3112\n",
            "108/108 - 4s - loss: 1.7697 - val_loss: 2.4047\n",
            "108/108 - 4s - loss: 1.7781 - val_loss: 2.3160\n",
            "108/108 - 4s - loss: 1.7824 - val_loss: 2.2877\n",
            "108/108 - 4s - loss: 1.7837 - val_loss: 2.3380\n",
            "108/108 - 4s - loss: 1.7774 - val_loss: 2.3719\n",
            "108/108 - 4s - loss: 1.7563 - val_loss: 2.4085\n",
            "108/108 - 4s - loss: 1.7719 - val_loss: 2.3857\n",
            "108/108 - 4s - loss: 1.7598 - val_loss: 2.2729\n",
            "108/108 - 4s - loss: 1.7418 - val_loss: 2.3516\n",
            "108/108 - 4s - loss: 1.7444 - val_loss: 2.3527\n",
            "108/108 - 4s - loss: 1.7576 - val_loss: 2.4142\n",
            "108/108 - 4s - loss: 1.7458 - val_loss: 2.3566\n",
            "108/108 - 4s - loss: 1.7340 - val_loss: 2.3285\n",
            "108/108 - 4s - loss: 1.7371 - val_loss: 2.3762\n",
            "108/108 - 4s - loss: 1.7494 - val_loss: 2.2929\n",
            "108/108 - 4s - loss: 1.7382 - val_loss: 2.3050\n",
            "108/108 - 4s - loss: 1.7664 - val_loss: 2.3004\n",
            "108/108 - 4s - loss: 1.7936 - val_loss: 2.3303\n",
            "108/108 - 4s - loss: 1.7877 - val_loss: 2.4017\n",
            "108/108 - 4s - loss: 1.7851 - val_loss: 2.3210\n",
            "108/108 - 4s - loss: 1.7857 - val_loss: 2.2675\n",
            "108/108 - 4s - loss: 1.7642 - val_loss: 2.2096\n",
            "108/108 - 4s - loss: 1.7551 - val_loss: 2.2893\n",
            "108/108 - 4s - loss: 1.7468 - val_loss: 2.3153\n",
            "108/108 - 4s - loss: 1.7274 - val_loss: 2.3130\n",
            "108/108 - 4s - loss: 1.7256 - val_loss: 2.3704\n",
            "108/108 - 4s - loss: 1.7175 - val_loss: 2.3824\n",
            "108/108 - 4s - loss: 1.7333 - val_loss: 2.3504\n",
            "108/108 - 4s - loss: 1.7270 - val_loss: 2.3523\n",
            "108/108 - 4s - loss: 1.7219 - val_loss: 2.4044\n",
            "108/108 - 4s - loss: 1.7251 - val_loss: 2.3835\n",
            "108/108 - 4s - loss: 1.7125 - val_loss: 2.4047\n",
            "108/108 - 4s - loss: 1.7377 - val_loss: 2.3171\n",
            "108/108 - 4s - loss: 1.7223 - val_loss: 2.3879\n",
            "108/108 - 4s - loss: 1.7232 - val_loss: 2.3572\n",
            "108/108 - 4s - loss: 1.7333 - val_loss: 2.2968\n",
            "108/108 - 4s - loss: 1.7099 - val_loss: 2.3185\n",
            "108/108 - 4s - loss: 1.7255 - val_loss: 2.3054\n",
            "108/108 - 4s - loss: 1.7404 - val_loss: 2.3698\n",
            "108/108 - 4s - loss: 1.7314 - val_loss: 2.3336\n",
            "108/108 - 4s - loss: 1.7876 - val_loss: 2.3595\n",
            "108/108 - 4s - loss: 1.7636 - val_loss: 2.3310\n",
            "108/108 - 4s - loss: 1.7480 - val_loss: 2.3737\n",
            "108/108 - 4s - loss: 1.7481 - val_loss: 2.3088\n",
            "108/108 - 4s - loss: 1.7396 - val_loss: 2.4127\n",
            "108/108 - 4s - loss: 1.7107 - val_loss: 2.4317\n",
            "108/108 - 4s - loss: 1.7067 - val_loss: 2.3780\n",
            "108/108 - 4s - loss: 1.7029 - val_loss: 2.3545\n",
            "108/108 - 4s - loss: 1.7112 - val_loss: 2.3094\n",
            "108/108 - 4s - loss: 1.7061 - val_loss: 2.3321\n",
            "108/108 - 4s - loss: 1.7140 - val_loss: 2.4056\n",
            "108/108 - 4s - loss: 1.7242 - val_loss: 2.3694\n",
            "108/108 - 4s - loss: 1.7359 - val_loss: 2.4047\n",
            "108/108 - 4s - loss: 1.7310 - val_loss: 2.3610\n",
            "108/108 - 4s - loss: 1.7354 - val_loss: 2.2935\n",
            "108/108 - 4s - loss: 1.7273 - val_loss: 2.3038\n",
            "108/108 - 4s - loss: 1.7284 - val_loss: 2.3674\n",
            "108/108 - 4s - loss: 1.7285 - val_loss: 2.3115\n",
            "108/108 - 4s - loss: 1.6980 - val_loss: 2.3576\n",
            "108/108 - 4s - loss: 1.6775 - val_loss: 2.4547\n",
            "108/108 - 4s - loss: 1.6975 - val_loss: 2.3567\n",
            "108/108 - 4s - loss: 1.7177 - val_loss: 2.3126\n",
            "108/108 - 4s - loss: 1.7046 - val_loss: 2.3633\n",
            "108/108 - 4s - loss: 1.6859 - val_loss: 2.3388\n",
            "108/108 - 4s - loss: 1.7012 - val_loss: 2.3267\n",
            "108/108 - 4s - loss: 1.6949 - val_loss: 2.4407\n",
            "108/108 - 4s - loss: 1.6996 - val_loss: 2.3397\n",
            "108/108 - 4s - loss: 1.7089 - val_loss: 2.3905\n",
            "108/108 - 4s - loss: 1.7321 - val_loss: 2.2519\n",
            "108/108 - 4s - loss: 1.7224 - val_loss: 2.3226\n",
            "108/108 - 4s - loss: 1.7042 - val_loss: 2.3495\n",
            "108/108 - 4s - loss: 1.7142 - val_loss: 2.3479\n",
            "108/108 - 4s - loss: 1.6975 - val_loss: 2.4666\n",
            "108/108 - 4s - loss: 1.7013 - val_loss: 2.3046\n",
            "108/108 - 4s - loss: 1.7345 - val_loss: 2.3308\n",
            "108/108 - 4s - loss: 1.7264 - val_loss: 2.3292\n",
            "108/108 - 4s - loss: 1.7134 - val_loss: 2.2879\n",
            "108/108 - 4s - loss: 1.6868 - val_loss: 2.3691\n",
            "108/108 - 4s - loss: 1.6966 - val_loss: 2.3138\n",
            "108/108 - 4s - loss: 1.7060 - val_loss: 2.3244\n",
            "108/108 - 4s - loss: 1.6959 - val_loss: 2.3999\n",
            "108/108 - 4s - loss: 1.7083 - val_loss: 2.4199\n",
            "108/108 - 4s - loss: 1.6773 - val_loss: 2.4195\n",
            "108/108 - 4s - loss: 1.6853 - val_loss: 2.4182\n",
            "108/108 - 4s - loss: 1.6867 - val_loss: 2.3781\n",
            "108/108 - 4s - loss: 1.6901 - val_loss: 2.3871\n",
            "108/108 - 4s - loss: 1.6759 - val_loss: 2.3775\n",
            "108/108 - 4s - loss: 1.6751 - val_loss: 2.4351\n",
            "108/108 - 4s - loss: 1.7032 - val_loss: 2.4078\n",
            "108/108 - 4s - loss: 1.6920 - val_loss: 2.3623\n",
            "108/108 - 4s - loss: 1.6810 - val_loss: 2.3814\n",
            "108/108 - 4s - loss: 1.6955 - val_loss: 2.4114\n",
            "108/108 - 4s - loss: 1.6954 - val_loss: 2.3950\n",
            "108/108 - 4s - loss: 1.6781 - val_loss: 2.3772\n",
            "108/108 - 4s - loss: 1.6633 - val_loss: 2.3553\n",
            "108/108 - 4s - loss: 1.6739 - val_loss: 2.4224\n",
            "108/108 - 4s - loss: 1.6610 - val_loss: 2.3300\n",
            "108/108 - 4s - loss: 1.7043 - val_loss: 2.4322\n",
            "108/108 - 4s - loss: 1.7040 - val_loss: 2.3513\n",
            "108/108 - 4s - loss: 1.6894 - val_loss: 2.2983\n",
            "108/108 - 4s - loss: 1.6685 - val_loss: 2.3177\n",
            "108/108 - 4s - loss: 1.6685 - val_loss: 2.4017\n",
            "108/108 - 4s - loss: 1.6734 - val_loss: 2.3983\n",
            "108/108 - 4s - loss: 1.6742 - val_loss: 2.4424\n",
            "108/108 - 4s - loss: 1.6792 - val_loss: 2.3925\n",
            "108/108 - 4s - loss: 1.6732 - val_loss: 2.4412\n",
            "108/108 - 4s - loss: 1.6805 - val_loss: 2.3193\n",
            "108/108 - 4s - loss: 1.6812 - val_loss: 2.3118\n",
            "108/108 - 4s - loss: 1.6547 - val_loss: 2.3739\n",
            "108/108 - 4s - loss: 1.6633 - val_loss: 2.3603\n",
            "108/108 - 4s - loss: 1.6576 - val_loss: 2.3568\n",
            "108/108 - 4s - loss: 1.6686 - val_loss: 2.4138\n",
            "108/108 - 4s - loss: 1.6531 - val_loss: 2.3507\n",
            "108/108 - 4s - loss: 1.6376 - val_loss: 2.3568\n",
            "108/108 - 4s - loss: 1.6744 - val_loss: 2.4037\n",
            "108/108 - 4s - loss: 1.6586 - val_loss: 2.3352\n",
            "108/108 - 4s - loss: 1.6578 - val_loss: 2.3583\n",
            "108/108 - 4s - loss: 1.6538 - val_loss: 2.4078\n",
            "108/108 - 4s - loss: 1.6710 - val_loss: 2.3076\n",
            "108/108 - 4s - loss: 1.6764 - val_loss: 2.3013\n",
            "108/108 - 4s - loss: 1.6646 - val_loss: 2.3876\n",
            "108/108 - 4s - loss: 1.6663 - val_loss: 2.3924\n",
            "108/108 - 4s - loss: 1.6588 - val_loss: 2.4413\n",
            "108/108 - 4s - loss: 1.6437 - val_loss: 2.3841\n",
            "108/108 - 4s - loss: 1.6442 - val_loss: 2.3625\n",
            "108/108 - 4s - loss: 1.6535 - val_loss: 2.3792\n",
            "108/108 - 4s - loss: 1.6622 - val_loss: 2.4280\n",
            "108/108 - 4s - loss: 1.6461 - val_loss: 2.3659\n",
            "108/108 - 4s - loss: 1.6325 - val_loss: 2.3515\n",
            "108/108 - 4s - loss: 1.6428 - val_loss: 2.3898\n",
            "108/108 - 4s - loss: 1.6303 - val_loss: 2.3398\n",
            "108/108 - 4s - loss: 1.6335 - val_loss: 2.3393\n",
            "108/108 - 4s - loss: 1.6240 - val_loss: 2.3500\n",
            "108/108 - 4s - loss: 1.6467 - val_loss: 2.3617\n",
            "108/108 - 4s - loss: 1.6865 - val_loss: 2.3413\n",
            "108/108 - 4s - loss: 1.6616 - val_loss: 2.2534\n",
            "108/108 - 4s - loss: 1.6313 - val_loss: 2.2986\n",
            "108/108 - 4s - loss: 1.6365 - val_loss: 2.3604\n",
            "108/108 - 4s - loss: 1.6282 - val_loss: 2.3285\n",
            "108/108 - 4s - loss: 1.6260 - val_loss: 2.3354\n",
            "108/108 - 4s - loss: 1.6175 - val_loss: 2.4643\n",
            "108/108 - 4s - loss: 1.6417 - val_loss: 2.3395\n",
            "108/108 - 4s - loss: 1.6429 - val_loss: 2.3560\n",
            "108/108 - 4s - loss: 1.6208 - val_loss: 2.4152\n",
            "108/108 - 4s - loss: 1.6164 - val_loss: 2.4319\n",
            "108/108 - 4s - loss: 1.6090 - val_loss: 2.3525\n",
            "108/108 - 4s - loss: 1.6347 - val_loss: 2.4112\n",
            "108/108 - 4s - loss: 1.6902 - val_loss: 2.3184\n",
            "108/108 - 4s - loss: 1.6233 - val_loss: 2.3206\n",
            "108/108 - 4s - loss: 1.6207 - val_loss: 2.4045\n",
            "108/108 - 4s - loss: 1.6189 - val_loss: 2.3618\n",
            "108/108 - 4s - loss: 1.6208 - val_loss: 2.3149\n",
            "108/108 - 4s - loss: 1.6049 - val_loss: 2.3331\n",
            "108/108 - 4s - loss: 1.6124 - val_loss: 2.3377\n",
            "108/108 - 4s - loss: 1.6077 - val_loss: 2.3027\n",
            "108/108 - 4s - loss: 1.6034 - val_loss: 2.3386\n",
            "108/108 - 4s - loss: 1.6062 - val_loss: 2.2961\n",
            "108/108 - 4s - loss: 1.6014 - val_loss: 2.3431\n",
            "108/108 - 4s - loss: 1.6048 - val_loss: 2.3355\n",
            "108/108 - 4s - loss: 1.6084 - val_loss: 2.3454\n",
            "108/108 - 4s - loss: 1.6064 - val_loss: 2.3408\n",
            "108/108 - 4s - loss: 1.5948 - val_loss: 2.3366\n",
            "108/108 - 4s - loss: 1.6001 - val_loss: 2.3286\n",
            "108/108 - 4s - loss: 1.6118 - val_loss: 2.3868\n",
            "108/108 - 4s - loss: 1.6159 - val_loss: 2.3474\n",
            "108/108 - 4s - loss: 1.6186 - val_loss: 2.3001\n",
            "108/108 - 4s - loss: 1.5977 - val_loss: 2.3485\n",
            "108/108 - 4s - loss: 1.6161 - val_loss: 2.3659\n",
            "108/108 - 4s - loss: 1.6333 - val_loss: 2.3235\n",
            "108/108 - 4s - loss: 1.6220 - val_loss: 2.3693\n",
            "108/108 - 4s - loss: 1.6122 - val_loss: 2.4306\n",
            "108/108 - 4s - loss: 1.5905 - val_loss: 2.4621\n",
            "108/108 - 4s - loss: 1.5829 - val_loss: 2.4062\n",
            "108/108 - 4s - loss: 1.5921 - val_loss: 2.4438\n",
            "108/108 - 4s - loss: 1.5827 - val_loss: 2.3782\n",
            "108/108 - 4s - loss: 1.5766 - val_loss: 2.2581\n",
            "108/108 - 4s - loss: 1.5744 - val_loss: 2.3164\n",
            "108/108 - 4s - loss: 1.5585 - val_loss: 2.3824\n",
            "108/108 - 4s - loss: 1.5797 - val_loss: 2.3235\n",
            "108/108 - 4s - loss: 1.5682 - val_loss: 2.3382\n",
            "108/108 - 4s - loss: 1.5804 - val_loss: 2.3292\n",
            "108/108 - 4s - loss: 1.5815 - val_loss: 2.3984\n",
            "108/108 - 3s - loss: 1.5870 - val_loss: 2.3438\n",
            "108/108 - 4s - loss: 1.5920 - val_loss: 2.3410\n",
            "108/108 - 4s - loss: 1.5849 - val_loss: 2.3075\n",
            "108/108 - 4s - loss: 1.6287 - val_loss: 2.3320\n",
            "0.30000000000000004\n",
            "108/108 - 7s - loss: 5.0926 - val_loss: 4.2701\n",
            "108/108 - 4s - loss: 4.3669 - val_loss: 3.7915\n",
            "108/108 - 4s - loss: 3.9797 - val_loss: 3.5149\n",
            "108/108 - 4s - loss: 3.7536 - val_loss: 3.3468\n",
            "108/108 - 4s - loss: 3.5917 - val_loss: 3.2429\n",
            "108/108 - 4s - loss: 3.4988 - val_loss: 3.1826\n",
            "108/108 - 4s - loss: 3.4506 - val_loss: 3.1527\n",
            "108/108 - 4s - loss: 3.4168 - val_loss: 3.1401\n",
            "108/108 - 4s - loss: 3.3937 - val_loss: 3.1372\n",
            "108/108 - 4s - loss: 3.4202 - val_loss: 3.1365\n",
            "108/108 - 4s - loss: 3.3800 - val_loss: 3.1373\n",
            "108/108 - 4s - loss: 3.4005 - val_loss: 3.1366\n",
            "108/108 - 4s - loss: 3.3755 - val_loss: 3.1371\n",
            "108/108 - 4s - loss: 3.3908 - val_loss: 3.1368\n",
            "108/108 - 4s - loss: 3.3821 - val_loss: 3.1376\n",
            "108/108 - 4s - loss: 3.3815 - val_loss: 3.1365\n",
            "108/108 - 4s - loss: 3.3724 - val_loss: 3.0906\n",
            "108/108 - 4s - loss: 3.3094 - val_loss: 3.0348\n",
            "108/108 - 4s - loss: 3.2702 - val_loss: 3.0325\n",
            "108/108 - 4s - loss: 3.2438 - val_loss: 2.9824\n",
            "108/108 - 4s - loss: 3.1821 - val_loss: 2.9522\n",
            "108/108 - 4s - loss: 3.1626 - val_loss: 2.9231\n",
            "108/108 - 4s - loss: 3.1192 - val_loss: 2.8863\n",
            "108/108 - 4s - loss: 3.0832 - val_loss: 2.8152\n",
            "108/108 - 4s - loss: 3.0516 - val_loss: 2.8326\n",
            "108/108 - 4s - loss: 3.0158 - val_loss: 2.8112\n",
            "108/108 - 4s - loss: 2.9908 - val_loss: 2.7827\n",
            "108/108 - 4s - loss: 2.9813 - val_loss: 2.7814\n",
            "108/108 - 4s - loss: 2.9570 - val_loss: 2.7653\n",
            "108/108 - 4s - loss: 2.9384 - val_loss: 2.7642\n",
            "108/108 - 4s - loss: 2.9289 - val_loss: 2.7490\n",
            "108/108 - 4s - loss: 2.9007 - val_loss: 2.7451\n",
            "108/108 - 4s - loss: 2.9008 - val_loss: 2.7214\n",
            "108/108 - 4s - loss: 2.8933 - val_loss: 2.7322\n",
            "108/108 - 4s - loss: 2.8780 - val_loss: 2.7357\n",
            "108/108 - 4s - loss: 2.8759 - val_loss: 2.7201\n",
            "108/108 - 4s - loss: 2.8469 - val_loss: 2.7341\n",
            "108/108 - 4s - loss: 2.8575 - val_loss: 2.7254\n",
            "108/108 - 4s - loss: 2.8513 - val_loss: 2.7344\n",
            "108/108 - 4s - loss: 2.8488 - val_loss: 2.7060\n",
            "108/108 - 4s - loss: 2.8300 - val_loss: 2.7018\n",
            "108/108 - 4s - loss: 2.8246 - val_loss: 2.6947\n",
            "108/108 - 4s - loss: 2.8200 - val_loss: 2.7002\n",
            "108/108 - 4s - loss: 2.8167 - val_loss: 2.7642\n",
            "108/108 - 4s - loss: 2.8181 - val_loss: 2.6978\n",
            "108/108 - 4s - loss: 2.7878 - val_loss: 2.7374\n",
            "108/108 - 4s - loss: 2.8153 - val_loss: 2.7375\n",
            "108/108 - 4s - loss: 2.8166 - val_loss: 2.7152\n",
            "108/108 - 4s - loss: 2.7810 - val_loss: 2.6973\n",
            "108/108 - 4s - loss: 2.7897 - val_loss: 2.7025\n",
            "108/108 - 4s - loss: 2.8042 - val_loss: 2.6847\n",
            "108/108 - 4s - loss: 2.7987 - val_loss: 2.7200\n",
            "108/108 - 4s - loss: 2.7691 - val_loss: 2.7167\n",
            "108/108 - 4s - loss: 2.7778 - val_loss: 2.6790\n",
            "108/108 - 4s - loss: 2.7848 - val_loss: 2.6817\n",
            "108/108 - 4s - loss: 2.7788 - val_loss: 2.6863\n",
            "108/108 - 4s - loss: 2.7779 - val_loss: 2.6863\n",
            "108/108 - 4s - loss: 2.7840 - val_loss: 2.6899\n",
            "108/108 - 4s - loss: 2.7657 - val_loss: 2.6589\n",
            "108/108 - 4s - loss: 2.7724 - val_loss: 2.6652\n",
            "108/108 - 4s - loss: 2.7503 - val_loss: 2.6815\n",
            "108/108 - 4s - loss: 2.7476 - val_loss: 2.6904\n",
            "108/108 - 4s - loss: 2.7476 - val_loss: 2.6644\n",
            "108/108 - 4s - loss: 2.7499 - val_loss: 2.6890\n",
            "108/108 - 4s - loss: 2.7261 - val_loss: 2.6844\n",
            "108/108 - 4s - loss: 2.6948 - val_loss: 2.7033\n",
            "108/108 - 4s - loss: 2.7413 - val_loss: 2.7015\n",
            "108/108 - 4s - loss: 2.7279 - val_loss: 2.7210\n",
            "108/108 - 4s - loss: 2.6968 - val_loss: 2.6914\n",
            "108/108 - 4s - loss: 2.6952 - val_loss: 2.6791\n",
            "108/108 - 4s - loss: 2.6818 - val_loss: 2.7237\n",
            "108/108 - 4s - loss: 2.7075 - val_loss: 2.6798\n",
            "108/108 - 4s - loss: 2.7368 - val_loss: 2.6657\n",
            "108/108 - 4s - loss: 2.6910 - val_loss: 2.6768\n",
            "108/108 - 4s - loss: 2.6974 - val_loss: 2.6734\n",
            "108/108 - 4s - loss: 2.7034 - val_loss: 2.6487\n",
            "108/108 - 4s - loss: 2.6717 - val_loss: 2.6443\n",
            "108/108 - 4s - loss: 2.6647 - val_loss: 2.6654\n",
            "108/108 - 4s - loss: 2.6746 - val_loss: 2.6375\n",
            "108/108 - 4s - loss: 2.6450 - val_loss: 2.6609\n",
            "108/108 - 4s - loss: 2.6459 - val_loss: 2.6373\n",
            "108/108 - 4s - loss: 2.6424 - val_loss: 2.6187\n",
            "108/108 - 4s - loss: 2.6558 - val_loss: 2.6224\n",
            "108/108 - 4s - loss: 2.6134 - val_loss: 2.6413\n",
            "108/108 - 4s - loss: 2.6808 - val_loss: 2.7438\n",
            "108/108 - 4s - loss: 2.6395 - val_loss: 2.6497\n",
            "108/108 - 4s - loss: 2.6282 - val_loss: 2.6194\n",
            "108/108 - 4s - loss: 2.5902 - val_loss: 2.6021\n",
            "108/108 - 4s - loss: 2.6401 - val_loss: 2.6182\n",
            "108/108 - 4s - loss: 2.5965 - val_loss: 2.6036\n",
            "108/108 - 4s - loss: 2.5957 - val_loss: 2.6299\n",
            "108/108 - 4s - loss: 2.5920 - val_loss: 2.6176\n",
            "108/108 - 4s - loss: 2.6310 - val_loss: 2.6817\n",
            "108/108 - 4s - loss: 2.6399 - val_loss: 2.6321\n",
            "108/108 - 4s - loss: 2.5976 - val_loss: 2.6759\n",
            "108/108 - 4s - loss: 2.6101 - val_loss: 2.6738\n",
            "108/108 - 4s - loss: 2.6252 - val_loss: 2.6516\n",
            "108/108 - 4s - loss: 2.5614 - val_loss: 2.6684\n",
            "108/108 - 4s - loss: 2.5523 - val_loss: 2.6783\n",
            "108/108 - 4s - loss: 2.5858 - val_loss: 2.6251\n",
            "108/108 - 4s - loss: 2.5398 - val_loss: 2.6590\n",
            "108/108 - 4s - loss: 2.5496 - val_loss: 2.7403\n",
            "108/108 - 4s - loss: 2.5418 - val_loss: 2.6847\n",
            "108/108 - 4s - loss: 2.5532 - val_loss: 2.7131\n",
            "108/108 - 4s - loss: 2.5316 - val_loss: 2.6984\n",
            "108/108 - 4s - loss: 2.5185 - val_loss: 2.6626\n",
            "108/108 - 4s - loss: 2.5192 - val_loss: 2.7202\n",
            "108/108 - 4s - loss: 2.5268 - val_loss: 2.6856\n",
            "108/108 - 4s - loss: 2.5331 - val_loss: 2.6256\n",
            "108/108 - 4s - loss: 2.4985 - val_loss: 2.6682\n",
            "108/108 - 4s - loss: 2.4945 - val_loss: 2.6512\n",
            "108/108 - 4s - loss: 2.5253 - val_loss: 2.6583\n",
            "108/108 - 4s - loss: 2.4886 - val_loss: 2.6774\n",
            "108/108 - 4s - loss: 2.5260 - val_loss: 2.6777\n",
            "108/108 - 4s - loss: 2.4912 - val_loss: 2.6759\n",
            "108/108 - 4s - loss: 2.4800 - val_loss: 2.6352\n",
            "108/108 - 4s - loss: 2.5124 - val_loss: 2.6693\n",
            "108/108 - 4s - loss: 2.4878 - val_loss: 2.6878\n",
            "108/108 - 4s - loss: 2.5067 - val_loss: 2.6508\n",
            "108/108 - 4s - loss: 2.4626 - val_loss: 2.6398\n",
            "108/108 - 4s - loss: 2.5108 - val_loss: 2.6943\n",
            "108/108 - 4s - loss: 2.5126 - val_loss: 2.6746\n",
            "108/108 - 4s - loss: 2.4682 - val_loss: 2.6662\n",
            "108/108 - 4s - loss: 2.4723 - val_loss: 2.7020\n",
            "108/108 - 3s - loss: 2.4619 - val_loss: 2.6698\n",
            "108/108 - 4s - loss: 2.4447 - val_loss: 2.7113\n",
            "108/108 - 4s - loss: 2.4876 - val_loss: 2.7143\n",
            "108/108 - 4s - loss: 2.4495 - val_loss: 2.6987\n",
            "108/108 - 4s - loss: 2.4628 - val_loss: 2.7368\n",
            "108/108 - 4s - loss: 2.4377 - val_loss: 2.7032\n",
            "108/108 - 4s - loss: 2.4693 - val_loss: 2.6845\n",
            "108/108 - 4s - loss: 2.4662 - val_loss: 2.7083\n",
            "108/108 - 4s - loss: 2.4456 - val_loss: 2.6854\n",
            "108/108 - 4s - loss: 2.4317 - val_loss: 2.6560\n",
            "108/108 - 4s - loss: 2.4245 - val_loss: 2.6887\n",
            "108/108 - 4s - loss: 2.4348 - val_loss: 2.6978\n",
            "108/108 - 4s - loss: 2.4397 - val_loss: 2.6915\n",
            "108/108 - 4s - loss: 2.4552 - val_loss: 2.7135\n",
            "108/108 - 4s - loss: 2.4361 - val_loss: 2.6873\n",
            "108/108 - 4s - loss: 2.3995 - val_loss: 2.7198\n",
            "108/108 - 4s - loss: 2.4171 - val_loss: 2.6809\n",
            "108/108 - 4s - loss: 2.4453 - val_loss: 2.6636\n",
            "108/108 - 4s - loss: 2.4307 - val_loss: 2.6893\n",
            "108/108 - 4s - loss: 2.4200 - val_loss: 2.7141\n",
            "108/108 - 4s - loss: 2.3908 - val_loss: 2.6949\n",
            "108/108 - 4s - loss: 2.4109 - val_loss: 2.6824\n",
            "108/108 - 4s - loss: 2.4069 - val_loss: 2.6871\n",
            "108/108 - 4s - loss: 2.4380 - val_loss: 2.6810\n",
            "108/108 - 4s - loss: 2.4129 - val_loss: 2.7008\n",
            "108/108 - 4s - loss: 2.4258 - val_loss: 2.6780\n",
            "108/108 - 4s - loss: 2.4029 - val_loss: 2.7061\n",
            "108/108 - 4s - loss: 2.4098 - val_loss: 2.7155\n",
            "108/108 - 4s - loss: 2.3948 - val_loss: 2.7096\n",
            "108/108 - 4s - loss: 2.4049 - val_loss: 2.6591\n",
            "108/108 - 4s - loss: 2.3788 - val_loss: 2.7421\n",
            "108/108 - 4s - loss: 2.3620 - val_loss: 2.6734\n",
            "108/108 - 4s - loss: 2.3628 - val_loss: 2.7019\n",
            "108/108 - 4s - loss: 2.4065 - val_loss: 2.7753\n",
            "108/108 - 4s - loss: 2.4043 - val_loss: 2.6687\n",
            "108/108 - 4s - loss: 2.4062 - val_loss: 2.6773\n",
            "108/108 - 4s - loss: 2.3729 - val_loss: 2.7054\n",
            "108/108 - 4s - loss: 2.4015 - val_loss: 2.7521\n",
            "108/108 - 4s - loss: 2.3823 - val_loss: 2.7423\n",
            "108/108 - 4s - loss: 2.3307 - val_loss: 2.7379\n",
            "108/108 - 4s - loss: 2.3701 - val_loss: 2.7609\n",
            "108/108 - 3s - loss: 2.3755 - val_loss: 2.7646\n",
            "108/108 - 4s - loss: 2.3506 - val_loss: 2.7132\n",
            "108/108 - 4s - loss: 2.3706 - val_loss: 2.7223\n",
            "108/108 - 4s - loss: 2.3656 - val_loss: 2.6723\n",
            "108/108 - 4s - loss: 2.3341 - val_loss: 2.6781\n",
            "108/108 - 4s - loss: 2.3457 - val_loss: 2.6941\n",
            "108/108 - 4s - loss: 2.3188 - val_loss: 2.6983\n",
            "108/108 - 4s - loss: 2.3417 - val_loss: 2.6634\n",
            "108/108 - 4s - loss: 2.4067 - val_loss: 2.7339\n",
            "108/108 - 4s - loss: 2.4918 - val_loss: 2.7447\n",
            "108/108 - 4s - loss: 2.3811 - val_loss: 2.7188\n",
            "108/108 - 4s - loss: 2.3963 - val_loss: 2.6625\n",
            "108/108 - 4s - loss: 2.3779 - val_loss: 2.6549\n",
            "108/108 - 4s - loss: 2.3665 - val_loss: 2.6603\n",
            "108/108 - 4s - loss: 2.3440 - val_loss: 2.6153\n",
            "108/108 - 4s - loss: 2.3346 - val_loss: 2.6961\n",
            "108/108 - 4s - loss: 2.3302 - val_loss: 2.7165\n",
            "108/108 - 4s - loss: 2.3687 - val_loss: 2.6534\n",
            "108/108 - 4s - loss: 2.3741 - val_loss: 2.6691\n",
            "108/108 - 4s - loss: 2.3249 - val_loss: 2.6816\n",
            "108/108 - 4s - loss: 2.3245 - val_loss: 2.7567\n",
            "108/108 - 4s - loss: 2.3278 - val_loss: 2.7004\n",
            "108/108 - 4s - loss: 2.3272 - val_loss: 2.6981\n",
            "108/108 - 4s - loss: 2.2975 - val_loss: 2.6960\n",
            "108/108 - 4s - loss: 2.3181 - val_loss: 2.7040\n",
            "108/108 - 4s - loss: 2.3259 - val_loss: 2.7418\n",
            "108/108 - 4s - loss: 2.2852 - val_loss: 2.6853\n",
            "108/108 - 4s - loss: 2.3093 - val_loss: 2.7177\n",
            "108/108 - 4s - loss: 2.2936 - val_loss: 2.7744\n",
            "108/108 - 4s - loss: 2.2936 - val_loss: 2.7031\n",
            "108/108 - 4s - loss: 2.3350 - val_loss: 2.7665\n",
            "108/108 - 4s - loss: 2.2939 - val_loss: 2.6790\n",
            "108/108 - 4s - loss: 2.2707 - val_loss: 2.7024\n",
            "108/108 - 4s - loss: 2.2807 - val_loss: 2.6843\n",
            "108/108 - 4s - loss: 2.2700 - val_loss: 2.7344\n",
            "108/108 - 4s - loss: 2.2925 - val_loss: 2.7654\n",
            "108/108 - 4s - loss: 2.2817 - val_loss: 2.6654\n",
            "108/108 - 4s - loss: 2.3078 - val_loss: 2.7385\n",
            "108/108 - 4s - loss: 2.2741 - val_loss: 2.7317\n",
            "108/108 - 4s - loss: 2.2781 - val_loss: 2.7769\n",
            "108/108 - 4s - loss: 2.2775 - val_loss: 2.6695\n",
            "108/108 - 4s - loss: 2.3014 - val_loss: 2.6725\n",
            "108/108 - 4s - loss: 2.2695 - val_loss: 2.7237\n",
            "108/108 - 4s - loss: 2.2955 - val_loss: 2.7037\n",
            "108/108 - 4s - loss: 2.2867 - val_loss: 2.7652\n",
            "108/108 - 4s - loss: 2.2551 - val_loss: 2.6950\n",
            "108/108 - 4s - loss: 2.2567 - val_loss: 2.6966\n",
            "108/108 - 4s - loss: 2.2410 - val_loss: 2.7141\n",
            "108/108 - 4s - loss: 2.2500 - val_loss: 2.6777\n",
            "108/108 - 4s - loss: 2.2910 - val_loss: 2.6664\n",
            "108/108 - 4s - loss: 2.2748 - val_loss: 2.6634\n",
            "108/108 - 4s - loss: 2.2814 - val_loss: 2.7156\n",
            "108/108 - 4s - loss: 2.2891 - val_loss: 2.7386\n",
            "108/108 - 4s - loss: 2.2401 - val_loss: 2.7406\n",
            "108/108 - 4s - loss: 2.2350 - val_loss: 2.6586\n",
            "108/108 - 4s - loss: 2.2713 - val_loss: 2.7553\n",
            "108/108 - 4s - loss: 2.2424 - val_loss: 2.7015\n",
            "108/108 - 4s - loss: 2.2241 - val_loss: 2.6465\n",
            "108/108 - 4s - loss: 2.2517 - val_loss: 2.6882\n",
            "108/108 - 4s - loss: 2.2486 - val_loss: 2.7134\n",
            "108/108 - 4s - loss: 2.2410 - val_loss: 2.6946\n",
            "108/108 - 4s - loss: 2.2459 - val_loss: 2.7321\n",
            "108/108 - 4s - loss: 2.3009 - val_loss: 2.7932\n",
            "108/108 - 4s - loss: 2.3162 - val_loss: 2.7031\n",
            "108/108 - 4s - loss: 2.3150 - val_loss: 2.6989\n",
            "108/108 - 4s - loss: 2.2807 - val_loss: 2.7533\n",
            "108/108 - 4s - loss: 2.2652 - val_loss: 2.7605\n",
            "108/108 - 4s - loss: 2.2439 - val_loss: 2.7889\n",
            "108/108 - 4s - loss: 2.2485 - val_loss: 2.8172\n",
            "108/108 - 4s - loss: 2.2524 - val_loss: 2.7532\n",
            "108/108 - 4s - loss: 2.2298 - val_loss: 2.7482\n",
            "108/108 - 4s - loss: 2.2149 - val_loss: 2.7585\n",
            "108/108 - 4s - loss: 2.2337 - val_loss: 2.7942\n",
            "108/108 - 4s - loss: 2.2315 - val_loss: 2.7409\n",
            "108/108 - 4s - loss: 2.2364 - val_loss: 2.8088\n",
            "108/108 - 4s - loss: 2.2553 - val_loss: 2.7178\n",
            "108/108 - 4s - loss: 2.2456 - val_loss: 2.7128\n",
            "108/108 - 4s - loss: 2.2830 - val_loss: 2.7152\n",
            "108/108 - 4s - loss: 2.3093 - val_loss: 2.7310\n",
            "108/108 - 4s - loss: 2.2616 - val_loss: 2.7014\n",
            "108/108 - 4s - loss: 2.2485 - val_loss: 2.7206\n",
            "108/108 - 4s - loss: 2.2083 - val_loss: 2.7547\n",
            "108/108 - 4s - loss: 2.2253 - val_loss: 2.7257\n",
            "108/108 - 4s - loss: 2.2342 - val_loss: 2.6697\n",
            "108/108 - 4s - loss: 2.2215 - val_loss: 2.6886\n",
            "108/108 - 4s - loss: 2.2427 - val_loss: 2.7436\n",
            "108/108 - 4s - loss: 2.2451 - val_loss: 2.7193\n",
            "108/108 - 4s - loss: 2.2295 - val_loss: 2.7187\n",
            "108/108 - 4s - loss: 2.2428 - val_loss: 2.7422\n",
            "108/108 - 4s - loss: 2.2473 - val_loss: 2.7107\n",
            "108/108 - 4s - loss: 2.2789 - val_loss: 2.6259\n",
            "108/108 - 4s - loss: 2.2919 - val_loss: 2.6694\n",
            "108/108 - 4s - loss: 2.2476 - val_loss: 2.6613\n",
            "108/108 - 4s - loss: 2.2479 - val_loss: 2.7268\n",
            "108/108 - 4s - loss: 2.2327 - val_loss: 2.7670\n",
            "108/108 - 4s - loss: 2.2440 - val_loss: 2.7689\n",
            "108/108 - 4s - loss: 2.2241 - val_loss: 2.6621\n",
            "108/108 - 4s - loss: 2.2257 - val_loss: 2.8036\n",
            "108/108 - 4s - loss: 2.2397 - val_loss: 2.7147\n",
            "108/108 - 4s - loss: 2.2586 - val_loss: 2.6998\n",
            "108/108 - 4s - loss: 2.2509 - val_loss: 2.7517\n",
            "108/108 - 4s - loss: 2.2813 - val_loss: 2.7449\n",
            "108/108 - 4s - loss: 2.2379 - val_loss: 2.7040\n",
            "108/108 - 4s - loss: 2.2319 - val_loss: 2.7398\n",
            "108/108 - 4s - loss: 2.2717 - val_loss: 2.6366\n",
            "108/108 - 4s - loss: 2.2156 - val_loss: 2.6523\n",
            "108/108 - 4s - loss: 2.2094 - val_loss: 2.7499\n",
            "108/108 - 4s - loss: 2.2287 - val_loss: 2.7564\n",
            "108/108 - 4s - loss: 2.1907 - val_loss: 2.8001\n",
            "108/108 - 4s - loss: 2.1954 - val_loss: 2.7260\n",
            "108/108 - 4s - loss: 2.1987 - val_loss: 2.8083\n",
            "108/108 - 4s - loss: 2.1918 - val_loss: 2.8006\n",
            "108/108 - 4s - loss: 2.1999 - val_loss: 2.7547\n",
            "108/108 - 4s - loss: 2.2022 - val_loss: 2.7306\n",
            "108/108 - 4s - loss: 2.2593 - val_loss: 2.7470\n",
            "108/108 - 4s - loss: 2.2493 - val_loss: 2.7283\n",
            "108/108 - 4s - loss: 2.1839 - val_loss: 2.7394\n",
            "108/108 - 4s - loss: 2.1570 - val_loss: 2.7508\n",
            "108/108 - 4s - loss: 2.1783 - val_loss: 2.8438\n",
            "108/108 - 4s - loss: 2.1804 - val_loss: 2.7202\n",
            "108/108 - 3s - loss: 2.1610 - val_loss: 2.7160\n",
            "108/108 - 4s - loss: 2.1874 - val_loss: 2.7540\n",
            "108/108 - 4s - loss: 2.1747 - val_loss: 2.7782\n",
            "108/108 - 4s - loss: 2.1662 - val_loss: 2.7592\n",
            "108/108 - 4s - loss: 2.1892 - val_loss: 2.7191\n",
            "108/108 - 4s - loss: 2.1808 - val_loss: 2.7545\n",
            "108/108 - 4s - loss: 2.1890 - val_loss: 2.7498\n",
            "108/108 - 4s - loss: 2.1829 - val_loss: 2.7713\n",
            "108/108 - 4s - loss: 2.1683 - val_loss: 2.7420\n",
            "108/108 - 4s - loss: 2.2053 - val_loss: 2.6641\n",
            "108/108 - 4s - loss: 2.1525 - val_loss: 2.7656\n",
            "108/108 - 4s - loss: 2.1593 - val_loss: 2.7634\n",
            "108/108 - 4s - loss: 2.1433 - val_loss: 2.8442\n",
            "108/108 - 4s - loss: 2.1715 - val_loss: 2.6803\n",
            "108/108 - 4s - loss: 2.1471 - val_loss: 2.7670\n",
            "108/108 - 4s - loss: 2.1730 - val_loss: 2.7127\n",
            "108/108 - 4s - loss: 2.1733 - val_loss: 2.7377\n",
            "108/108 - 4s - loss: 2.1648 - val_loss: 2.7561\n",
            "108/108 - 4s - loss: 2.1955 - val_loss: 2.8474\n",
            "108/108 - 4s - loss: 2.2066 - val_loss: 2.8495\n",
            "108/108 - 4s - loss: 2.1895 - val_loss: 2.8276\n",
            "108/108 - 4s - loss: 2.1672 - val_loss: 2.7368\n",
            "108/108 - 4s - loss: 2.1282 - val_loss: 2.8177\n",
            "108/108 - 4s - loss: 2.1341 - val_loss: 2.7267\n",
            "108/108 - 4s - loss: 2.1791 - val_loss: 2.7099\n",
            "108/108 - 4s - loss: 2.1605 - val_loss: 2.7693\n",
            "108/108 - 4s - loss: 2.1633 - val_loss: 2.7287\n",
            "108/108 - 4s - loss: 2.1441 - val_loss: 2.7439\n",
            "108/108 - 4s - loss: 2.2057 - val_loss: 2.7128\n",
            "108/108 - 4s - loss: 2.1386 - val_loss: 2.7435\n",
            "108/108 - 4s - loss: 2.1609 - val_loss: 2.7417\n",
            "108/108 - 4s - loss: 2.1537 - val_loss: 2.7375\n",
            "108/108 - 4s - loss: 2.1223 - val_loss: 2.7113\n",
            "108/108 - 4s - loss: 2.1355 - val_loss: 2.7727\n",
            "108/108 - 4s - loss: 2.1097 - val_loss: 2.7871\n",
            "108/108 - 4s - loss: 2.1592 - val_loss: 2.7611\n",
            "108/108 - 4s - loss: 2.1598 - val_loss: 2.7318\n",
            "108/108 - 4s - loss: 2.1532 - val_loss: 2.7631\n",
            "108/108 - 4s - loss: 2.1334 - val_loss: 2.7119\n",
            "108/108 - 4s - loss: 2.1459 - val_loss: 2.7289\n",
            "108/108 - 4s - loss: 2.1547 - val_loss: 2.7223\n",
            "108/108 - 4s - loss: 2.1409 - val_loss: 2.7888\n",
            "108/108 - 4s - loss: 2.1291 - val_loss: 2.7578\n",
            "108/108 - 4s - loss: 2.1888 - val_loss: 2.7423\n",
            "108/108 - 4s - loss: 2.2075 - val_loss: 2.7993\n",
            "108/108 - 4s - loss: 2.1876 - val_loss: 2.7523\n",
            "108/108 - 4s - loss: 2.1508 - val_loss: 2.7307\n",
            "108/108 - 4s - loss: 2.1211 - val_loss: 2.6943\n",
            "108/108 - 4s - loss: 2.1127 - val_loss: 2.7593\n",
            "108/108 - 4s - loss: 2.1469 - val_loss: 2.6839\n",
            "108/108 - 4s - loss: 2.1649 - val_loss: 2.7984\n",
            "108/108 - 4s - loss: 2.1409 - val_loss: 2.7929\n",
            "108/108 - 4s - loss: 2.1569 - val_loss: 2.7637\n",
            "108/108 - 4s - loss: 2.1242 - val_loss: 2.7120\n",
            "108/108 - 4s - loss: 2.1338 - val_loss: 2.7083\n",
            "108/108 - 4s - loss: 2.1223 - val_loss: 2.7639\n",
            "108/108 - 4s - loss: 2.1232 - val_loss: 2.7729\n",
            "108/108 - 4s - loss: 2.0936 - val_loss: 2.7613\n",
            "108/108 - 4s - loss: 2.1045 - val_loss: 2.7006\n",
            "108/108 - 4s - loss: 2.1092 - val_loss: 2.7800\n",
            "108/108 - 4s - loss: 2.0829 - val_loss: 2.6856\n",
            "108/108 - 4s - loss: 2.0911 - val_loss: 2.7495\n",
            "108/108 - 4s - loss: 2.1007 - val_loss: 2.8602\n",
            "108/108 - 4s - loss: 2.0832 - val_loss: 2.7802\n",
            "108/108 - 4s - loss: 2.1056 - val_loss: 2.8229\n",
            "108/108 - 4s - loss: 2.0959 - val_loss: 2.7675\n",
            "108/108 - 4s - loss: 2.1117 - val_loss: 2.7222\n",
            "108/108 - 4s - loss: 2.1223 - val_loss: 2.7392\n",
            "108/108 - 4s - loss: 2.1331 - val_loss: 2.7754\n",
            "108/108 - 4s - loss: 2.1019 - val_loss: 2.7337\n",
            "108/108 - 4s - loss: 2.0935 - val_loss: 2.7778\n",
            "108/108 - 4s - loss: 2.0800 - val_loss: 2.7687\n",
            "108/108 - 4s - loss: 2.0976 - val_loss: 2.6692\n",
            "108/108 - 4s - loss: 2.1021 - val_loss: 2.7477\n",
            "108/108 - 4s - loss: 2.1113 - val_loss: 2.7184\n",
            "108/108 - 4s - loss: 2.1391 - val_loss: 2.7687\n",
            "108/108 - 4s - loss: 2.1358 - val_loss: 2.7343\n",
            "108/108 - 4s - loss: 2.1311 - val_loss: 2.7977\n",
            "108/108 - 4s - loss: 2.1280 - val_loss: 2.7826\n",
            "108/108 - 4s - loss: 2.1439 - val_loss: 2.7047\n",
            "108/108 - 4s - loss: 2.1313 - val_loss: 2.6601\n",
            "108/108 - 4s - loss: 2.0938 - val_loss: 2.7712\n",
            "108/108 - 4s - loss: 2.0769 - val_loss: 2.7143\n",
            "108/108 - 4s - loss: 2.0868 - val_loss: 2.7125\n",
            "108/108 - 4s - loss: 2.0852 - val_loss: 2.7598\n",
            "108/108 - 4s - loss: 2.0751 - val_loss: 2.7365\n",
            "108/108 - 4s - loss: 2.0814 - val_loss: 2.7592\n",
            "108/108 - 4s - loss: 2.0931 - val_loss: 2.7370\n",
            "108/108 - 4s - loss: 2.0856 - val_loss: 2.7550\n",
            "108/108 - 4s - loss: 2.0460 - val_loss: 2.7228\n",
            "108/108 - 4s - loss: 2.0768 - val_loss: 2.7038\n",
            "108/108 - 4s - loss: 2.0722 - val_loss: 2.7623\n",
            "108/108 - 4s - loss: 2.1070 - val_loss: 2.7018\n",
            "108/108 - 4s - loss: 2.0878 - val_loss: 2.7525\n",
            "108/108 - 4s - loss: 2.0737 - val_loss: 2.6761\n",
            "108/108 - 4s - loss: 2.0973 - val_loss: 2.7089\n",
            "108/108 - 4s - loss: 2.0783 - val_loss: 2.7817\n",
            "108/108 - 4s - loss: 2.0680 - val_loss: 2.7691\n",
            "108/108 - 4s - loss: 2.0858 - val_loss: 2.8165\n",
            "108/108 - 4s - loss: 2.0805 - val_loss: 2.7242\n",
            "108/108 - 4s - loss: 2.0791 - val_loss: 2.7032\n",
            "108/108 - 4s - loss: 2.0641 - val_loss: 2.7669\n",
            "108/108 - 4s - loss: 2.1008 - val_loss: 2.7481\n",
            "108/108 - 4s - loss: 2.1163 - val_loss: 2.7752\n",
            "108/108 - 4s - loss: 2.0859 - val_loss: 2.7370\n",
            "108/108 - 4s - loss: 2.0799 - val_loss: 2.7367\n",
            "108/108 - 4s - loss: 2.0460 - val_loss: 2.7968\n",
            "108/108 - 4s - loss: 2.0511 - val_loss: 2.7205\n",
            "108/108 - 4s - loss: 2.0544 - val_loss: 2.7736\n",
            "108/108 - 4s - loss: 2.0579 - val_loss: 2.7690\n",
            "108/108 - 4s - loss: 2.0413 - val_loss: 2.8313\n",
            "108/108 - 4s - loss: 2.0472 - val_loss: 2.7538\n",
            "108/108 - 4s - loss: 2.0459 - val_loss: 2.7564\n",
            "108/108 - 4s - loss: 2.0510 - val_loss: 2.7734\n",
            "108/108 - 4s - loss: 2.0532 - val_loss: 2.7653\n",
            "108/108 - 4s - loss: 2.0596 - val_loss: 2.6712\n",
            "108/108 - 4s - loss: 2.0422 - val_loss: 2.7452\n",
            "108/108 - 4s - loss: 2.0294 - val_loss: 2.7104\n",
            "108/108 - 4s - loss: 2.0308 - val_loss: 2.8153\n",
            "108/108 - 4s - loss: 2.0391 - val_loss: 2.7368\n",
            "108/108 - 4s - loss: 2.0470 - val_loss: 2.7651\n",
            "108/108 - 4s - loss: 2.0755 - val_loss: 2.7797\n",
            "108/108 - 4s - loss: 2.0449 - val_loss: 2.7750\n",
            "108/108 - 4s - loss: 2.0245 - val_loss: 2.7906\n",
            "108/108 - 4s - loss: 2.0353 - val_loss: 2.8098\n",
            "108/108 - 4s - loss: 2.0500 - val_loss: 2.7623\n",
            "108/108 - 4s - loss: 2.0671 - val_loss: 2.7598\n",
            "108/108 - 4s - loss: 2.0705 - val_loss: 2.8173\n",
            "108/108 - 4s - loss: 2.0551 - val_loss: 2.7961\n",
            "108/108 - 4s - loss: 2.0576 - val_loss: 2.7856\n",
            "108/108 - 4s - loss: 2.0549 - val_loss: 2.7173\n",
            "108/108 - 4s - loss: 2.0485 - val_loss: 2.7467\n",
            "108/108 - 4s - loss: 2.0233 - val_loss: 2.7289\n",
            "108/108 - 4s - loss: 2.0337 - val_loss: 2.7140\n",
            "108/108 - 4s - loss: 2.0239 - val_loss: 2.7133\n",
            "108/108 - 4s - loss: 2.0509 - val_loss: 2.6494\n",
            "108/108 - 4s - loss: 2.0269 - val_loss: 2.7346\n",
            "108/108 - 4s - loss: 2.0476 - val_loss: 2.7240\n",
            "108/108 - 4s - loss: 2.0264 - val_loss: 2.7991\n",
            "108/108 - 4s - loss: 2.0136 - val_loss: 2.7263\n",
            "108/108 - 4s - loss: 2.0302 - val_loss: 2.8100\n",
            "108/108 - 4s - loss: 2.0234 - val_loss: 2.8338\n",
            "108/108 - 4s - loss: 2.0313 - val_loss: 2.8058\n",
            "108/108 - 3s - loss: 2.0162 - val_loss: 2.8153\n",
            "108/108 - 3s - loss: 2.0211 - val_loss: 2.7975\n",
            "108/108 - 4s - loss: 2.0342 - val_loss: 2.7244\n",
            "108/108 - 3s - loss: 2.0308 - val_loss: 2.7462\n",
            "108/108 - 3s - loss: 2.0310 - val_loss: 2.6889\n",
            "108/108 - 3s - loss: 2.0425 - val_loss: 2.7668\n",
            "108/108 - 3s - loss: 2.0105 - val_loss: 2.7172\n",
            "108/108 - 3s - loss: 2.0183 - val_loss: 2.6530\n",
            "108/108 - 3s - loss: 2.0105 - val_loss: 2.7503\n",
            "108/108 - 3s - loss: 2.0107 - val_loss: 2.7408\n",
            "108/108 - 3s - loss: 2.0033 - val_loss: 2.7340\n",
            "108/108 - 3s - loss: 2.0170 - val_loss: 2.7140\n",
            "108/108 - 3s - loss: 1.9910 - val_loss: 2.7092\n",
            "108/108 - 3s - loss: 2.0329 - val_loss: 2.7278\n",
            "108/108 - 3s - loss: 2.0116 - val_loss: 2.7225\n",
            "108/108 - 3s - loss: 2.0117 - val_loss: 2.7638\n",
            "108/108 - 3s - loss: 2.0164 - val_loss: 2.7006\n",
            "108/108 - 3s - loss: 2.0301 - val_loss: 2.8199\n",
            "108/108 - 3s - loss: 2.0339 - val_loss: 2.7575\n",
            "108/108 - 3s - loss: 1.9937 - val_loss: 2.7428\n",
            "108/108 - 3s - loss: 2.0039 - val_loss: 2.7136\n",
            "108/108 - 3s - loss: 2.0216 - val_loss: 2.7303\n",
            "108/108 - 3s - loss: 1.9980 - val_loss: 2.7251\n",
            "108/108 - 3s - loss: 2.0329 - val_loss: 2.7243\n",
            "108/108 - 3s - loss: 2.0016 - val_loss: 2.7555\n",
            "108/108 - 3s - loss: 1.9865 - val_loss: 2.7384\n",
            "108/108 - 3s - loss: 1.9787 - val_loss: 2.7256\n",
            "108/108 - 3s - loss: 2.0002 - val_loss: 2.7539\n",
            "108/108 - 3s - loss: 1.9869 - val_loss: 2.7649\n",
            "108/108 - 3s - loss: 1.9780 - val_loss: 2.7527\n",
            "108/108 - 3s - loss: 2.0147 - val_loss: 2.7587\n",
            "108/108 - 3s - loss: 2.0028 - val_loss: 2.7473\n",
            "108/108 - 3s - loss: 2.0112 - val_loss: 2.7747\n",
            "108/108 - 3s - loss: 1.9988 - val_loss: 2.6568\n",
            "108/108 - 3s - loss: 2.0211 - val_loss: 2.7573\n",
            "108/108 - 3s - loss: 1.9879 - val_loss: 2.7391\n",
            "108/108 - 3s - loss: 1.9808 - val_loss: 2.7411\n",
            "108/108 - 3s - loss: 1.9893 - val_loss: 2.7251\n",
            "108/108 - 3s - loss: 1.9997 - val_loss: 2.7269\n",
            "108/108 - 3s - loss: 1.9565 - val_loss: 2.7526\n",
            "108/108 - 3s - loss: 1.9959 - val_loss: 2.7701\n",
            "108/108 - 3s - loss: 1.9760 - val_loss: 2.7214\n",
            "108/108 - 3s - loss: 1.9677 - val_loss: 2.7135\n",
            "108/108 - 3s - loss: 1.9895 - val_loss: 2.7993\n",
            "108/108 - 3s - loss: 1.9752 - val_loss: 2.6940\n",
            "108/108 - 3s - loss: 1.9542 - val_loss: 2.7350\n",
            "108/108 - 3s - loss: 1.9899 - val_loss: 2.7335\n",
            "108/108 - 3s - loss: 1.9873 - val_loss: 2.6803\n",
            "108/108 - 4s - loss: 1.9865 - val_loss: 2.7625\n",
            "108/108 - 3s - loss: 1.9453 - val_loss: 2.6944\n",
            "108/108 - 3s - loss: 1.9804 - val_loss: 2.7870\n",
            "108/108 - 3s - loss: 1.9520 - val_loss: 2.7291\n",
            "108/108 - 3s - loss: 1.9661 - val_loss: 2.6681\n",
            "108/108 - 3s - loss: 2.0121 - val_loss: 2.7199\n",
            "108/108 - 3s - loss: 1.9347 - val_loss: 2.6882\n",
            "108/108 - 3s - loss: 1.9438 - val_loss: 2.7333\n",
            "108/108 - 3s - loss: 1.9583 - val_loss: 2.7702\n",
            "108/108 - 3s - loss: 1.9719 - val_loss: 2.7515\n",
            "108/108 - 3s - loss: 1.9723 - val_loss: 2.6954\n",
            "108/108 - 4s - loss: 1.9405 - val_loss: 2.7221\n",
            "108/108 - 3s - loss: 1.9476 - val_loss: 2.7454\n",
            "108/108 - 3s - loss: 1.9613 - val_loss: 2.6977\n",
            "108/108 - 3s - loss: 1.9480 - val_loss: 2.6956\n",
            "108/108 - 3s - loss: 1.9393 - val_loss: 2.7558\n",
            "108/108 - 3s - loss: 1.9463 - val_loss: 2.7061\n",
            "108/108 - 3s - loss: 1.9571 - val_loss: 2.7651\n",
            "108/108 - 3s - loss: 1.9526 - val_loss: 2.7155\n",
            "108/108 - 3s - loss: 1.9514 - val_loss: 2.7255\n",
            "108/108 - 3s - loss: 1.9448 - val_loss: 2.7796\n",
            "108/108 - 4s - loss: 1.9406 - val_loss: 2.7731\n",
            "108/108 - 4s - loss: 1.9381 - val_loss: 2.8052\n",
            "108/108 - 3s - loss: 1.9396 - val_loss: 2.7321\n",
            "0.4\n",
            "108/108 - 7s - loss: 6.7092 - val_loss: 5.6203\n",
            "108/108 - 3s - loss: 5.7141 - val_loss: 4.8998\n",
            "108/108 - 3s - loss: 5.0986 - val_loss: 4.4256\n",
            "108/108 - 4s - loss: 4.6929 - val_loss: 4.0997\n",
            "108/108 - 3s - loss: 4.3876 - val_loss: 3.8839\n",
            "108/108 - 3s - loss: 4.2002 - val_loss: 3.7493\n",
            "108/108 - 3s - loss: 4.0495 - val_loss: 3.6744\n",
            "108/108 - 3s - loss: 3.9879 - val_loss: 3.6390\n",
            "108/108 - 4s - loss: 3.9702 - val_loss: 3.6243\n",
            "108/108 - 4s - loss: 3.9527 - val_loss: 3.6199\n",
            "108/108 - 4s - loss: 3.9481 - val_loss: 3.6195\n",
            "108/108 - 4s - loss: 3.9219 - val_loss: 3.6208\n",
            "108/108 - 4s - loss: 3.9326 - val_loss: 3.6225\n",
            "108/108 - 3s - loss: 3.9254 - val_loss: 3.6228\n",
            "108/108 - 3s - loss: 3.9419 - val_loss: 3.6237\n",
            "108/108 - 3s - loss: 3.9112 - val_loss: 3.6246\n",
            "108/108 - 3s - loss: 3.9406 - val_loss: 3.5354\n",
            "108/108 - 3s - loss: 3.8512 - val_loss: 3.4768\n",
            "108/108 - 3s - loss: 3.7275 - val_loss: 3.4255\n",
            "108/108 - 3s - loss: 3.6678 - val_loss: 3.2807\n",
            "108/108 - 4s - loss: 3.5548 - val_loss: 3.2428\n",
            "108/108 - 3s - loss: 3.4856 - val_loss: 3.1646\n",
            "108/108 - 3s - loss: 3.3894 - val_loss: 3.1171\n",
            "108/108 - 3s - loss: 3.3484 - val_loss: 3.1105\n",
            "108/108 - 3s - loss: 3.2971 - val_loss: 3.0877\n",
            "108/108 - 4s - loss: 3.2915 - val_loss: 3.0279\n",
            "108/108 - 3s - loss: 3.2234 - val_loss: 3.0151\n",
            "108/108 - 3s - loss: 3.2140 - val_loss: 2.9723\n",
            "108/108 - 3s - loss: 3.1953 - val_loss: 2.9569\n",
            "108/108 - 3s - loss: 3.1624 - val_loss: 2.9334\n",
            "108/108 - 3s - loss: 3.1146 - val_loss: 2.9425\n",
            "108/108 - 3s - loss: 3.1154 - val_loss: 2.9157\n",
            "108/108 - 3s - loss: 3.1116 - val_loss: 2.9219\n",
            "108/108 - 3s - loss: 3.0779 - val_loss: 2.9109\n",
            "108/108 - 3s - loss: 3.1038 - val_loss: 2.9178\n",
            "108/108 - 3s - loss: 3.0549 - val_loss: 2.8854\n",
            "108/108 - 3s - loss: 3.0702 - val_loss: 2.8971\n",
            "108/108 - 3s - loss: 3.0673 - val_loss: 2.8793\n",
            "108/108 - 4s - loss: 3.0360 - val_loss: 2.8996\n",
            "108/108 - 3s - loss: 3.0771 - val_loss: 2.9362\n",
            "108/108 - 3s - loss: 3.0065 - val_loss: 2.8633\n",
            "108/108 - 3s - loss: 3.0226 - val_loss: 2.9251\n",
            "108/108 - 4s - loss: 3.0246 - val_loss: 2.9012\n",
            "108/108 - 3s - loss: 3.0216 - val_loss: 2.9119\n",
            "108/108 - 4s - loss: 3.0288 - val_loss: 2.9278\n",
            "108/108 - 3s - loss: 2.9695 - val_loss: 2.8782\n",
            "108/108 - 3s - loss: 3.0244 - val_loss: 2.8903\n",
            "108/108 - 3s - loss: 3.0389 - val_loss: 2.8762\n",
            "108/108 - 3s - loss: 2.9741 - val_loss: 2.8690\n",
            "108/108 - 3s - loss: 3.0183 - val_loss: 2.8949\n",
            "108/108 - 3s - loss: 2.9733 - val_loss: 2.9277\n",
            "108/108 - 3s - loss: 2.9958 - val_loss: 2.8462\n",
            "108/108 - 3s - loss: 2.9552 - val_loss: 2.8701\n",
            "108/108 - 3s - loss: 2.9449 - val_loss: 2.8374\n",
            "108/108 - 3s - loss: 2.9823 - val_loss: 2.8757\n",
            "108/108 - 3s - loss: 2.9816 - val_loss: 2.8750\n",
            "108/108 - 4s - loss: 2.9625 - val_loss: 2.8437\n",
            "108/108 - 3s - loss: 2.9564 - val_loss: 2.8537\n",
            "108/108 - 3s - loss: 2.9476 - val_loss: 2.8715\n",
            "108/108 - 4s - loss: 2.9356 - val_loss: 2.9103\n",
            "108/108 - 3s - loss: 2.9243 - val_loss: 2.9220\n",
            "108/108 - 3s - loss: 2.9239 - val_loss: 2.8808\n",
            "108/108 - 3s - loss: 2.9085 - val_loss: 2.9103\n",
            "108/108 - 3s - loss: 2.8921 - val_loss: 2.9110\n",
            "108/108 - 3s - loss: 2.9267 - val_loss: 2.8730\n",
            "108/108 - 3s - loss: 2.8841 - val_loss: 2.9106\n",
            "108/108 - 4s - loss: 2.9074 - val_loss: 2.9224\n",
            "108/108 - 3s - loss: 2.8632 - val_loss: 2.8455\n",
            "108/108 - 4s - loss: 2.8660 - val_loss: 2.8679\n",
            "108/108 - 3s - loss: 2.9017 - val_loss: 2.8494\n",
            "108/108 - 3s - loss: 2.8547 - val_loss: 2.8828\n",
            "108/108 - 3s - loss: 2.8698 - val_loss: 2.9019\n",
            "108/108 - 3s - loss: 2.8697 - val_loss: 2.8999\n",
            "108/108 - 3s - loss: 2.8625 - val_loss: 2.8739\n",
            "108/108 - 3s - loss: 2.8620 - val_loss: 2.8943\n",
            "108/108 - 3s - loss: 2.8519 - val_loss: 2.9000\n",
            "108/108 - 3s - loss: 2.8612 - val_loss: 2.8742\n",
            "108/108 - 3s - loss: 2.8439 - val_loss: 2.8828\n",
            "108/108 - 4s - loss: 2.8171 - val_loss: 2.8712\n",
            "108/108 - 3s - loss: 2.8289 - val_loss: 2.9057\n",
            "108/108 - 4s - loss: 2.8205 - val_loss: 2.8760\n",
            "108/108 - 4s - loss: 2.8504 - val_loss: 2.8845\n",
            "108/108 - 4s - loss: 2.8347 - val_loss: 2.8983\n",
            "108/108 - 4s - loss: 2.8047 - val_loss: 2.8454\n",
            "108/108 - 4s - loss: 2.8134 - val_loss: 2.8319\n",
            "108/108 - 4s - loss: 2.8433 - val_loss: 2.8986\n",
            "108/108 - 3s - loss: 2.8206 - val_loss: 2.8823\n",
            "108/108 - 3s - loss: 2.8212 - val_loss: 2.8813\n",
            "108/108 - 4s - loss: 2.8359 - val_loss: 2.8635\n",
            "108/108 - 4s - loss: 2.8041 - val_loss: 2.8559\n",
            "108/108 - 3s - loss: 2.7835 - val_loss: 2.9305\n",
            "108/108 - 3s - loss: 2.8068 - val_loss: 2.8928\n",
            "108/108 - 3s - loss: 2.8070 - val_loss: 2.8653\n",
            "108/108 - 4s - loss: 2.7796 - val_loss: 2.8878\n",
            "108/108 - 4s - loss: 2.7939 - val_loss: 2.8404\n",
            "108/108 - 4s - loss: 2.7462 - val_loss: 2.8755\n",
            "108/108 - 3s - loss: 2.7728 - val_loss: 2.8868\n",
            "108/108 - 3s - loss: 2.7417 - val_loss: 2.8565\n",
            "108/108 - 4s - loss: 2.7711 - val_loss: 2.8533\n",
            "108/108 - 4s - loss: 2.7603 - val_loss: 2.8443\n",
            "108/108 - 4s - loss: 2.7757 - val_loss: 2.8993\n",
            "108/108 - 4s - loss: 2.7181 - val_loss: 2.9154\n",
            "108/108 - 3s - loss: 2.7591 - val_loss: 2.8899\n",
            "108/108 - 4s - loss: 2.7797 - val_loss: 2.8533\n",
            "108/108 - 3s - loss: 2.7106 - val_loss: 2.8215\n",
            "108/108 - 3s - loss: 2.7758 - val_loss: 2.8594\n",
            "108/108 - 3s - loss: 2.7351 - val_loss: 2.8622\n",
            "108/108 - 3s - loss: 2.7615 - val_loss: 2.9036\n",
            "108/108 - 4s - loss: 2.7314 - val_loss: 2.8730\n",
            "108/108 - 4s - loss: 2.7418 - val_loss: 2.8547\n",
            "108/108 - 4s - loss: 2.7258 - val_loss: 2.8955\n",
            "108/108 - 4s - loss: 2.6953 - val_loss: 2.8307\n",
            "108/108 - 3s - loss: 2.7313 - val_loss: 2.8234\n",
            "108/108 - 4s - loss: 2.7251 - val_loss: 2.8606\n",
            "108/108 - 3s - loss: 2.7400 - val_loss: 2.9222\n",
            "108/108 - 3s - loss: 2.7448 - val_loss: 2.8757\n",
            "108/108 - 4s - loss: 2.7105 - val_loss: 2.8625\n",
            "108/108 - 3s - loss: 2.7760 - val_loss: 2.9067\n",
            "108/108 - 3s - loss: 2.7561 - val_loss: 2.9218\n",
            "108/108 - 4s - loss: 2.7538 - val_loss: 3.0074\n",
            "108/108 - 4s - loss: 2.7492 - val_loss: 2.8625\n",
            "108/108 - 4s - loss: 2.6962 - val_loss: 2.8141\n",
            "108/108 - 3s - loss: 2.6971 - val_loss: 2.8996\n",
            "108/108 - 4s - loss: 2.7374 - val_loss: 2.8416\n",
            "108/108 - 4s - loss: 2.6959 - val_loss: 2.8931\n",
            "108/108 - 3s - loss: 2.7402 - val_loss: 2.8859\n",
            "108/108 - 4s - loss: 2.7095 - val_loss: 2.8937\n",
            "108/108 - 3s - loss: 2.6735 - val_loss: 2.8451\n",
            "108/108 - 3s - loss: 2.6934 - val_loss: 2.8974\n",
            "108/108 - 4s - loss: 2.7666 - val_loss: 2.8218\n",
            "108/108 - 4s - loss: 2.6982 - val_loss: 2.8811\n",
            "108/108 - 3s - loss: 2.6621 - val_loss: 2.8236\n",
            "108/108 - 3s - loss: 2.6475 - val_loss: 2.8381\n",
            "108/108 - 4s - loss: 2.6638 - val_loss: 2.8967\n",
            "108/108 - 4s - loss: 2.6982 - val_loss: 2.8321\n",
            "108/108 - 4s - loss: 2.7111 - val_loss: 2.8573\n",
            "108/108 - 3s - loss: 2.6834 - val_loss: 2.8170\n",
            "108/108 - 3s - loss: 2.6593 - val_loss: 2.8746\n",
            "108/108 - 3s - loss: 2.6478 - val_loss: 2.8312\n",
            "108/108 - 4s - loss: 2.6668 - val_loss: 2.8878\n",
            "108/108 - 3s - loss: 2.6207 - val_loss: 2.8855\n",
            "108/108 - 3s - loss: 2.6644 - val_loss: 2.8567\n",
            "108/108 - 4s - loss: 2.6525 - val_loss: 2.8577\n",
            "108/108 - 4s - loss: 2.6471 - val_loss: 2.8486\n",
            "108/108 - 4s - loss: 2.6250 - val_loss: 2.8044\n",
            "108/108 - 3s - loss: 2.6245 - val_loss: 2.7940\n",
            "108/108 - 4s - loss: 2.6608 - val_loss: 2.8486\n",
            "108/108 - 3s - loss: 2.6401 - val_loss: 2.8733\n",
            "108/108 - 3s - loss: 2.6910 - val_loss: 2.8766\n",
            "108/108 - 3s - loss: 2.6427 - val_loss: 2.8590\n",
            "108/108 - 3s - loss: 2.6483 - val_loss: 2.8202\n",
            "108/108 - 3s - loss: 2.6789 - val_loss: 2.8152\n",
            "108/108 - 3s - loss: 2.6731 - val_loss: 2.8322\n",
            "108/108 - 3s - loss: 2.6464 - val_loss: 2.8412\n",
            "108/108 - 3s - loss: 2.6092 - val_loss: 2.8335\n",
            "108/108 - 3s - loss: 2.6014 - val_loss: 2.8450\n",
            "108/108 - 3s - loss: 2.6183 - val_loss: 2.8892\n",
            "108/108 - 3s - loss: 2.6259 - val_loss: 2.8372\n",
            "108/108 - 3s - loss: 2.5972 - val_loss: 2.8025\n",
            "108/108 - 3s - loss: 2.6188 - val_loss: 2.8107\n",
            "108/108 - 4s - loss: 2.6196 - val_loss: 2.8670\n",
            "108/108 - 3s - loss: 2.6033 - val_loss: 2.8131\n",
            "108/108 - 3s - loss: 2.6102 - val_loss: 2.8224\n",
            "108/108 - 4s - loss: 2.5950 - val_loss: 2.8343\n",
            "108/108 - 3s - loss: 2.6087 - val_loss: 2.8202\n",
            "108/108 - 4s - loss: 2.6751 - val_loss: 2.8148\n",
            "108/108 - 3s - loss: 2.6535 - val_loss: 2.8197\n",
            "108/108 - 4s - loss: 2.6071 - val_loss: 2.8744\n",
            "108/108 - 4s - loss: 2.6009 - val_loss: 2.8318\n",
            "108/108 - 4s - loss: 2.6075 - val_loss: 2.8225\n",
            "108/108 - 4s - loss: 2.5969 - val_loss: 2.8298\n",
            "108/108 - 3s - loss: 2.5532 - val_loss: 2.8292\n",
            "108/108 - 3s - loss: 2.6091 - val_loss: 2.8923\n",
            "108/108 - 3s - loss: 2.6171 - val_loss: 2.8038\n",
            "108/108 - 3s - loss: 2.6024 - val_loss: 2.7761\n",
            "108/108 - 3s - loss: 2.6197 - val_loss: 2.7689\n",
            "108/108 - 4s - loss: 2.6076 - val_loss: 2.8004\n",
            "108/108 - 4s - loss: 2.6423 - val_loss: 2.7833\n",
            "108/108 - 4s - loss: 2.5974 - val_loss: 2.8029\n",
            "108/108 - 4s - loss: 2.5794 - val_loss: 2.8304\n",
            "108/108 - 3s - loss: 2.5801 - val_loss: 2.8625\n",
            "108/108 - 3s - loss: 2.5607 - val_loss: 2.8168\n",
            "108/108 - 3s - loss: 2.5595 - val_loss: 2.8257\n",
            "108/108 - 3s - loss: 2.5583 - val_loss: 2.8675\n",
            "108/108 - 3s - loss: 2.5982 - val_loss: 2.8784\n",
            "108/108 - 3s - loss: 2.5512 - val_loss: 2.8525\n",
            "108/108 - 4s - loss: 2.5214 - val_loss: 2.8139\n",
            "108/108 - 4s - loss: 2.5780 - val_loss: 2.7795\n",
            "108/108 - 4s - loss: 2.5553 - val_loss: 2.8425\n",
            "108/108 - 3s - loss: 2.5602 - val_loss: 2.8287\n",
            "108/108 - 3s - loss: 2.5469 - val_loss: 2.8607\n",
            "108/108 - 3s - loss: 2.5470 - val_loss: 2.8147\n",
            "108/108 - 3s - loss: 2.5639 - val_loss: 2.8424\n",
            "108/108 - 3s - loss: 2.5384 - val_loss: 2.7911\n",
            "108/108 - 4s - loss: 2.5382 - val_loss: 2.8960\n",
            "108/108 - 3s - loss: 2.5387 - val_loss: 2.8566\n",
            "108/108 - 3s - loss: 2.5360 - val_loss: 2.8877\n",
            "108/108 - 3s - loss: 2.5485 - val_loss: 2.9033\n",
            "108/108 - 3s - loss: 2.5439 - val_loss: 2.8461\n",
            "108/108 - 3s - loss: 2.5468 - val_loss: 2.8139\n",
            "108/108 - 3s - loss: 2.5360 - val_loss: 2.8441\n",
            "108/108 - 3s - loss: 2.5156 - val_loss: 2.8678\n",
            "108/108 - 3s - loss: 2.5363 - val_loss: 2.8666\n",
            "108/108 - 4s - loss: 2.5212 - val_loss: 2.7784\n",
            "108/108 - 3s - loss: 2.5242 - val_loss: 2.8489\n",
            "108/108 - 3s - loss: 2.4974 - val_loss: 2.8519\n",
            "108/108 - 3s - loss: 2.5186 - val_loss: 2.7796\n",
            "108/108 - 3s - loss: 2.5050 - val_loss: 2.8328\n",
            "108/108 - 3s - loss: 2.5096 - val_loss: 2.8856\n",
            "108/108 - 3s - loss: 2.5163 - val_loss: 2.8146\n",
            "108/108 - 3s - loss: 2.5447 - val_loss: 2.8223\n",
            "108/108 - 3s - loss: 2.5488 - val_loss: 2.9142\n",
            "108/108 - 3s - loss: 2.5531 - val_loss: 2.8309\n",
            "108/108 - 3s - loss: 2.5047 - val_loss: 2.9097\n",
            "108/108 - 4s - loss: 2.5560 - val_loss: 2.8403\n",
            "108/108 - 3s - loss: 2.5556 - val_loss: 2.8163\n",
            "108/108 - 3s - loss: 2.5071 - val_loss: 2.8872\n",
            "108/108 - 3s - loss: 2.5149 - val_loss: 2.7668\n",
            "108/108 - 3s - loss: 2.5230 - val_loss: 2.8202\n",
            "108/108 - 3s - loss: 2.5082 - val_loss: 2.8221\n",
            "108/108 - 3s - loss: 2.5300 - val_loss: 2.7947\n",
            "108/108 - 3s - loss: 2.4943 - val_loss: 2.7701\n",
            "108/108 - 3s - loss: 2.5176 - val_loss: 2.8389\n",
            "108/108 - 3s - loss: 2.5055 - val_loss: 2.8136\n",
            "108/108 - 3s - loss: 2.5440 - val_loss: 2.8399\n",
            "108/108 - 3s - loss: 2.5082 - val_loss: 2.8026\n",
            "108/108 - 4s - loss: 2.5193 - val_loss: 2.8031\n",
            "108/108 - 3s - loss: 2.5215 - val_loss: 2.7741\n",
            "108/108 - 3s - loss: 2.5187 - val_loss: 2.8588\n",
            "108/108 - 3s - loss: 2.5679 - val_loss: 2.7989\n",
            "108/108 - 3s - loss: 2.6285 - val_loss: 2.8189\n",
            "108/108 - 3s - loss: 2.5849 - val_loss: 2.8619\n",
            "108/108 - 3s - loss: 2.5525 - val_loss: 2.7744\n",
            "108/108 - 3s - loss: 2.5256 - val_loss: 2.8451\n",
            "108/108 - 3s - loss: 2.5311 - val_loss: 2.7575\n",
            "108/108 - 3s - loss: 2.5519 - val_loss: 2.8157\n",
            "108/108 - 3s - loss: 2.5305 - val_loss: 2.7862\n",
            "108/108 - 3s - loss: 2.5936 - val_loss: 2.7596\n",
            "108/108 - 3s - loss: 2.5397 - val_loss: 2.8217\n",
            "108/108 - 3s - loss: 2.5151 - val_loss: 2.7966\n",
            "108/108 - 3s - loss: 2.5292 - val_loss: 2.7912\n",
            "108/108 - 3s - loss: 2.5234 - val_loss: 2.7710\n",
            "108/108 - 3s - loss: 2.4816 - val_loss: 2.7982\n",
            "108/108 - 3s - loss: 2.4914 - val_loss: 2.7713\n",
            "108/108 - 3s - loss: 2.5014 - val_loss: 2.8150\n",
            "108/108 - 3s - loss: 2.4950 - val_loss: 2.7836\n",
            "108/108 - 4s - loss: 2.5155 - val_loss: 2.7927\n",
            "108/108 - 3s - loss: 2.5035 - val_loss: 2.7853\n",
            "108/108 - 3s - loss: 2.4616 - val_loss: 2.7580\n",
            "108/108 - 3s - loss: 2.5083 - val_loss: 2.7467\n",
            "108/108 - 3s - loss: 2.4882 - val_loss: 2.7407\n",
            "108/108 - 3s - loss: 2.5009 - val_loss: 2.7834\n",
            "108/108 - 3s - loss: 2.4950 - val_loss: 2.8344\n",
            "108/108 - 3s - loss: 2.4932 - val_loss: 2.7961\n",
            "108/108 - 3s - loss: 2.4508 - val_loss: 2.7549\n",
            "108/108 - 3s - loss: 2.4950 - val_loss: 2.7505\n",
            "108/108 - 3s - loss: 2.4634 - val_loss: 2.8250\n",
            "108/108 - 3s - loss: 2.5214 - val_loss: 2.8513\n",
            "108/108 - 3s - loss: 2.5131 - val_loss: 2.8250\n",
            "108/108 - 3s - loss: 2.4949 - val_loss: 2.8517\n",
            "108/108 - 3s - loss: 2.4656 - val_loss: 2.8186\n",
            "108/108 - 3s - loss: 2.5173 - val_loss: 2.8803\n",
            "108/108 - 4s - loss: 2.4674 - val_loss: 2.8145\n",
            "108/108 - 3s - loss: 2.4439 - val_loss: 2.8693\n",
            "108/108 - 4s - loss: 2.4716 - val_loss: 2.8390\n",
            "108/108 - 3s - loss: 2.4811 - val_loss: 2.8408\n",
            "108/108 - 3s - loss: 2.5060 - val_loss: 2.8295\n",
            "108/108 - 3s - loss: 2.4768 - val_loss: 2.7635\n",
            "108/108 - 3s - loss: 2.4461 - val_loss: 2.7542\n",
            "108/108 - 3s - loss: 2.4576 - val_loss: 2.7355\n",
            "108/108 - 3s - loss: 2.4802 - val_loss: 2.7673\n",
            "108/108 - 3s - loss: 2.4724 - val_loss: 2.8156\n",
            "108/108 - 3s - loss: 2.5049 - val_loss: 2.8757\n",
            "108/108 - 3s - loss: 2.4441 - val_loss: 2.8390\n",
            "108/108 - 3s - loss: 2.4673 - val_loss: 2.7449\n",
            "108/108 - 4s - loss: 2.4506 - val_loss: 2.8544\n",
            "108/108 - 4s - loss: 2.4381 - val_loss: 2.7818\n",
            "108/108 - 4s - loss: 2.4410 - val_loss: 2.8279\n",
            "108/108 - 3s - loss: 2.4868 - val_loss: 2.7665\n",
            "108/108 - 3s - loss: 2.5192 - val_loss: 2.7376\n",
            "108/108 - 4s - loss: 2.4410 - val_loss: 2.8039\n",
            "108/108 - 3s - loss: 2.4712 - val_loss: 2.7850\n",
            "108/108 - 3s - loss: 2.4774 - val_loss: 2.7446\n",
            "108/108 - 3s - loss: 2.4553 - val_loss: 2.8337\n",
            "108/108 - 3s - loss: 2.4870 - val_loss: 2.8288\n",
            "108/108 - 3s - loss: 2.4352 - val_loss: 2.8727\n",
            "108/108 - 3s - loss: 2.4825 - val_loss: 2.7463\n",
            "108/108 - 3s - loss: 2.4515 - val_loss: 2.8012\n",
            "108/108 - 3s - loss: 2.4617 - val_loss: 2.8080\n",
            "108/108 - 3s - loss: 2.4349 - val_loss: 2.8142\n",
            "108/108 - 3s - loss: 2.4873 - val_loss: 2.7127\n",
            "108/108 - 3s - loss: 2.4613 - val_loss: 2.8106\n",
            "108/108 - 3s - loss: 2.4536 - val_loss: 2.7959\n",
            "108/108 - 3s - loss: 2.4193 - val_loss: 2.8529\n",
            "108/108 - 3s - loss: 2.4345 - val_loss: 2.8353\n",
            "108/108 - 3s - loss: 2.4389 - val_loss: 2.7824\n",
            "108/108 - 3s - loss: 2.4273 - val_loss: 2.7730\n",
            "108/108 - 3s - loss: 2.4192 - val_loss: 2.7644\n",
            "108/108 - 4s - loss: 2.4192 - val_loss: 2.7877\n",
            "108/108 - 3s - loss: 2.4869 - val_loss: 2.7525\n",
            "108/108 - 3s - loss: 2.4597 - val_loss: 2.8034\n",
            "108/108 - 3s - loss: 2.4760 - val_loss: 2.8472\n",
            "108/108 - 3s - loss: 2.4380 - val_loss: 2.8345\n",
            "108/108 - 4s - loss: 2.4338 - val_loss: 2.7999\n",
            "108/108 - 3s - loss: 2.4870 - val_loss: 2.7862\n",
            "108/108 - 3s - loss: 2.4634 - val_loss: 2.7573\n",
            "108/108 - 3s - loss: 2.4691 - val_loss: 2.7520\n",
            "108/108 - 3s - loss: 2.4211 - val_loss: 2.8038\n",
            "108/108 - 3s - loss: 2.4446 - val_loss: 2.7839\n",
            "108/108 - 3s - loss: 2.4161 - val_loss: 2.8358\n",
            "108/108 - 3s - loss: 2.4184 - val_loss: 2.8155\n",
            "108/108 - 3s - loss: 2.4189 - val_loss: 2.7572\n",
            "108/108 - 3s - loss: 2.4167 - val_loss: 2.8325\n",
            "108/108 - 3s - loss: 2.4529 - val_loss: 2.7815\n",
            "108/108 - 3s - loss: 2.4040 - val_loss: 2.7698\n",
            "108/108 - 3s - loss: 2.3865 - val_loss: 2.7741\n",
            "108/108 - 3s - loss: 2.4052 - val_loss: 2.8237\n",
            "108/108 - 3s - loss: 2.4353 - val_loss: 2.7563\n",
            "108/108 - 3s - loss: 2.3869 - val_loss: 2.7779\n",
            "108/108 - 3s - loss: 2.4285 - val_loss: 2.8255\n",
            "108/108 - 3s - loss: 2.3827 - val_loss: 2.7207\n",
            "108/108 - 3s - loss: 2.3931 - val_loss: 2.7689\n",
            "108/108 - 3s - loss: 2.4142 - val_loss: 2.7219\n",
            "108/108 - 3s - loss: 2.4013 - val_loss: 2.7851\n",
            "108/108 - 3s - loss: 2.4141 - val_loss: 2.7334\n",
            "108/108 - 3s - loss: 2.4118 - val_loss: 2.7675\n",
            "108/108 - 3s - loss: 2.4087 - val_loss: 2.8139\n",
            "108/108 - 3s - loss: 2.4103 - val_loss: 2.8330\n",
            "108/108 - 4s - loss: 2.5189 - val_loss: 2.7632\n",
            "108/108 - 3s - loss: 2.4522 - val_loss: 2.7840\n",
            "108/108 - 3s - loss: 2.4260 - val_loss: 2.7760\n",
            "108/108 - 3s - loss: 2.4120 - val_loss: 2.7917\n",
            "108/108 - 3s - loss: 2.4117 - val_loss: 2.8436\n",
            "108/108 - 3s - loss: 2.4277 - val_loss: 2.7707\n",
            "108/108 - 3s - loss: 2.3758 - val_loss: 2.8380\n",
            "108/108 - 3s - loss: 2.3918 - val_loss: 2.7902\n",
            "108/108 - 3s - loss: 2.4074 - val_loss: 2.7984\n",
            "108/108 - 3s - loss: 2.3911 - val_loss: 2.8042\n",
            "108/108 - 3s - loss: 2.4024 - val_loss: 2.7980\n",
            "108/108 - 3s - loss: 2.3853 - val_loss: 2.8294\n",
            "108/108 - 3s - loss: 2.4064 - val_loss: 2.7817\n",
            "108/108 - 3s - loss: 2.4143 - val_loss: 2.7386\n",
            "108/108 - 3s - loss: 2.4138 - val_loss: 2.7673\n",
            "108/108 - 3s - loss: 2.4106 - val_loss: 2.8329\n",
            "108/108 - 3s - loss: 2.3636 - val_loss: 2.8678\n",
            "108/108 - 3s - loss: 2.3739 - val_loss: 2.7488\n",
            "108/108 - 3s - loss: 2.3764 - val_loss: 2.8186\n",
            "108/108 - 3s - loss: 2.3923 - val_loss: 2.8199\n",
            "108/108 - 3s - loss: 2.3633 - val_loss: 2.8064\n",
            "108/108 - 3s - loss: 2.3565 - val_loss: 2.7654\n",
            "108/108 - 4s - loss: 2.3537 - val_loss: 2.8351\n",
            "108/108 - 4s - loss: 2.3590 - val_loss: 2.7453\n",
            "108/108 - 3s - loss: 2.3606 - val_loss: 2.7762\n",
            "108/108 - 3s - loss: 2.4012 - val_loss: 2.8301\n",
            "108/108 - 3s - loss: 2.3963 - val_loss: 2.8131\n",
            "108/108 - 3s - loss: 2.3962 - val_loss: 2.7905\n",
            "108/108 - 3s - loss: 2.3791 - val_loss: 2.8452\n",
            "108/108 - 3s - loss: 2.3914 - val_loss: 2.8292\n",
            "108/108 - 3s - loss: 2.3568 - val_loss: 2.7819\n",
            "108/108 - 3s - loss: 2.3771 - val_loss: 2.7446\n",
            "108/108 - 3s - loss: 2.3821 - val_loss: 2.8195\n",
            "108/108 - 3s - loss: 2.3594 - val_loss: 2.8028\n",
            "108/108 - 3s - loss: 2.3607 - val_loss: 2.8139\n",
            "108/108 - 3s - loss: 2.3884 - val_loss: 2.8392\n",
            "108/108 - 3s - loss: 2.3688 - val_loss: 2.7705\n",
            "108/108 - 4s - loss: 2.3351 - val_loss: 2.7570\n",
            "108/108 - 4s - loss: 2.3729 - val_loss: 2.8727\n",
            "108/108 - 3s - loss: 2.3912 - val_loss: 2.8129\n",
            "108/108 - 3s - loss: 2.4173 - val_loss: 2.8148\n",
            "108/108 - 3s - loss: 2.4240 - val_loss: 2.8115\n",
            "108/108 - 3s - loss: 2.3699 - val_loss: 2.8378\n",
            "108/108 - 3s - loss: 2.3697 - val_loss: 2.8042\n",
            "108/108 - 3s - loss: 2.3468 - val_loss: 2.7649\n",
            "108/108 - 3s - loss: 2.3409 - val_loss: 2.8145\n",
            "108/108 - 3s - loss: 2.4220 - val_loss: 2.9614\n",
            "108/108 - 3s - loss: 2.4862 - val_loss: 2.8156\n",
            "108/108 - 3s - loss: 2.3685 - val_loss: 2.8117\n",
            "108/108 - 3s - loss: 2.3669 - val_loss: 2.7428\n",
            "108/108 - 3s - loss: 2.3456 - val_loss: 2.8125\n",
            "108/108 - 3s - loss: 2.3560 - val_loss: 2.8920\n",
            "108/108 - 3s - loss: 2.3552 - val_loss: 2.8253\n",
            "108/108 - 3s - loss: 2.3355 - val_loss: 2.8185\n",
            "108/108 - 3s - loss: 2.3449 - val_loss: 2.8428\n",
            "108/108 - 3s - loss: 2.3356 - val_loss: 2.7878\n",
            "108/108 - 4s - loss: 2.3155 - val_loss: 2.7912\n",
            "108/108 - 4s - loss: 2.2990 - val_loss: 2.8343\n",
            "108/108 - 3s - loss: 2.3705 - val_loss: 2.7786\n",
            "108/108 - 3s - loss: 2.3942 - val_loss: 2.7655\n",
            "108/108 - 3s - loss: 2.3306 - val_loss: 2.7821\n",
            "108/108 - 3s - loss: 2.3205 - val_loss: 2.7408\n",
            "108/108 - 3s - loss: 2.3272 - val_loss: 2.8007\n",
            "108/108 - 3s - loss: 2.2946 - val_loss: 2.7796\n",
            "108/108 - 3s - loss: 2.3356 - val_loss: 2.7224\n",
            "108/108 - 3s - loss: 2.3673 - val_loss: 2.8236\n",
            "108/108 - 3s - loss: 2.3471 - val_loss: 2.7927\n",
            "108/108 - 3s - loss: 2.3169 - val_loss: 2.8078\n",
            "108/108 - 3s - loss: 2.3470 - val_loss: 2.8235\n",
            "108/108 - 3s - loss: 2.3310 - val_loss: 2.8479\n",
            "108/108 - 3s - loss: 2.3504 - val_loss: 2.8039\n",
            "108/108 - 3s - loss: 2.3433 - val_loss: 2.8199\n",
            "108/108 - 3s - loss: 2.3282 - val_loss: 2.7781\n",
            "108/108 - 3s - loss: 2.3535 - val_loss: 2.7544\n",
            "108/108 - 3s - loss: 2.3389 - val_loss: 2.7751\n",
            "108/108 - 3s - loss: 2.3259 - val_loss: 2.8166\n",
            "108/108 - 3s - loss: 2.3431 - val_loss: 2.8690\n",
            "108/108 - 3s - loss: 2.3542 - val_loss: 2.8064\n",
            "108/108 - 3s - loss: 2.3534 - val_loss: 2.7616\n",
            "108/108 - 3s - loss: 2.3358 - val_loss: 2.8146\n",
            "108/108 - 3s - loss: 2.3477 - val_loss: 2.7489\n",
            "108/108 - 3s - loss: 2.3520 - val_loss: 2.8042\n",
            "108/108 - 3s - loss: 2.3375 - val_loss: 2.7675\n",
            "108/108 - 3s - loss: 2.3238 - val_loss: 2.8134\n",
            "108/108 - 3s - loss: 2.2987 - val_loss: 2.7674\n",
            "108/108 - 3s - loss: 2.3099 - val_loss: 2.7848\n",
            "108/108 - 3s - loss: 2.3215 - val_loss: 2.7832\n",
            "108/108 - 3s - loss: 2.3075 - val_loss: 2.7839\n",
            "108/108 - 3s - loss: 2.3311 - val_loss: 2.7843\n",
            "108/108 - 3s - loss: 2.3404 - val_loss: 2.8069\n",
            "108/108 - 3s - loss: 2.3120 - val_loss: 2.7492\n",
            "108/108 - 3s - loss: 2.3040 - val_loss: 2.7567\n",
            "108/108 - 3s - loss: 2.3456 - val_loss: 2.7246\n",
            "108/108 - 4s - loss: 2.3257 - val_loss: 2.8218\n",
            "108/108 - 3s - loss: 2.3043 - val_loss: 2.7378\n",
            "108/108 - 3s - loss: 2.3069 - val_loss: 2.6818\n",
            "108/108 - 3s - loss: 2.3041 - val_loss: 2.7216\n",
            "108/108 - 3s - loss: 2.2948 - val_loss: 2.7906\n",
            "108/108 - 3s - loss: 2.2951 - val_loss: 2.7728\n",
            "108/108 - 3s - loss: 2.3172 - val_loss: 2.8185\n",
            "108/108 - 3s - loss: 2.3064 - val_loss: 2.8661\n",
            "108/108 - 4s - loss: 2.3217 - val_loss: 2.8020\n",
            "108/108 - 4s - loss: 2.2950 - val_loss: 2.8273\n",
            "108/108 - 3s - loss: 2.2837 - val_loss: 2.7961\n",
            "108/108 - 3s - loss: 2.3158 - val_loss: 2.7703\n",
            "108/108 - 3s - loss: 2.3156 - val_loss: 2.7732\n",
            "108/108 - 3s - loss: 2.3059 - val_loss: 2.8117\n",
            "108/108 - 3s - loss: 2.3210 - val_loss: 2.7487\n",
            "108/108 - 4s - loss: 2.2998 - val_loss: 2.7344\n",
            "108/108 - 3s - loss: 2.3072 - val_loss: 2.8725\n",
            "108/108 - 3s - loss: 2.3297 - val_loss: 2.8353\n",
            "108/108 - 3s - loss: 2.3101 - val_loss: 2.7787\n",
            "108/108 - 3s - loss: 2.3146 - val_loss: 2.7588\n",
            "108/108 - 3s - loss: 2.2992 - val_loss: 2.8132\n",
            "108/108 - 3s - loss: 2.3034 - val_loss: 2.7776\n",
            "108/108 - 3s - loss: 2.3106 - val_loss: 2.7679\n",
            "108/108 - 3s - loss: 2.3143 - val_loss: 2.8555\n",
            "108/108 - 3s - loss: 2.3044 - val_loss: 2.8962\n",
            "108/108 - 3s - loss: 2.2985 - val_loss: 2.7652\n",
            "108/108 - 3s - loss: 2.3237 - val_loss: 2.7320\n",
            "108/108 - 3s - loss: 2.3108 - val_loss: 2.7949\n",
            "108/108 - 3s - loss: 2.3621 - val_loss: 2.7681\n",
            "108/108 - 3s - loss: 2.2836 - val_loss: 2.7888\n",
            "108/108 - 3s - loss: 2.2908 - val_loss: 2.7688\n",
            "108/108 - 3s - loss: 2.3019 - val_loss: 2.7357\n",
            "108/108 - 4s - loss: 2.2845 - val_loss: 2.7538\n",
            "108/108 - 4s - loss: 2.2748 - val_loss: 2.7144\n",
            "108/108 - 4s - loss: 2.2788 - val_loss: 2.7872\n",
            "108/108 - 4s - loss: 2.2801 - val_loss: 2.7140\n",
            "108/108 - 3s - loss: 2.2634 - val_loss: 2.8156\n",
            "108/108 - 3s - loss: 2.3336 - val_loss: 2.7756\n",
            "108/108 - 3s - loss: 2.3158 - val_loss: 2.7387\n",
            "108/108 - 3s - loss: 2.2972 - val_loss: 2.7476\n",
            "108/108 - 3s - loss: 2.2913 - val_loss: 2.7128\n",
            "108/108 - 3s - loss: 2.3083 - val_loss: 2.6459\n",
            "108/108 - 3s - loss: 2.2803 - val_loss: 2.7005\n",
            "108/108 - 3s - loss: 2.3214 - val_loss: 2.7182\n",
            "108/108 - 3s - loss: 2.2958 - val_loss: 2.7333\n",
            "108/108 - 3s - loss: 2.2901 - val_loss: 2.7314\n",
            "108/108 - 3s - loss: 2.2773 - val_loss: 2.7821\n",
            "108/108 - 3s - loss: 2.3213 - val_loss: 2.6887\n",
            "108/108 - 3s - loss: 2.2728 - val_loss: 2.7492\n",
            "108/108 - 3s - loss: 2.2719 - val_loss: 2.7230\n",
            "108/108 - 3s - loss: 2.2505 - val_loss: 2.7324\n",
            "108/108 - 3s - loss: 2.2763 - val_loss: 2.7349\n",
            "108/108 - 3s - loss: 2.2890 - val_loss: 2.7842\n",
            "108/108 - 3s - loss: 2.2966 - val_loss: 2.7852\n",
            "108/108 - 3s - loss: 2.2921 - val_loss: 2.7923\n",
            "108/108 - 3s - loss: 2.3179 - val_loss: 2.7848\n",
            "108/108 - 3s - loss: 2.3278 - val_loss: 2.7849\n",
            "108/108 - 3s - loss: 2.3006 - val_loss: 2.7787\n",
            "108/108 - 3s - loss: 2.2684 - val_loss: 2.7332\n",
            "108/108 - 3s - loss: 2.2867 - val_loss: 2.7079\n",
            "108/108 - 3s - loss: 2.2743 - val_loss: 2.7326\n",
            "108/108 - 3s - loss: 2.2624 - val_loss: 2.8341\n",
            "108/108 - 3s - loss: 2.2506 - val_loss: 2.7215\n",
            "108/108 - 3s - loss: 2.3016 - val_loss: 2.7465\n",
            "108/108 - 3s - loss: 2.2804 - val_loss: 2.7369\n",
            "108/108 - 4s - loss: 2.3113 - val_loss: 2.7577\n",
            "108/108 - 4s - loss: 2.2570 - val_loss: 2.7368\n",
            "108/108 - 3s - loss: 2.2664 - val_loss: 2.7332\n",
            "108/108 - 3s - loss: 2.2473 - val_loss: 2.7057\n",
            "108/108 - 3s - loss: 2.2488 - val_loss: 2.6865\n",
            "108/108 - 3s - loss: 2.2630 - val_loss: 2.7394\n",
            "108/108 - 3s - loss: 2.2861 - val_loss: 2.7535\n",
            "108/108 - 3s - loss: 2.2565 - val_loss: 2.7189\n",
            "108/108 - 3s - loss: 2.2290 - val_loss: 2.7693\n",
            "108/108 - 3s - loss: 2.2431 - val_loss: 2.7729\n",
            "108/108 - 3s - loss: 2.2255 - val_loss: 2.7835\n",
            "108/108 - 3s - loss: 2.2461 - val_loss: 2.7689\n",
            "108/108 - 3s - loss: 2.2519 - val_loss: 2.7479\n",
            "108/108 - 3s - loss: 2.2412 - val_loss: 2.7054\n",
            "0.5\n",
            "108/108 - 7s - loss: 8.2628 - val_loss: 6.9149\n",
            "108/108 - 3s - loss: 6.9949 - val_loss: 5.9637\n",
            "108/108 - 3s - loss: 6.1665 - val_loss: 5.2749\n",
            "108/108 - 3s - loss: 5.5418 - val_loss: 4.7717\n",
            "108/108 - 3s - loss: 5.0646 - val_loss: 4.4074\n",
            "108/108 - 3s - loss: 4.7623 - val_loss: 4.1670\n",
            "108/108 - 4s - loss: 4.5197 - val_loss: 4.0166\n",
            "108/108 - 3s - loss: 4.3915 - val_loss: 3.9294\n",
            "108/108 - 3s - loss: 4.3054 - val_loss: 3.8875\n",
            "108/108 - 3s - loss: 4.2632 - val_loss: 3.8681\n",
            "108/108 - 3s - loss: 4.2172 - val_loss: 3.8602\n",
            "108/108 - 3s - loss: 4.2039 - val_loss: 3.8578\n",
            "108/108 - 3s - loss: 4.2087 - val_loss: 3.8568\n",
            "108/108 - 3s - loss: 4.2136 - val_loss: 3.8584\n",
            "108/108 - 3s - loss: 4.1729 - val_loss: 3.8594\n",
            "108/108 - 3s - loss: 4.1846 - val_loss: 3.8607\n",
            "108/108 - 3s - loss: 4.1706 - val_loss: 3.8631\n",
            "108/108 - 3s - loss: 4.2075 - val_loss: 3.8630\n",
            "108/108 - 3s - loss: 4.2243 - val_loss: 3.8635\n",
            "108/108 - 4s - loss: 4.1918 - val_loss: 3.8653\n",
            "108/108 - 3s - loss: 4.2182 - val_loss: 3.8657\n",
            "108/108 - 4s - loss: 4.2078 - val_loss: 3.8658\n",
            "108/108 - 3s - loss: 4.1769 - val_loss: 3.8656\n",
            "108/108 - 4s - loss: 4.1993 - val_loss: 3.8671\n",
            "108/108 - 3s - loss: 4.2260 - val_loss: 3.8657\n",
            "108/108 - 3s - loss: 4.1810 - val_loss: 3.8671\n",
            "108/108 - 3s - loss: 4.1933 - val_loss: 3.8673\n",
            "108/108 - 3s - loss: 4.2163 - val_loss: 3.8427\n",
            "108/108 - 3s - loss: 3.9882 - val_loss: 3.4988\n",
            "108/108 - 3s - loss: 3.8217 - val_loss: 3.3900\n",
            "108/108 - 3s - loss: 3.6470 - val_loss: 3.2704\n",
            "108/108 - 3s - loss: 3.5114 - val_loss: 3.1983\n",
            "108/108 - 3s - loss: 3.4326 - val_loss: 3.0841\n",
            "108/108 - 3s - loss: 3.3628 - val_loss: 3.0270\n",
            "108/108 - 3s - loss: 3.2487 - val_loss: 3.0134\n",
            "108/108 - 3s - loss: 3.2089 - val_loss: 2.9788\n",
            "108/108 - 3s - loss: 3.1519 - val_loss: 2.9395\n",
            "108/108 - 3s - loss: 3.1273 - val_loss: 2.9225\n",
            "108/108 - 3s - loss: 3.0779 - val_loss: 2.9070\n",
            "108/108 - 3s - loss: 3.0509 - val_loss: 2.8983\n",
            "108/108 - 4s - loss: 3.0320 - val_loss: 2.8723\n",
            "108/108 - 4s - loss: 3.0466 - val_loss: 2.8665\n",
            "108/108 - 4s - loss: 3.0433 - val_loss: 2.8360\n",
            "108/108 - 3s - loss: 3.0083 - val_loss: 2.8398\n",
            "108/108 - 3s - loss: 2.9973 - val_loss: 2.8434\n",
            "108/108 - 3s - loss: 2.9636 - val_loss: 2.8252\n",
            "108/108 - 3s - loss: 3.0141 - val_loss: 2.8373\n",
            "108/108 - 3s - loss: 2.9820 - val_loss: 2.8435\n",
            "108/108 - 3s - loss: 2.9782 - val_loss: 2.8059\n",
            "108/108 - 3s - loss: 2.9840 - val_loss: 2.7979\n",
            "108/108 - 3s - loss: 2.9487 - val_loss: 2.7807\n",
            "108/108 - 3s - loss: 2.9316 - val_loss: 2.7858\n",
            "108/108 - 3s - loss: 2.9255 - val_loss: 2.7775\n",
            "108/108 - 4s - loss: 2.9600 - val_loss: 2.8080\n",
            "108/108 - 3s - loss: 2.9244 - val_loss: 2.8064\n",
            "108/108 - 3s - loss: 2.9437 - val_loss: 2.7620\n",
            "108/108 - 3s - loss: 2.9269 - val_loss: 2.7704\n",
            "108/108 - 3s - loss: 2.9123 - val_loss: 2.7967\n",
            "108/108 - 3s - loss: 2.9382 - val_loss: 2.8621\n",
            "108/108 - 3s - loss: 2.9208 - val_loss: 2.7986\n",
            "108/108 - 3s - loss: 2.8723 - val_loss: 2.7880\n",
            "108/108 - 3s - loss: 2.9235 - val_loss: 2.7571\n",
            "108/108 - 3s - loss: 2.8924 - val_loss: 2.7596\n",
            "108/108 - 3s - loss: 2.8856 - val_loss: 2.7638\n",
            "108/108 - 3s - loss: 2.9120 - val_loss: 2.7630\n",
            "108/108 - 3s - loss: 2.8751 - val_loss: 2.7481\n",
            "108/108 - 3s - loss: 2.8836 - val_loss: 2.7851\n",
            "108/108 - 3s - loss: 2.8889 - val_loss: 2.7544\n",
            "108/108 - 3s - loss: 2.8480 - val_loss: 2.7324\n",
            "108/108 - 3s - loss: 2.8692 - val_loss: 2.7641\n",
            "108/108 - 4s - loss: 2.9077 - val_loss: 2.7575\n",
            "108/108 - 3s - loss: 2.8673 - val_loss: 2.7669\n",
            "108/108 - 3s - loss: 2.8842 - val_loss: 2.7298\n",
            "108/108 - 3s - loss: 2.8414 - val_loss: 2.7433\n",
            "108/108 - 3s - loss: 2.8894 - val_loss: 2.7653\n",
            "108/108 - 3s - loss: 2.8464 - val_loss: 2.7312\n",
            "108/108 - 3s - loss: 2.8378 - val_loss: 2.7619\n",
            "108/108 - 4s - loss: 2.8191 - val_loss: 2.7162\n",
            "108/108 - 3s - loss: 2.8337 - val_loss: 2.7508\n",
            "108/108 - 3s - loss: 2.8542 - val_loss: 2.7315\n",
            "108/108 - 3s - loss: 2.7869 - val_loss: 2.7730\n",
            "108/108 - 3s - loss: 2.8191 - val_loss: 2.7489\n",
            "108/108 - 3s - loss: 2.8413 - val_loss: 2.7446\n",
            "108/108 - 3s - loss: 2.8317 - val_loss: 2.7421\n",
            "108/108 - 3s - loss: 2.7886 - val_loss: 2.7343\n",
            "108/108 - 3s - loss: 2.8281 - val_loss: 2.7473\n",
            "108/108 - 3s - loss: 2.7710 - val_loss: 2.7363\n",
            "108/108 - 3s - loss: 2.8335 - val_loss: 2.7707\n",
            "108/108 - 3s - loss: 2.8041 - val_loss: 2.8166\n",
            "108/108 - 3s - loss: 2.7999 - val_loss: 2.7413\n",
            "108/108 - 3s - loss: 2.8170 - val_loss: 2.7200\n",
            "108/108 - 3s - loss: 2.8038 - val_loss: 2.7177\n",
            "108/108 - 3s - loss: 2.8227 - val_loss: 2.7535\n",
            "108/108 - 3s - loss: 2.8483 - val_loss: 2.7252\n",
            "108/108 - 3s - loss: 2.8144 - val_loss: 2.7219\n",
            "108/108 - 3s - loss: 2.8056 - val_loss: 2.7076\n",
            "108/108 - 3s - loss: 2.7555 - val_loss: 2.7450\n",
            "108/108 - 3s - loss: 2.7676 - val_loss: 2.7104\n",
            "108/108 - 3s - loss: 2.7772 - val_loss: 2.7218\n",
            "108/108 - 3s - loss: 2.7519 - val_loss: 2.7056\n",
            "108/108 - 3s - loss: 2.7984 - val_loss: 2.7433\n",
            "108/108 - 3s - loss: 2.7643 - val_loss: 2.7536\n",
            "108/108 - 3s - loss: 2.7865 - val_loss: 2.7287\n",
            "108/108 - 3s - loss: 2.7733 - val_loss: 2.7892\n",
            "108/108 - 3s - loss: 2.7635 - val_loss: 2.7135\n",
            "108/108 - 3s - loss: 2.7808 - val_loss: 2.7600\n",
            "108/108 - 3s - loss: 2.7696 - val_loss: 2.7294\n",
            "108/108 - 4s - loss: 2.7453 - val_loss: 2.7281\n",
            "108/108 - 3s - loss: 2.7749 - val_loss: 2.7507\n",
            "108/108 - 3s - loss: 2.7806 - val_loss: 2.7446\n",
            "108/108 - 3s - loss: 2.7432 - val_loss: 2.6777\n",
            "108/108 - 3s - loss: 2.7520 - val_loss: 2.7277\n",
            "108/108 - 3s - loss: 2.7605 - val_loss: 2.7553\n",
            "108/108 - 3s - loss: 2.7102 - val_loss: 2.7240\n",
            "108/108 - 3s - loss: 2.7667 - val_loss: 2.7338\n",
            "108/108 - 3s - loss: 2.7309 - val_loss: 2.7114\n",
            "108/108 - 3s - loss: 2.7579 - val_loss: 2.7192\n",
            "108/108 - 3s - loss: 2.7462 - val_loss: 2.7008\n",
            "108/108 - 3s - loss: 2.7303 - val_loss: 2.7727\n",
            "108/108 - 3s - loss: 2.7208 - val_loss: 2.7209\n",
            "108/108 - 3s - loss: 2.6953 - val_loss: 2.7747\n",
            "108/108 - 3s - loss: 2.7345 - val_loss: 2.7707\n",
            "108/108 - 3s - loss: 2.7192 - val_loss: 2.7865\n",
            "108/108 - 3s - loss: 2.7313 - val_loss: 2.7608\n",
            "108/108 - 3s - loss: 2.7231 - val_loss: 2.7126\n",
            "108/108 - 3s - loss: 2.7399 - val_loss: 2.6914\n",
            "108/108 - 3s - loss: 2.7207 - val_loss: 2.7108\n",
            "108/108 - 3s - loss: 2.7180 - val_loss: 2.6955\n",
            "108/108 - 4s - loss: 2.6810 - val_loss: 2.7144\n",
            "108/108 - 4s - loss: 2.7643 - val_loss: 2.6635\n",
            "108/108 - 4s - loss: 2.7047 - val_loss: 2.7260\n",
            "108/108 - 4s - loss: 2.6935 - val_loss: 2.7205\n",
            "108/108 - 3s - loss: 2.6469 - val_loss: 2.7306\n",
            "108/108 - 3s - loss: 2.6863 - val_loss: 2.6903\n",
            "108/108 - 3s - loss: 2.7232 - val_loss: 2.7211\n",
            "108/108 - 3s - loss: 2.6594 - val_loss: 2.6985\n",
            "108/108 - 3s - loss: 2.6906 - val_loss: 2.6959\n",
            "108/108 - 3s - loss: 2.7050 - val_loss: 2.6796\n",
            "108/108 - 3s - loss: 2.6948 - val_loss: 2.6771\n",
            "108/108 - 3s - loss: 2.6661 - val_loss: 2.7066\n",
            "108/108 - 3s - loss: 2.6499 - val_loss: 2.7168\n",
            "108/108 - 3s - loss: 2.6401 - val_loss: 2.7585\n",
            "108/108 - 3s - loss: 2.6930 - val_loss: 2.7289\n",
            "108/108 - 3s - loss: 2.6678 - val_loss: 2.7546\n",
            "108/108 - 3s - loss: 2.6524 - val_loss: 2.6870\n",
            "108/108 - 3s - loss: 2.7189 - val_loss: 2.6633\n",
            "108/108 - 3s - loss: 2.6961 - val_loss: 2.7260\n",
            "108/108 - 3s - loss: 2.6765 - val_loss: 2.7366\n",
            "108/108 - 3s - loss: 2.6548 - val_loss: 2.7088\n",
            "108/108 - 3s - loss: 2.6651 - val_loss: 2.7312\n",
            "108/108 - 3s - loss: 2.6726 - val_loss: 2.6935\n",
            "108/108 - 3s - loss: 2.7067 - val_loss: 2.7167\n",
            "108/108 - 3s - loss: 2.6416 - val_loss: 2.7250\n",
            "108/108 - 3s - loss: 2.6437 - val_loss: 2.7548\n",
            "108/108 - 3s - loss: 2.6365 - val_loss: 2.7254\n",
            "108/108 - 3s - loss: 2.6520 - val_loss: 2.7197\n",
            "108/108 - 3s - loss: 2.6299 - val_loss: 2.7133\n",
            "108/108 - 3s - loss: 2.6664 - val_loss: 2.7550\n",
            "108/108 - 3s - loss: 2.6886 - val_loss: 2.6997\n",
            "108/108 - 3s - loss: 2.6877 - val_loss: 2.6990\n",
            "108/108 - 3s - loss: 2.6231 - val_loss: 2.7329\n",
            "108/108 - 3s - loss: 2.6809 - val_loss: 2.6808\n",
            "108/108 - 3s - loss: 2.6583 - val_loss: 2.6821\n",
            "108/108 - 3s - loss: 2.6383 - val_loss: 2.7261\n",
            "108/108 - 4s - loss: 2.6434 - val_loss: 2.7026\n",
            "108/108 - 3s - loss: 2.6441 - val_loss: 2.6980\n",
            "108/108 - 3s - loss: 2.6290 - val_loss: 2.7664\n",
            "108/108 - 3s - loss: 2.6255 - val_loss: 2.7192\n",
            "108/108 - 3s - loss: 2.6471 - val_loss: 2.7647\n",
            "108/108 - 3s - loss: 2.6072 - val_loss: 2.7440\n",
            "108/108 - 3s - loss: 2.6236 - val_loss: 2.7134\n",
            "108/108 - 3s - loss: 2.6008 - val_loss: 2.6745\n",
            "108/108 - 3s - loss: 2.6125 - val_loss: 2.7036\n",
            "108/108 - 3s - loss: 2.6363 - val_loss: 2.7345\n",
            "108/108 - 3s - loss: 2.6288 - val_loss: 2.6885\n",
            "108/108 - 3s - loss: 2.6071 - val_loss: 2.7237\n",
            "108/108 - 3s - loss: 2.6286 - val_loss: 2.7050\n",
            "108/108 - 3s - loss: 2.6155 - val_loss: 2.7730\n",
            "108/108 - 3s - loss: 2.5930 - val_loss: 2.7069\n",
            "108/108 - 3s - loss: 2.6496 - val_loss: 2.7009\n",
            "108/108 - 3s - loss: 2.6546 - val_loss: 2.6739\n",
            "108/108 - 3s - loss: 2.6224 - val_loss: 2.6795\n",
            "108/108 - 3s - loss: 2.6023 - val_loss: 2.7005\n",
            "108/108 - 3s - loss: 2.5933 - val_loss: 2.6985\n",
            "108/108 - 3s - loss: 2.6253 - val_loss: 2.7103\n",
            "108/108 - 3s - loss: 2.5699 - val_loss: 2.7173\n",
            "108/108 - 3s - loss: 2.6247 - val_loss: 2.7799\n",
            "108/108 - 3s - loss: 2.5836 - val_loss: 2.7504\n",
            "108/108 - 3s - loss: 2.5942 - val_loss: 2.7416\n",
            "108/108 - 3s - loss: 2.5773 - val_loss: 2.7948\n",
            "108/108 - 4s - loss: 2.5645 - val_loss: 2.7516\n",
            "108/108 - 4s - loss: 2.6233 - val_loss: 2.7237\n",
            "108/108 - 4s - loss: 2.5649 - val_loss: 2.7789\n",
            "108/108 - 3s - loss: 2.5944 - val_loss: 2.7255\n",
            "108/108 - 3s - loss: 2.6128 - val_loss: 2.7482\n",
            "108/108 - 3s - loss: 2.6190 - val_loss: 2.7148\n",
            "108/108 - 3s - loss: 2.5965 - val_loss: 2.7340\n",
            "108/108 - 3s - loss: 2.5903 - val_loss: 2.7106\n",
            "108/108 - 3s - loss: 2.5798 - val_loss: 2.7450\n",
            "108/108 - 3s - loss: 2.5978 - val_loss: 2.7190\n",
            "108/108 - 4s - loss: 2.5921 - val_loss: 2.7926\n",
            "108/108 - 3s - loss: 2.5520 - val_loss: 2.8214\n",
            "108/108 - 3s - loss: 2.5940 - val_loss: 2.7435\n",
            "108/108 - 3s - loss: 2.5815 - val_loss: 2.7646\n",
            "108/108 - 3s - loss: 2.6359 - val_loss: 2.7489\n",
            "108/108 - 3s - loss: 2.5937 - val_loss: 2.8025\n",
            "108/108 - 3s - loss: 2.5894 - val_loss: 2.7683\n",
            "108/108 - 3s - loss: 2.5699 - val_loss: 2.7537\n",
            "108/108 - 3s - loss: 2.5580 - val_loss: 2.7518\n",
            "108/108 - 3s - loss: 2.5527 - val_loss: 2.7670\n",
            "108/108 - 3s - loss: 2.5661 - val_loss: 2.7699\n",
            "108/108 - 3s - loss: 2.5511 - val_loss: 2.7763\n",
            "108/108 - 3s - loss: 2.5299 - val_loss: 2.8174\n",
            "108/108 - 3s - loss: 2.5562 - val_loss: 2.7311\n",
            "108/108 - 3s - loss: 2.5445 - val_loss: 2.7383\n",
            "108/108 - 3s - loss: 2.5403 - val_loss: 2.7805\n",
            "108/108 - 3s - loss: 2.5590 - val_loss: 2.7341\n",
            "108/108 - 3s - loss: 2.5782 - val_loss: 2.7386\n",
            "108/108 - 4s - loss: 2.5488 - val_loss: 2.7462\n",
            "108/108 - 4s - loss: 2.5545 - val_loss: 2.8021\n",
            "108/108 - 4s - loss: 2.5572 - val_loss: 2.7363\n",
            "108/108 - 3s - loss: 2.5360 - val_loss: 2.7652\n",
            "108/108 - 3s - loss: 2.5421 - val_loss: 2.7379\n",
            "108/108 - 3s - loss: 2.5457 - val_loss: 2.7228\n",
            "108/108 - 3s - loss: 2.5359 - val_loss: 2.7846\n",
            "108/108 - 3s - loss: 2.5455 - val_loss: 2.7256\n",
            "108/108 - 3s - loss: 2.5590 - val_loss: 2.7514\n",
            "108/108 - 3s - loss: 2.5588 - val_loss: 2.8177\n",
            "108/108 - 3s - loss: 2.5561 - val_loss: 2.7094\n",
            "108/108 - 3s - loss: 2.5346 - val_loss: 2.7566\n",
            "108/108 - 3s - loss: 2.5639 - val_loss: 2.8049\n",
            "108/108 - 3s - loss: 2.5652 - val_loss: 2.6898\n",
            "108/108 - 3s - loss: 2.5532 - val_loss: 2.7825\n",
            "108/108 - 3s - loss: 2.5411 - val_loss: 2.7553\n",
            "108/108 - 3s - loss: 2.5125 - val_loss: 2.7429\n",
            "108/108 - 3s - loss: 2.5294 - val_loss: 2.7181\n",
            "108/108 - 4s - loss: 2.5298 - val_loss: 2.7869\n",
            "108/108 - 3s - loss: 2.5075 - val_loss: 2.7628\n",
            "108/108 - 3s - loss: 2.4950 - val_loss: 2.7357\n",
            "108/108 - 3s - loss: 2.5225 - val_loss: 2.7139\n",
            "108/108 - 3s - loss: 2.5121 - val_loss: 2.7696\n",
            "108/108 - 3s - loss: 2.5347 - val_loss: 2.6925\n",
            "108/108 - 3s - loss: 2.4956 - val_loss: 2.7416\n",
            "108/108 - 4s - loss: 2.5540 - val_loss: 2.7154\n",
            "108/108 - 3s - loss: 2.5044 - val_loss: 2.7506\n",
            "108/108 - 3s - loss: 2.5403 - val_loss: 2.8042\n",
            "108/108 - 3s - loss: 2.5108 - val_loss: 2.7993\n",
            "108/108 - 3s - loss: 2.4999 - val_loss: 2.6788\n",
            "108/108 - 3s - loss: 2.5451 - val_loss: 2.7386\n",
            "108/108 - 3s - loss: 2.5240 - val_loss: 2.7142\n",
            "108/108 - 3s - loss: 2.5032 - val_loss: 2.7581\n",
            "108/108 - 3s - loss: 2.4972 - val_loss: 2.7213\n",
            "108/108 - 3s - loss: 2.5091 - val_loss: 2.7315\n",
            "108/108 - 3s - loss: 2.5079 - val_loss: 2.7168\n",
            "108/108 - 3s - loss: 2.5024 - val_loss: 2.7419\n",
            "108/108 - 3s - loss: 2.4970 - val_loss: 2.7412\n",
            "108/108 - 3s - loss: 2.4911 - val_loss: 2.7374\n",
            "108/108 - 3s - loss: 2.4871 - val_loss: 2.7674\n",
            "108/108 - 3s - loss: 2.5154 - val_loss: 2.7964\n",
            "108/108 - 3s - loss: 2.4908 - val_loss: 2.7608\n",
            "108/108 - 3s - loss: 2.4681 - val_loss: 2.7345\n",
            "108/108 - 3s - loss: 2.4849 - val_loss: 2.7421\n",
            "108/108 - 3s - loss: 2.4733 - val_loss: 2.6725\n",
            "108/108 - 3s - loss: 2.4922 - val_loss: 2.7353\n",
            "108/108 - 3s - loss: 2.4882 - val_loss: 2.7130\n",
            "108/108 - 3s - loss: 2.4926 - val_loss: 2.8479\n",
            "108/108 - 4s - loss: 2.4451 - val_loss: 2.7225\n",
            "108/108 - 3s - loss: 2.4889 - val_loss: 2.7580\n",
            "108/108 - 3s - loss: 2.4529 - val_loss: 2.7257\n",
            "108/108 - 3s - loss: 2.4689 - val_loss: 2.7311\n",
            "108/108 - 3s - loss: 2.4759 - val_loss: 2.6741\n",
            "108/108 - 3s - loss: 2.5024 - val_loss: 2.7386\n",
            "108/108 - 3s - loss: 2.5042 - val_loss: 2.7458\n",
            "108/108 - 3s - loss: 2.4773 - val_loss: 2.7247\n",
            "108/108 - 3s - loss: 2.4708 - val_loss: 2.7271\n",
            "108/108 - 3s - loss: 2.4667 - val_loss: 2.7528\n",
            "108/108 - 4s - loss: 2.4826 - val_loss: 2.7322\n",
            "108/108 - 4s - loss: 2.5134 - val_loss: 2.7133\n",
            "108/108 - 3s - loss: 2.4741 - val_loss: 2.6800\n",
            "108/108 - 3s - loss: 2.4856 - val_loss: 2.7578\n",
            "108/108 - 3s - loss: 2.5107 - val_loss: 2.7678\n",
            "108/108 - 3s - loss: 2.5093 - val_loss: 2.7390\n",
            "108/108 - 3s - loss: 2.4720 - val_loss: 2.7259\n",
            "108/108 - 3s - loss: 2.4523 - val_loss: 2.7381\n",
            "108/108 - 3s - loss: 2.4923 - val_loss: 2.7473\n",
            "108/108 - 3s - loss: 2.4512 - val_loss: 2.7767\n",
            "108/108 - 3s - loss: 2.4374 - val_loss: 2.7659\n",
            "108/108 - 3s - loss: 2.4601 - val_loss: 2.7439\n",
            "108/108 - 3s - loss: 2.4888 - val_loss: 2.7591\n",
            "108/108 - 3s - loss: 2.4494 - val_loss: 2.7611\n",
            "108/108 - 3s - loss: 2.4463 - val_loss: 2.7581\n",
            "108/108 - 3s - loss: 2.4463 - val_loss: 2.7567\n",
            "108/108 - 3s - loss: 2.4785 - val_loss: 2.8188\n",
            "108/108 - 3s - loss: 2.4682 - val_loss: 2.7233\n",
            "108/108 - 3s - loss: 2.4518 - val_loss: 2.6928\n",
            "108/108 - 3s - loss: 2.4349 - val_loss: 2.7489\n",
            "108/108 - 3s - loss: 2.4663 - val_loss: 2.7214\n",
            "108/108 - 3s - loss: 2.4812 - val_loss: 2.7010\n",
            "108/108 - 3s - loss: 2.4523 - val_loss: 2.8014\n",
            "108/108 - 3s - loss: 2.4868 - val_loss: 2.7458\n",
            "108/108 - 3s - loss: 2.4675 - val_loss: 2.7346\n",
            "108/108 - 3s - loss: 2.4450 - val_loss: 2.6998\n",
            "108/108 - 3s - loss: 2.4592 - val_loss: 2.7593\n",
            "108/108 - 3s - loss: 2.4434 - val_loss: 2.7468\n",
            "108/108 - 4s - loss: 2.4609 - val_loss: 2.7319\n",
            "108/108 - 3s - loss: 2.4588 - val_loss: 2.6823\n",
            "108/108 - 3s - loss: 2.4303 - val_loss: 2.7050\n",
            "108/108 - 4s - loss: 2.4961 - val_loss: 2.7827\n",
            "108/108 - 4s - loss: 2.4921 - val_loss: 2.7429\n",
            "108/108 - 4s - loss: 2.4535 - val_loss: 2.6505\n",
            "108/108 - 3s - loss: 2.4508 - val_loss: 2.7685\n",
            "108/108 - 3s - loss: 2.4532 - val_loss: 2.7142\n",
            "108/108 - 3s - loss: 2.4371 - val_loss: 2.7180\n",
            "108/108 - 3s - loss: 2.4459 - val_loss: 2.6990\n",
            "108/108 - 3s - loss: 2.4740 - val_loss: 2.7256\n",
            "108/108 - 3s - loss: 2.4773 - val_loss: 2.6865\n",
            "108/108 - 3s - loss: 2.4618 - val_loss: 2.6768\n",
            "108/108 - 3s - loss: 2.4290 - val_loss: 2.6826\n",
            "108/108 - 3s - loss: 2.4682 - val_loss: 2.7207\n",
            "108/108 - 3s - loss: 2.4342 - val_loss: 2.7414\n",
            "108/108 - 3s - loss: 2.4394 - val_loss: 2.7344\n",
            "108/108 - 3s - loss: 2.4365 - val_loss: 2.8170\n",
            "108/108 - 3s - loss: 2.4633 - val_loss: 2.7468\n",
            "108/108 - 4s - loss: 2.4295 - val_loss: 2.7098\n",
            "108/108 - 3s - loss: 2.4597 - val_loss: 2.6915\n",
            "108/108 - 3s - loss: 2.4188 - val_loss: 2.7218\n",
            "108/108 - 3s - loss: 2.3998 - val_loss: 2.7515\n",
            "108/108 - 3s - loss: 2.4497 - val_loss: 2.7351\n",
            "108/108 - 3s - loss: 2.4416 - val_loss: 2.7133\n",
            "108/108 - 3s - loss: 2.4134 - val_loss: 2.7284\n",
            "108/108 - 3s - loss: 2.3978 - val_loss: 2.6900\n",
            "108/108 - 3s - loss: 2.4159 - val_loss: 2.6408\n",
            "108/108 - 3s - loss: 2.4706 - val_loss: 2.6920\n",
            "108/108 - 3s - loss: 2.4415 - val_loss: 2.7615\n",
            "108/108 - 3s - loss: 2.3944 - val_loss: 2.7572\n",
            "108/108 - 3s - loss: 2.4314 - val_loss: 2.7932\n",
            "108/108 - 3s - loss: 2.4376 - val_loss: 2.7251\n",
            "108/108 - 3s - loss: 2.4135 - val_loss: 2.7520\n",
            "108/108 - 3s - loss: 2.4026 - val_loss: 2.7070\n",
            "108/108 - 3s - loss: 2.3829 - val_loss: 2.7747\n",
            "108/108 - 3s - loss: 2.4022 - val_loss: 2.7327\n",
            "108/108 - 3s - loss: 2.4188 - val_loss: 2.7480\n",
            "108/108 - 3s - loss: 2.4356 - val_loss: 2.7566\n",
            "108/108 - 3s - loss: 2.4341 - val_loss: 2.7120\n",
            "108/108 - 3s - loss: 2.4045 - val_loss: 2.7373\n",
            "108/108 - 3s - loss: 2.4032 - val_loss: 2.7156\n",
            "108/108 - 3s - loss: 2.4258 - val_loss: 2.6934\n",
            "108/108 - 3s - loss: 2.5594 - val_loss: 2.8045\n",
            "108/108 - 3s - loss: 2.4872 - val_loss: 2.7437\n",
            "108/108 - 3s - loss: 2.4122 - val_loss: 2.6765\n",
            "108/108 - 3s - loss: 2.4022 - val_loss: 2.7346\n",
            "108/108 - 3s - loss: 2.4446 - val_loss: 2.7061\n",
            "108/108 - 3s - loss: 2.4154 - val_loss: 2.6998\n",
            "108/108 - 3s - loss: 2.3830 - val_loss: 2.7329\n",
            "108/108 - 3s - loss: 2.3929 - val_loss: 2.7931\n",
            "108/108 - 3s - loss: 2.3636 - val_loss: 2.7359\n",
            "108/108 - 3s - loss: 2.4065 - val_loss: 2.7383\n",
            "108/108 - 3s - loss: 2.3663 - val_loss: 2.7676\n",
            "108/108 - 3s - loss: 2.3956 - val_loss: 2.7747\n",
            "108/108 - 4s - loss: 2.4324 - val_loss: 2.8029\n",
            "108/108 - 3s - loss: 2.4693 - val_loss: 2.7688\n",
            "108/108 - 3s - loss: 2.4089 - val_loss: 2.7254\n",
            "108/108 - 3s - loss: 2.3966 - val_loss: 2.7220\n",
            "108/108 - 3s - loss: 2.4272 - val_loss: 2.7264\n",
            "108/108 - 4s - loss: 2.4453 - val_loss: 2.7351\n",
            "108/108 - 4s - loss: 2.3983 - val_loss: 2.7368\n",
            "108/108 - 3s - loss: 2.4206 - val_loss: 2.7789\n",
            "108/108 - 3s - loss: 2.4107 - val_loss: 2.7421\n",
            "108/108 - 3s - loss: 2.3813 - val_loss: 2.7200\n",
            "108/108 - 3s - loss: 2.4062 - val_loss: 2.7551\n",
            "108/108 - 3s - loss: 2.4096 - val_loss: 2.7783\n",
            "108/108 - 3s - loss: 2.3583 - val_loss: 2.7847\n",
            "108/108 - 3s - loss: 2.3658 - val_loss: 2.7235\n",
            "108/108 - 3s - loss: 2.3873 - val_loss: 2.7227\n",
            "108/108 - 3s - loss: 2.4083 - val_loss: 2.7788\n",
            "108/108 - 3s - loss: 2.4124 - val_loss: 2.7472\n",
            "108/108 - 3s - loss: 2.3832 - val_loss: 2.7008\n",
            "108/108 - 3s - loss: 2.4038 - val_loss: 2.7379\n",
            "108/108 - 3s - loss: 2.3947 - val_loss: 2.7126\n",
            "108/108 - 3s - loss: 2.3745 - val_loss: 2.7022\n",
            "108/108 - 3s - loss: 2.3729 - val_loss: 2.6888\n",
            "108/108 - 3s - loss: 2.3722 - val_loss: 2.7472\n",
            "108/108 - 3s - loss: 2.3686 - val_loss: 2.7091\n",
            "108/108 - 3s - loss: 2.3935 - val_loss: 2.7489\n",
            "108/108 - 3s - loss: 2.3973 - val_loss: 2.6931\n",
            "108/108 - 3s - loss: 2.3679 - val_loss: 2.6789\n",
            "108/108 - 3s - loss: 2.3973 - val_loss: 2.6998\n",
            "108/108 - 3s - loss: 2.4098 - val_loss: 2.6526\n",
            "108/108 - 3s - loss: 2.4153 - val_loss: 2.7828\n",
            "108/108 - 3s - loss: 2.4138 - val_loss: 2.7382\n",
            "108/108 - 3s - loss: 2.3944 - val_loss: 2.6848\n",
            "108/108 - 3s - loss: 2.3754 - val_loss: 2.7227\n",
            "108/108 - 3s - loss: 2.3867 - val_loss: 2.7192\n",
            "108/108 - 3s - loss: 2.4157 - val_loss: 2.6796\n",
            "108/108 - 4s - loss: 2.4042 - val_loss: 2.7178\n",
            "108/108 - 3s - loss: 2.3589 - val_loss: 2.7020\n",
            "108/108 - 4s - loss: 2.3663 - val_loss: 2.7716\n",
            "108/108 - 4s - loss: 2.3814 - val_loss: 2.7452\n",
            "108/108 - 4s - loss: 2.3530 - val_loss: 2.7459\n",
            "108/108 - 4s - loss: 2.3793 - val_loss: 2.7653\n",
            "108/108 - 3s - loss: 2.3608 - val_loss: 2.7679\n",
            "108/108 - 3s - loss: 2.3587 - val_loss: 2.7306\n",
            "108/108 - 3s - loss: 2.3613 - val_loss: 2.7731\n",
            "108/108 - 3s - loss: 2.3352 - val_loss: 2.7301\n",
            "108/108 - 3s - loss: 2.3508 - val_loss: 2.6826\n",
            "108/108 - 3s - loss: 2.3642 - val_loss: 2.7213\n",
            "108/108 - 3s - loss: 2.3507 - val_loss: 2.6645\n",
            "108/108 - 3s - loss: 2.3727 - val_loss: 2.7144\n",
            "108/108 - 3s - loss: 2.3942 - val_loss: 2.7048\n",
            "108/108 - 3s - loss: 2.3964 - val_loss: 2.7277\n",
            "108/108 - 3s - loss: 2.3766 - val_loss: 2.7265\n",
            "108/108 - 3s - loss: 2.4087 - val_loss: 2.7789\n",
            "108/108 - 3s - loss: 2.3942 - val_loss: 2.7105\n",
            "108/108 - 3s - loss: 2.3734 - val_loss: 2.7555\n",
            "108/108 - 3s - loss: 2.3599 - val_loss: 2.6826\n",
            "108/108 - 3s - loss: 2.4202 - val_loss: 2.7159\n",
            "108/108 - 3s - loss: 2.3892 - val_loss: 2.7152\n",
            "108/108 - 3s - loss: 2.3662 - val_loss: 2.7562\n",
            "108/108 - 3s - loss: 2.4111 - val_loss: 2.6757\n",
            "108/108 - 3s - loss: 2.3855 - val_loss: 2.7709\n",
            "108/108 - 3s - loss: 2.4003 - val_loss: 2.7043\n",
            "108/108 - 3s - loss: 2.3847 - val_loss: 2.7045\n",
            "108/108 - 3s - loss: 2.3852 - val_loss: 2.7338\n",
            "108/108 - 3s - loss: 2.4001 - val_loss: 2.7427\n",
            "108/108 - 4s - loss: 2.3716 - val_loss: 2.7586\n",
            "108/108 - 3s - loss: 2.3776 - val_loss: 2.6925\n",
            "108/108 - 3s - loss: 2.3555 - val_loss: 2.7293\n",
            "108/108 - 3s - loss: 2.3619 - val_loss: 2.6849\n",
            "108/108 - 3s - loss: 2.3717 - val_loss: 2.7185\n",
            "108/108 - 4s - loss: 2.3441 - val_loss: 2.7339\n",
            "108/108 - 3s - loss: 2.3676 - val_loss: 2.6760\n",
            "108/108 - 3s - loss: 2.3494 - val_loss: 2.7261\n",
            "108/108 - 3s - loss: 2.3542 - val_loss: 2.6866\n",
            "108/108 - 3s - loss: 2.3716 - val_loss: 2.7259\n",
            "108/108 - 3s - loss: 2.3348 - val_loss: 2.6845\n",
            "108/108 - 3s - loss: 2.3450 - val_loss: 2.7474\n",
            "108/108 - 3s - loss: 2.3367 - val_loss: 2.7073\n",
            "108/108 - 3s - loss: 2.3721 - val_loss: 2.7993\n",
            "108/108 - 3s - loss: 2.4426 - val_loss: 2.8642\n",
            "108/108 - 3s - loss: 2.4496 - val_loss: 2.8377\n",
            "108/108 - 3s - loss: 2.5426 - val_loss: 2.7196\n",
            "108/108 - 3s - loss: 2.4498 - val_loss: 2.7420\n",
            "108/108 - 3s - loss: 2.4328 - val_loss: 2.7539\n",
            "108/108 - 3s - loss: 2.3747 - val_loss: 2.7497\n",
            "108/108 - 3s - loss: 2.3711 - val_loss: 2.7182\n",
            "108/108 - 3s - loss: 2.3568 - val_loss: 2.7375\n",
            "108/108 - 3s - loss: 2.3239 - val_loss: 2.7426\n",
            "108/108 - 3s - loss: 2.3336 - val_loss: 2.6796\n",
            "108/108 - 3s - loss: 2.3299 - val_loss: 2.6976\n",
            "108/108 - 3s - loss: 2.3507 - val_loss: 2.6975\n",
            "108/108 - 4s - loss: 2.3215 - val_loss: 2.6853\n",
            "108/108 - 3s - loss: 2.3305 - val_loss: 2.6937\n",
            "108/108 - 4s - loss: 2.3327 - val_loss: 2.7644\n",
            "108/108 - 3s - loss: 2.3321 - val_loss: 2.7179\n",
            "108/108 - 3s - loss: 2.3402 - val_loss: 2.7183\n",
            "108/108 - 3s - loss: 2.3915 - val_loss: 2.7846\n",
            "108/108 - 3s - loss: 2.3458 - val_loss: 2.6869\n",
            "108/108 - 3s - loss: 2.3535 - val_loss: 2.6919\n",
            "108/108 - 3s - loss: 2.3290 - val_loss: 2.7153\n",
            "108/108 - 3s - loss: 2.3101 - val_loss: 2.6851\n",
            "108/108 - 3s - loss: 2.3304 - val_loss: 2.7696\n",
            "108/108 - 3s - loss: 2.3225 - val_loss: 2.7105\n",
            "108/108 - 3s - loss: 2.3136 - val_loss: 2.6736\n",
            "108/108 - 3s - loss: 2.3290 - val_loss: 2.6683\n",
            "108/108 - 3s - loss: 2.3394 - val_loss: 2.7734\n",
            "108/108 - 3s - loss: 2.3187 - val_loss: 2.6817\n",
            "108/108 - 4s - loss: 2.3089 - val_loss: 2.7151\n",
            "108/108 - 3s - loss: 2.3445 - val_loss: 2.7614\n",
            "108/108 - 3s - loss: 2.3640 - val_loss: 2.7517\n",
            "108/108 - 3s - loss: 2.3222 - val_loss: 2.6955\n",
            "108/108 - 3s - loss: 2.3463 - val_loss: 2.7033\n",
            "108/108 - 3s - loss: 2.3437 - val_loss: 2.7302\n",
            "108/108 - 3s - loss: 2.3407 - val_loss: 2.7670\n",
            "108/108 - 3s - loss: 2.2986 - val_loss: 2.7259\n",
            "108/108 - 3s - loss: 2.3029 - val_loss: 2.7232\n",
            "108/108 - 3s - loss: 2.2961 - val_loss: 2.7482\n",
            "108/108 - 3s - loss: 2.3297 - val_loss: 2.7544\n",
            "108/108 - 3s - loss: 2.3651 - val_loss: 2.7599\n",
            "108/108 - 3s - loss: 2.3212 - val_loss: 2.7358\n",
            "108/108 - 3s - loss: 2.3161 - val_loss: 2.7197\n",
            "108/108 - 3s - loss: 2.3103 - val_loss: 2.6852\n",
            "108/108 - 4s - loss: 2.3012 - val_loss: 2.7045\n",
            "108/108 - 3s - loss: 2.3196 - val_loss: 2.7554\n",
            "108/108 - 3s - loss: 2.3297 - val_loss: 2.6467\n",
            "108/108 - 3s - loss: 2.3388 - val_loss: 2.7773\n",
            "108/108 - 3s - loss: 2.3393 - val_loss: 2.7655\n",
            "108/108 - 4s - loss: 2.3551 - val_loss: 2.6721\n",
            "108/108 - 4s - loss: 2.3168 - val_loss: 2.6930\n",
            "108/108 - 4s - loss: 2.3292 - val_loss: 2.7201\n",
            "108/108 - 3s - loss: 2.3256 - val_loss: 2.6662\n",
            "108/108 - 3s - loss: 2.3087 - val_loss: 2.7614\n",
            "108/108 - 3s - loss: 2.3488 - val_loss: 2.8955\n",
            "108/108 - 3s - loss: 2.4066 - val_loss: 2.7317\n",
            "108/108 - 3s - loss: 2.4118 - val_loss: 2.6882\n",
            "108/108 - 3s - loss: 2.3604 - val_loss: 2.6885\n",
            "108/108 - 3s - loss: 2.3586 - val_loss: 2.7453\n",
            "108/108 - 3s - loss: 2.3554 - val_loss: 2.8066\n",
            "108/108 - 3s - loss: 2.3286 - val_loss: 2.7441\n",
            "108/108 - 3s - loss: 2.3567 - val_loss: 2.7024\n",
            "108/108 - 3s - loss: 2.3492 - val_loss: 2.6415\n",
            "0.6\n",
            "108/108 - 8s - loss: 9.8933 - val_loss: 8.2591\n",
            "108/108 - 3s - loss: 8.2855 - val_loss: 7.0052\n",
            "108/108 - 3s - loss: 7.1630 - val_loss: 6.0689\n",
            "108/108 - 3s - loss: 6.3168 - val_loss: 5.3530\n",
            "108/108 - 3s - loss: 5.6285 - val_loss: 4.8127\n",
            "108/108 - 3s - loss: 5.1617 - val_loss: 4.4308\n",
            "108/108 - 3s - loss: 4.8113 - val_loss: 4.1759\n",
            "108/108 - 3s - loss: 4.5871 - val_loss: 4.0163\n",
            "108/108 - 3s - loss: 4.4157 - val_loss: 3.9215\n",
            "108/108 - 3s - loss: 4.3309 - val_loss: 3.8628\n",
            "108/108 - 3s - loss: 4.2298 - val_loss: 3.8320\n",
            "108/108 - 3s - loss: 4.1640 - val_loss: 3.8159\n",
            "108/108 - 4s - loss: 4.1618 - val_loss: 3.8075\n",
            "108/108 - 4s - loss: 4.1483 - val_loss: 3.8032\n",
            "108/108 - 3s - loss: 4.1468 - val_loss: 3.8034\n",
            "108/108 - 3s - loss: 4.1221 - val_loss: 3.8049\n",
            "108/108 - 3s - loss: 4.1110 - val_loss: 3.8074\n",
            "108/108 - 3s - loss: 4.1432 - val_loss: 3.8103\n",
            "108/108 - 3s - loss: 4.1446 - val_loss: 3.8128\n",
            "108/108 - 3s - loss: 4.0638 - val_loss: 3.8144\n",
            "108/108 - 3s - loss: 4.1611 - val_loss: 3.8144\n",
            "108/108 - 3s - loss: 4.1343 - val_loss: 3.8164\n",
            "108/108 - 3s - loss: 4.1230 - val_loss: 3.8162\n",
            "108/108 - 3s - loss: 4.1266 - val_loss: 3.8191\n",
            "108/108 - 3s - loss: 4.1466 - val_loss: 3.8171\n",
            "108/108 - 3s - loss: 4.1242 - val_loss: 3.8204\n",
            "108/108 - 3s - loss: 4.1544 - val_loss: 3.8191\n",
            "108/108 - 3s - loss: 4.1520 - val_loss: 3.8179\n",
            "108/108 - 3s - loss: 4.1676 - val_loss: 3.7997\n",
            "108/108 - 3s - loss: 3.8099 - val_loss: 3.3056\n",
            "108/108 - 3s - loss: 3.5332 - val_loss: 3.1128\n",
            "108/108 - 3s - loss: 3.3223 - val_loss: 2.9553\n",
            "108/108 - 4s - loss: 3.1934 - val_loss: 2.8908\n",
            "108/108 - 4s - loss: 3.0472 - val_loss: 2.8060\n",
            "108/108 - 3s - loss: 3.0196 - val_loss: 2.7785\n",
            "108/108 - 3s - loss: 3.0003 - val_loss: 2.7935\n",
            "108/108 - 3s - loss: 2.9653 - val_loss: 2.7181\n",
            "108/108 - 3s - loss: 2.9065 - val_loss: 2.7133\n",
            "108/108 - 3s - loss: 2.8921 - val_loss: 2.6785\n",
            "108/108 - 3s - loss: 2.8969 - val_loss: 2.7320\n",
            "108/108 - 3s - loss: 2.8766 - val_loss: 2.6145\n",
            "108/108 - 3s - loss: 2.8208 - val_loss: 2.5935\n",
            "108/108 - 3s - loss: 2.7681 - val_loss: 2.6130\n",
            "108/108 - 3s - loss: 2.7842 - val_loss: 2.5610\n",
            "108/108 - 3s - loss: 2.7618 - val_loss: 2.5413\n",
            "108/108 - 3s - loss: 2.7786 - val_loss: 2.5906\n",
            "108/108 - 3s - loss: 2.7215 - val_loss: 2.5561\n",
            "108/108 - 3s - loss: 2.7340 - val_loss: 2.5593\n",
            "108/108 - 3s - loss: 2.7568 - val_loss: 2.5491\n",
            "108/108 - 3s - loss: 2.7346 - val_loss: 2.5086\n",
            "108/108 - 3s - loss: 2.7644 - val_loss: 2.5361\n",
            "108/108 - 3s - loss: 2.7104 - val_loss: 2.5147\n",
            "108/108 - 3s - loss: 2.7127 - val_loss: 2.5781\n",
            "108/108 - 3s - loss: 2.7044 - val_loss: 2.5124\n",
            "108/108 - 3s - loss: 2.7459 - val_loss: 2.5289\n",
            "108/108 - 3s - loss: 2.6742 - val_loss: 2.5090\n",
            "108/108 - 3s - loss: 2.7228 - val_loss: 2.5154\n",
            "108/108 - 3s - loss: 2.6555 - val_loss: 2.5400\n",
            "108/108 - 3s - loss: 2.6758 - val_loss: 2.4979\n",
            "108/108 - 3s - loss: 2.6922 - val_loss: 2.4935\n",
            "108/108 - 3s - loss: 2.6513 - val_loss: 2.4997\n",
            "108/108 - 3s - loss: 2.6494 - val_loss: 2.4884\n",
            "108/108 - 4s - loss: 2.6628 - val_loss: 2.4973\n",
            "108/108 - 3s - loss: 2.6482 - val_loss: 2.5027\n",
            "108/108 - 3s - loss: 2.6757 - val_loss: 2.4982\n",
            "108/108 - 3s - loss: 2.6959 - val_loss: 2.4998\n",
            "108/108 - 4s - loss: 2.6568 - val_loss: 2.5052\n",
            "108/108 - 3s - loss: 2.5971 - val_loss: 2.4771\n",
            "108/108 - 3s - loss: 2.6311 - val_loss: 2.5101\n",
            "108/108 - 3s - loss: 2.6450 - val_loss: 2.5155\n",
            "108/108 - 3s - loss: 2.6537 - val_loss: 2.5051\n",
            "108/108 - 3s - loss: 2.6623 - val_loss: 2.4839\n",
            "108/108 - 4s - loss: 2.5950 - val_loss: 2.5088\n",
            "108/108 - 4s - loss: 2.6256 - val_loss: 2.4982\n",
            "108/108 - 4s - loss: 2.6240 - val_loss: 2.4851\n",
            "108/108 - 3s - loss: 2.6388 - val_loss: 2.4694\n",
            "108/108 - 3s - loss: 2.6187 - val_loss: 2.5352\n",
            "108/108 - 3s - loss: 2.6661 - val_loss: 2.4802\n",
            "108/108 - 3s - loss: 2.6638 - val_loss: 2.4939\n",
            "108/108 - 3s - loss: 2.6434 - val_loss: 2.4834\n",
            "108/108 - 3s - loss: 2.5952 - val_loss: 2.4837\n",
            "108/108 - 3s - loss: 2.6246 - val_loss: 2.4705\n",
            "108/108 - 3s - loss: 2.5806 - val_loss: 2.5017\n",
            "108/108 - 4s - loss: 2.5989 - val_loss: 2.4795\n",
            "108/108 - 3s - loss: 2.6246 - val_loss: 2.4604\n",
            "108/108 - 3s - loss: 2.6160 - val_loss: 2.4904\n",
            "108/108 - 3s - loss: 2.6194 - val_loss: 2.5002\n",
            "108/108 - 3s - loss: 2.5956 - val_loss: 2.4677\n",
            "108/108 - 3s - loss: 2.6174 - val_loss: 2.4725\n",
            "108/108 - 3s - loss: 2.6057 - val_loss: 2.4730\n",
            "108/108 - 3s - loss: 2.6249 - val_loss: 2.5122\n",
            "108/108 - 3s - loss: 2.5890 - val_loss: 2.4893\n",
            "108/108 - 3s - loss: 2.5915 - val_loss: 2.4992\n",
            "108/108 - 3s - loss: 2.6188 - val_loss: 2.4803\n",
            "108/108 - 3s - loss: 2.6148 - val_loss: 2.5277\n",
            "108/108 - 4s - loss: 2.6175 - val_loss: 2.4801\n",
            "108/108 - 3s - loss: 2.6164 - val_loss: 2.4820\n",
            "108/108 - 4s - loss: 2.5837 - val_loss: 2.4848\n",
            "108/108 - 3s - loss: 2.5724 - val_loss: 2.4545\n",
            "108/108 - 3s - loss: 2.5536 - val_loss: 2.4732\n",
            "108/108 - 3s - loss: 2.5502 - val_loss: 2.4817\n",
            "108/108 - 3s - loss: 2.5740 - val_loss: 2.4854\n",
            "108/108 - 3s - loss: 2.5839 - val_loss: 2.4701\n",
            "108/108 - 3s - loss: 2.5534 - val_loss: 2.4872\n",
            "108/108 - 3s - loss: 2.6021 - val_loss: 2.4983\n",
            "108/108 - 3s - loss: 2.5946 - val_loss: 2.5003\n",
            "108/108 - 3s - loss: 2.6188 - val_loss: 2.4923\n",
            "108/108 - 3s - loss: 2.5939 - val_loss: 2.5551\n",
            "108/108 - 3s - loss: 2.5896 - val_loss: 2.4867\n",
            "108/108 - 3s - loss: 2.5778 - val_loss: 2.4762\n",
            "108/108 - 3s - loss: 2.5648 - val_loss: 2.5294\n",
            "108/108 - 3s - loss: 2.5626 - val_loss: 2.5178\n",
            "108/108 - 3s - loss: 2.5929 - val_loss: 2.5089\n",
            "108/108 - 3s - loss: 2.5970 - val_loss: 2.4877\n",
            "108/108 - 3s - loss: 2.5983 - val_loss: 2.5258\n",
            "108/108 - 3s - loss: 2.6000 - val_loss: 2.4683\n",
            "108/108 - 3s - loss: 2.5757 - val_loss: 2.5004\n",
            "108/108 - 3s - loss: 2.5704 - val_loss: 2.4775\n",
            "108/108 - 3s - loss: 2.5519 - val_loss: 2.4708\n",
            "108/108 - 3s - loss: 2.5496 - val_loss: 2.5221\n",
            "108/108 - 3s - loss: 2.5680 - val_loss: 2.4547\n",
            "108/108 - 3s - loss: 2.5790 - val_loss: 2.4639\n",
            "108/108 - 4s - loss: 2.5448 - val_loss: 2.5433\n",
            "108/108 - 3s - loss: 2.5496 - val_loss: 2.5262\n",
            "108/108 - 4s - loss: 2.5559 - val_loss: 2.5392\n",
            "108/108 - 3s - loss: 2.5337 - val_loss: 2.4899\n",
            "108/108 - 3s - loss: 2.5413 - val_loss: 2.4909\n",
            "108/108 - 3s - loss: 2.5377 - val_loss: 2.4589\n",
            "108/108 - 3s - loss: 2.5366 - val_loss: 2.4393\n",
            "108/108 - 3s - loss: 2.5187 - val_loss: 2.4420\n",
            "108/108 - 3s - loss: 2.5497 - val_loss: 2.4730\n",
            "108/108 - 3s - loss: 2.5442 - val_loss: 2.4831\n",
            "108/108 - 3s - loss: 2.5546 - val_loss: 2.4588\n",
            "108/108 - 4s - loss: 2.5368 - val_loss: 2.4173\n",
            "108/108 - 4s - loss: 2.5183 - val_loss: 2.4344\n",
            "108/108 - 3s - loss: 2.5472 - val_loss: 2.4047\n",
            "108/108 - 3s - loss: 2.5116 - val_loss: 2.4287\n",
            "108/108 - 3s - loss: 2.5371 - val_loss: 2.4002\n",
            "108/108 - 3s - loss: 2.5319 - val_loss: 2.4518\n",
            "108/108 - 3s - loss: 2.5366 - val_loss: 2.3843\n",
            "108/108 - 3s - loss: 2.5215 - val_loss: 2.4582\n",
            "108/108 - 3s - loss: 2.5405 - val_loss: 2.4345\n",
            "108/108 - 3s - loss: 2.5277 - val_loss: 2.4128\n",
            "108/108 - 3s - loss: 2.4859 - val_loss: 2.4360\n",
            "108/108 - 3s - loss: 2.5111 - val_loss: 2.3834\n",
            "108/108 - 3s - loss: 2.5039 - val_loss: 2.3819\n",
            "108/108 - 3s - loss: 2.4998 - val_loss: 2.3911\n",
            "108/108 - 3s - loss: 2.4845 - val_loss: 2.4090\n",
            "108/108 - 3s - loss: 2.4744 - val_loss: 2.4330\n",
            "108/108 - 3s - loss: 2.4956 - val_loss: 2.4191\n",
            "108/108 - 3s - loss: 2.5136 - val_loss: 2.4224\n",
            "108/108 - 3s - loss: 2.4958 - val_loss: 2.3965\n",
            "108/108 - 3s - loss: 2.5052 - val_loss: 2.4171\n",
            "108/108 - 3s - loss: 2.4784 - val_loss: 2.4318\n",
            "108/108 - 3s - loss: 2.4723 - val_loss: 2.3884\n",
            "108/108 - 3s - loss: 2.4779 - val_loss: 2.4316\n",
            "108/108 - 3s - loss: 2.4854 - val_loss: 2.4181\n",
            "108/108 - 3s - loss: 2.4751 - val_loss: 2.4263\n",
            "108/108 - 3s - loss: 2.4900 - val_loss: 2.4300\n",
            "108/108 - 3s - loss: 2.4671 - val_loss: 2.4164\n",
            "108/108 - 4s - loss: 2.4767 - val_loss: 2.4253\n",
            "108/108 - 4s - loss: 2.4828 - val_loss: 2.4537\n",
            "108/108 - 4s - loss: 2.4675 - val_loss: 2.4148\n",
            "108/108 - 4s - loss: 2.4805 - val_loss: 2.3927\n",
            "108/108 - 3s - loss: 2.4445 - val_loss: 2.4118\n",
            "108/108 - 3s - loss: 2.4641 - val_loss: 2.3899\n",
            "108/108 - 3s - loss: 2.4952 - val_loss: 2.4325\n",
            "108/108 - 3s - loss: 2.4874 - val_loss: 2.4165\n",
            "108/108 - 3s - loss: 2.4847 - val_loss: 2.4340\n",
            "108/108 - 3s - loss: 2.4664 - val_loss: 2.4176\n",
            "108/108 - 3s - loss: 2.4523 - val_loss: 2.4332\n",
            "108/108 - 3s - loss: 2.4644 - val_loss: 2.4604\n",
            "108/108 - 3s - loss: 2.4654 - val_loss: 2.4018\n",
            "108/108 - 3s - loss: 2.4679 - val_loss: 2.4020\n",
            "108/108 - 3s - loss: 2.4736 - val_loss: 2.4101\n",
            "108/108 - 3s - loss: 2.4550 - val_loss: 2.4293\n",
            "108/108 - 3s - loss: 2.4298 - val_loss: 2.4597\n",
            "108/108 - 3s - loss: 2.4499 - val_loss: 2.3961\n",
            "108/108 - 3s - loss: 2.4450 - val_loss: 2.4314\n",
            "108/108 - 3s - loss: 2.4511 - val_loss: 2.4127\n",
            "108/108 - 3s - loss: 2.4244 - val_loss: 2.4164\n",
            "108/108 - 3s - loss: 2.4877 - val_loss: 2.3946\n",
            "108/108 - 3s - loss: 2.4572 - val_loss: 2.4112\n",
            "108/108 - 3s - loss: 2.4471 - val_loss: 2.4297\n",
            "108/108 - 3s - loss: 2.4471 - val_loss: 2.4205\n",
            "108/108 - 3s - loss: 2.4838 - val_loss: 2.4369\n",
            "108/108 - 4s - loss: 2.4605 - val_loss: 2.4447\n",
            "108/108 - 3s - loss: 2.4604 - val_loss: 2.3936\n",
            "108/108 - 3s - loss: 2.4577 - val_loss: 2.4156\n",
            "108/108 - 3s - loss: 2.4247 - val_loss: 2.3930\n",
            "108/108 - 3s - loss: 2.4224 - val_loss: 2.4553\n",
            "108/108 - 3s - loss: 2.4558 - val_loss: 2.4080\n",
            "108/108 - 3s - loss: 2.4785 - val_loss: 2.3834\n",
            "108/108 - 3s - loss: 2.4215 - val_loss: 2.4093\n",
            "108/108 - 3s - loss: 2.4178 - val_loss: 2.4104\n",
            "108/108 - 3s - loss: 2.4058 - val_loss: 2.4383\n",
            "108/108 - 3s - loss: 2.4358 - val_loss: 2.4257\n",
            "108/108 - 3s - loss: 2.4284 - val_loss: 2.3716\n",
            "108/108 - 3s - loss: 2.4189 - val_loss: 2.4151\n",
            "108/108 - 3s - loss: 2.4272 - val_loss: 2.4360\n",
            "108/108 - 3s - loss: 2.4329 - val_loss: 2.4007\n",
            "108/108 - 3s - loss: 2.4193 - val_loss: 2.4095\n",
            "108/108 - 3s - loss: 2.4062 - val_loss: 2.4584\n",
            "108/108 - 3s - loss: 2.4085 - val_loss: 2.3965\n",
            "108/108 - 4s - loss: 2.4209 - val_loss: 2.4553\n",
            "108/108 - 3s - loss: 2.4013 - val_loss: 2.4301\n",
            "108/108 - 3s - loss: 2.4064 - val_loss: 2.3778\n",
            "108/108 - 3s - loss: 2.4234 - val_loss: 2.4821\n",
            "108/108 - 3s - loss: 2.4203 - val_loss: 2.4660\n",
            "108/108 - 3s - loss: 2.4075 - val_loss: 2.4086\n",
            "108/108 - 3s - loss: 2.4033 - val_loss: 2.4155\n",
            "108/108 - 4s - loss: 2.4076 - val_loss: 2.4222\n",
            "108/108 - 3s - loss: 2.4332 - val_loss: 2.3991\n",
            "108/108 - 3s - loss: 2.4008 - val_loss: 2.3679\n",
            "108/108 - 3s - loss: 2.3872 - val_loss: 2.4036\n",
            "108/108 - 3s - loss: 2.4179 - val_loss: 2.3987\n",
            "108/108 - 3s - loss: 2.4243 - val_loss: 2.3645\n",
            "108/108 - 3s - loss: 2.4122 - val_loss: 2.3616\n",
            "108/108 - 3s - loss: 2.4141 - val_loss: 2.3955\n",
            "108/108 - 3s - loss: 2.3940 - val_loss: 2.3992\n",
            "108/108 - 4s - loss: 2.4356 - val_loss: 2.4064\n",
            "108/108 - 3s - loss: 2.4282 - val_loss: 2.4245\n",
            "108/108 - 3s - loss: 2.3977 - val_loss: 2.3933\n",
            "108/108 - 3s - loss: 2.3717 - val_loss: 2.4095\n",
            "108/108 - 3s - loss: 2.3958 - val_loss: 2.3877\n",
            "108/108 - 3s - loss: 2.4737 - val_loss: 2.4943\n",
            "108/108 - 3s - loss: 2.4784 - val_loss: 2.3950\n",
            "108/108 - 3s - loss: 2.4155 - val_loss: 2.4062\n",
            "108/108 - 3s - loss: 2.4055 - val_loss: 2.3886\n",
            "108/108 - 3s - loss: 2.3815 - val_loss: 2.4224\n",
            "108/108 - 3s - loss: 2.3728 - val_loss: 2.3863\n",
            "108/108 - 3s - loss: 2.3496 - val_loss: 2.3801\n",
            "108/108 - 3s - loss: 2.3946 - val_loss: 2.4155\n",
            "108/108 - 3s - loss: 2.4001 - val_loss: 2.4537\n",
            "108/108 - 3s - loss: 2.4296 - val_loss: 2.3904\n",
            "108/108 - 3s - loss: 2.4196 - val_loss: 2.3929\n",
            "108/108 - 3s - loss: 2.4042 - val_loss: 2.4003\n",
            "108/108 - 3s - loss: 2.3893 - val_loss: 2.4496\n",
            "108/108 - 3s - loss: 2.4051 - val_loss: 2.4463\n",
            "108/108 - 3s - loss: 2.4121 - val_loss: 2.4363\n",
            "108/108 - 3s - loss: 2.3765 - val_loss: 2.4363\n",
            "108/108 - 3s - loss: 2.3854 - val_loss: 2.4308\n",
            "108/108 - 3s - loss: 2.3493 - val_loss: 2.4181\n",
            "108/108 - 3s - loss: 2.4040 - val_loss: 2.3808\n",
            "108/108 - 3s - loss: 2.3671 - val_loss: 2.4018\n",
            "108/108 - 3s - loss: 2.3867 - val_loss: 2.3925\n",
            "108/108 - 3s - loss: 2.4034 - val_loss: 2.4010\n",
            "108/108 - 3s - loss: 2.3345 - val_loss: 2.3822\n",
            "108/108 - 3s - loss: 2.3740 - val_loss: 2.4934\n",
            "108/108 - 3s - loss: 2.3612 - val_loss: 2.4548\n",
            "108/108 - 4s - loss: 2.3561 - val_loss: 2.5159\n",
            "108/108 - 4s - loss: 2.3983 - val_loss: 2.3966\n",
            "108/108 - 4s - loss: 2.3514 - val_loss: 2.4386\n",
            "108/108 - 3s - loss: 2.4013 - val_loss: 2.4550\n",
            "108/108 - 4s - loss: 2.3681 - val_loss: 2.4093\n",
            "108/108 - 4s - loss: 2.3478 - val_loss: 2.4056\n",
            "108/108 - 4s - loss: 2.3562 - val_loss: 2.3616\n",
            "108/108 - 3s - loss: 2.3690 - val_loss: 2.4111\n",
            "108/108 - 3s - loss: 2.3424 - val_loss: 2.3827\n",
            "108/108 - 3s - loss: 2.3676 - val_loss: 2.3804\n",
            "108/108 - 3s - loss: 2.3789 - val_loss: 2.4359\n",
            "108/108 - 3s - loss: 2.3705 - val_loss: 2.4421\n",
            "108/108 - 3s - loss: 2.3186 - val_loss: 2.4452\n",
            "108/108 - 3s - loss: 2.3225 - val_loss: 2.4227\n",
            "108/108 - 3s - loss: 2.3422 - val_loss: 2.4706\n",
            "108/108 - 3s - loss: 2.3699 - val_loss: 2.4100\n",
            "108/108 - 3s - loss: 2.3658 - val_loss: 2.4429\n",
            "108/108 - 3s - loss: 2.3529 - val_loss: 2.4582\n",
            "108/108 - 3s - loss: 2.3780 - val_loss: 2.4490\n",
            "108/108 - 3s - loss: 2.3434 - val_loss: 2.4399\n",
            "108/108 - 3s - loss: 2.3447 - val_loss: 2.4807\n",
            "108/108 - 3s - loss: 2.3259 - val_loss: 2.4739\n",
            "108/108 - 4s - loss: 2.3454 - val_loss: 2.3948\n",
            "108/108 - 3s - loss: 2.3681 - val_loss: 2.4007\n",
            "108/108 - 3s - loss: 2.3321 - val_loss: 2.4426\n",
            "108/108 - 3s - loss: 2.3574 - val_loss: 2.3975\n",
            "108/108 - 3s - loss: 2.3334 - val_loss: 2.4119\n",
            "108/108 - 3s - loss: 2.4044 - val_loss: 2.4485\n",
            "108/108 - 3s - loss: 2.3720 - val_loss: 2.3910\n",
            "108/108 - 3s - loss: 2.3322 - val_loss: 2.3900\n",
            "108/108 - 3s - loss: 2.3403 - val_loss: 2.4196\n",
            "108/108 - 3s - loss: 2.3641 - val_loss: 2.4430\n",
            "108/108 - 3s - loss: 2.3163 - val_loss: 2.4730\n",
            "108/108 - 3s - loss: 2.3282 - val_loss: 2.5390\n",
            "108/108 - 3s - loss: 2.3330 - val_loss: 2.4787\n",
            "108/108 - 3s - loss: 2.2906 - val_loss: 2.4653\n",
            "108/108 - 3s - loss: 2.3291 - val_loss: 2.4409\n",
            "108/108 - 3s - loss: 2.3639 - val_loss: 2.4487\n",
            "108/108 - 3s - loss: 2.3817 - val_loss: 2.4080\n",
            "108/108 - 4s - loss: 2.3423 - val_loss: 2.3995\n",
            "108/108 - 4s - loss: 2.3545 - val_loss: 2.4129\n",
            "108/108 - 3s - loss: 2.3515 - val_loss: 2.4318\n",
            "108/108 - 4s - loss: 2.3296 - val_loss: 2.4314\n",
            "108/108 - 3s - loss: 2.3303 - val_loss: 2.4306\n",
            "108/108 - 3s - loss: 2.3083 - val_loss: 2.4171\n",
            "108/108 - 3s - loss: 2.3256 - val_loss: 2.4219\n",
            "108/108 - 3s - loss: 2.3256 - val_loss: 2.4516\n",
            "108/108 - 3s - loss: 2.3098 - val_loss: 2.4897\n",
            "108/108 - 4s - loss: 2.3183 - val_loss: 2.3704\n",
            "108/108 - 3s - loss: 2.3211 - val_loss: 2.4239\n",
            "108/108 - 3s - loss: 2.3417 - val_loss: 2.4449\n",
            "108/108 - 3s - loss: 2.3509 - val_loss: 2.4273\n",
            "108/108 - 3s - loss: 2.3326 - val_loss: 2.4157\n",
            "108/108 - 3s - loss: 2.3189 - val_loss: 2.4160\n",
            "108/108 - 3s - loss: 2.3396 - val_loss: 2.4413\n",
            "108/108 - 3s - loss: 2.3030 - val_loss: 2.4009\n",
            "108/108 - 3s - loss: 2.3025 - val_loss: 2.4625\n",
            "108/108 - 3s - loss: 2.3549 - val_loss: 2.4372\n",
            "108/108 - 3s - loss: 2.3478 - val_loss: 2.4642\n",
            "108/108 - 3s - loss: 2.3435 - val_loss: 2.4094\n",
            "108/108 - 3s - loss: 2.3494 - val_loss: 2.4316\n",
            "108/108 - 3s - loss: 2.3234 - val_loss: 2.4891\n",
            "108/108 - 3s - loss: 2.3396 - val_loss: 2.4853\n",
            "108/108 - 3s - loss: 2.2802 - val_loss: 2.4631\n",
            "108/108 - 3s - loss: 2.3381 - val_loss: 2.4147\n",
            "108/108 - 3s - loss: 2.3166 - val_loss: 2.3978\n",
            "108/108 - 3s - loss: 2.3498 - val_loss: 2.4109\n",
            "108/108 - 4s - loss: 2.3300 - val_loss: 2.4171\n",
            "108/108 - 3s - loss: 2.3307 - val_loss: 2.4225\n",
            "108/108 - 3s - loss: 2.3156 - val_loss: 2.3717\n",
            "108/108 - 3s - loss: 2.3140 - val_loss: 2.3628\n",
            "108/108 - 4s - loss: 2.3020 - val_loss: 2.3653\n",
            "108/108 - 4s - loss: 2.2888 - val_loss: 2.3785\n",
            "108/108 - 3s - loss: 2.2836 - val_loss: 2.4160\n",
            "108/108 - 3s - loss: 2.3247 - val_loss: 2.4495\n",
            "108/108 - 3s - loss: 2.2936 - val_loss: 2.4011\n",
            "108/108 - 3s - loss: 2.2991 - val_loss: 2.3859\n",
            "108/108 - 3s - loss: 2.2836 - val_loss: 2.3963\n",
            "108/108 - 3s - loss: 2.2799 - val_loss: 2.4720\n",
            "108/108 - 3s - loss: 2.3336 - val_loss: 2.3921\n",
            "108/108 - 3s - loss: 2.3363 - val_loss: 2.4110\n",
            "108/108 - 3s - loss: 2.3055 - val_loss: 2.4196\n",
            "108/108 - 3s - loss: 2.2862 - val_loss: 2.5370\n",
            "108/108 - 4s - loss: 2.3521 - val_loss: 2.4045\n",
            "108/108 - 3s - loss: 2.2837 - val_loss: 2.4380\n",
            "108/108 - 3s - loss: 2.2872 - val_loss: 2.4290\n",
            "108/108 - 3s - loss: 2.2815 - val_loss: 2.4692\n",
            "108/108 - 3s - loss: 2.3077 - val_loss: 2.4480\n",
            "108/108 - 4s - loss: 2.3002 - val_loss: 2.4697\n",
            "108/108 - 4s - loss: 2.3119 - val_loss: 2.4703\n",
            "108/108 - 4s - loss: 2.2788 - val_loss: 2.4969\n",
            "108/108 - 3s - loss: 2.2710 - val_loss: 2.4704\n",
            "108/108 - 4s - loss: 2.2719 - val_loss: 2.4942\n",
            "108/108 - 3s - loss: 2.2966 - val_loss: 2.3957\n",
            "108/108 - 4s - loss: 2.2568 - val_loss: 2.4641\n",
            "108/108 - 4s - loss: 2.3011 - val_loss: 2.4507\n",
            "108/108 - 3s - loss: 2.2661 - val_loss: 2.4404\n",
            "108/108 - 3s - loss: 2.2504 - val_loss: 2.4409\n",
            "108/108 - 3s - loss: 2.2815 - val_loss: 2.3961\n",
            "108/108 - 3s - loss: 2.2796 - val_loss: 2.4143\n",
            "108/108 - 3s - loss: 2.3044 - val_loss: 2.4566\n",
            "108/108 - 3s - loss: 2.3142 - val_loss: 2.4567\n",
            "108/108 - 3s - loss: 2.2692 - val_loss: 2.4536\n",
            "108/108 - 3s - loss: 2.2799 - val_loss: 2.4504\n",
            "108/108 - 3s - loss: 2.2972 - val_loss: 2.4699\n",
            "108/108 - 3s - loss: 2.2694 - val_loss: 2.4766\n",
            "108/108 - 3s - loss: 2.3108 - val_loss: 2.4939\n",
            "108/108 - 4s - loss: 2.3098 - val_loss: 2.4372\n",
            "108/108 - 3s - loss: 2.2703 - val_loss: 2.4191\n",
            "108/108 - 3s - loss: 2.2911 - val_loss: 2.4430\n",
            "108/108 - 3s - loss: 2.2891 - val_loss: 2.4279\n",
            "108/108 - 3s - loss: 2.2581 - val_loss: 2.4298\n",
            "108/108 - 3s - loss: 2.3011 - val_loss: 2.5219\n",
            "108/108 - 3s - loss: 2.2735 - val_loss: 2.4147\n",
            "108/108 - 3s - loss: 2.2530 - val_loss: 2.4770\n",
            "108/108 - 3s - loss: 2.3183 - val_loss: 2.4110\n",
            "108/108 - 3s - loss: 2.2492 - val_loss: 2.4010\n",
            "108/108 - 3s - loss: 2.2787 - val_loss: 2.3999\n",
            "108/108 - 3s - loss: 2.2647 - val_loss: 2.4542\n",
            "108/108 - 4s - loss: 2.3052 - val_loss: 2.4960\n",
            "108/108 - 3s - loss: 2.2628 - val_loss: 2.4635\n",
            "108/108 - 3s - loss: 2.2462 - val_loss: 2.4336\n",
            "108/108 - 3s - loss: 2.3062 - val_loss: 2.4978\n",
            "108/108 - 3s - loss: 2.2807 - val_loss: 2.4722\n",
            "108/108 - 4s - loss: 2.2844 - val_loss: 2.4504\n",
            "108/108 - 4s - loss: 2.2755 - val_loss: 2.5621\n",
            "108/108 - 3s - loss: 2.2775 - val_loss: 2.4921\n",
            "108/108 - 3s - loss: 2.2657 - val_loss: 2.4293\n",
            "108/108 - 3s - loss: 2.2474 - val_loss: 2.4172\n",
            "108/108 - 4s - loss: 2.2913 - val_loss: 2.3800\n",
            "108/108 - 3s - loss: 2.2579 - val_loss: 2.5263\n",
            "108/108 - 3s - loss: 2.2492 - val_loss: 2.4642\n",
            "108/108 - 3s - loss: 2.2647 - val_loss: 2.4694\n",
            "108/108 - 3s - loss: 2.2662 - val_loss: 2.5259\n",
            "108/108 - 3s - loss: 2.2599 - val_loss: 2.4887\n",
            "108/108 - 4s - loss: 2.2862 - val_loss: 2.4767\n",
            "108/108 - 4s - loss: 2.2467 - val_loss: 2.5316\n",
            "108/108 - 3s - loss: 2.2905 - val_loss: 2.4586\n",
            "108/108 - 3s - loss: 2.2463 - val_loss: 2.4646\n",
            "108/108 - 3s - loss: 2.2618 - val_loss: 2.4785\n",
            "108/108 - 3s - loss: 2.2506 - val_loss: 2.4344\n",
            "108/108 - 4s - loss: 2.2415 - val_loss: 2.4464\n",
            "108/108 - 3s - loss: 2.2486 - val_loss: 2.5000\n",
            "108/108 - 3s - loss: 2.2570 - val_loss: 2.4590\n",
            "108/108 - 3s - loss: 2.2534 - val_loss: 2.4448\n",
            "108/108 - 3s - loss: 2.2408 - val_loss: 2.4021\n",
            "108/108 - 3s - loss: 2.2733 - val_loss: 2.4595\n",
            "108/108 - 3s - loss: 2.2513 - val_loss: 2.4792\n",
            "108/108 - 3s - loss: 2.2340 - val_loss: 2.3963\n",
            "108/108 - 3s - loss: 2.2509 - val_loss: 2.4352\n",
            "108/108 - 3s - loss: 2.2538 - val_loss: 2.4300\n",
            "108/108 - 3s - loss: 2.3261 - val_loss: 2.4307\n",
            "108/108 - 3s - loss: 2.3101 - val_loss: 2.3821\n",
            "108/108 - 3s - loss: 2.2535 - val_loss: 2.4193\n",
            "108/108 - 4s - loss: 2.2690 - val_loss: 2.4455\n",
            "108/108 - 3s - loss: 2.2504 - val_loss: 2.4683\n",
            "108/108 - 3s - loss: 2.2208 - val_loss: 2.3779\n",
            "108/108 - 3s - loss: 2.2968 - val_loss: 2.4222\n",
            "108/108 - 3s - loss: 2.2687 - val_loss: 2.4356\n",
            "108/108 - 3s - loss: 2.2534 - val_loss: 2.4139\n",
            "108/108 - 3s - loss: 2.2290 - val_loss: 2.4206\n",
            "108/108 - 3s - loss: 2.2689 - val_loss: 2.4157\n",
            "108/108 - 3s - loss: 2.2336 - val_loss: 2.3888\n",
            "108/108 - 3s - loss: 2.2005 - val_loss: 2.4276\n",
            "108/108 - 3s - loss: 2.2397 - val_loss: 2.4144\n",
            "108/108 - 4s - loss: 2.2527 - val_loss: 2.4823\n",
            "108/108 - 3s - loss: 2.2642 - val_loss: 2.3916\n",
            "108/108 - 3s - loss: 2.2550 - val_loss: 2.4868\n",
            "108/108 - 3s - loss: 2.2225 - val_loss: 2.4474\n",
            "108/108 - 3s - loss: 2.2279 - val_loss: 2.3944\n",
            "108/108 - 3s - loss: 2.2769 - val_loss: 2.4425\n",
            "108/108 - 3s - loss: 2.2206 - val_loss: 2.4012\n",
            "108/108 - 3s - loss: 2.2323 - val_loss: 2.3956\n",
            "108/108 - 3s - loss: 2.2785 - val_loss: 2.4562\n",
            "108/108 - 3s - loss: 2.2395 - val_loss: 2.4596\n",
            "108/108 - 4s - loss: 2.2645 - val_loss: 2.4167\n",
            "108/108 - 4s - loss: 2.2444 - val_loss: 2.4737\n",
            "108/108 - 4s - loss: 2.2420 - val_loss: 2.4563\n",
            "108/108 - 4s - loss: 2.2604 - val_loss: 2.4860\n",
            "108/108 - 4s - loss: 2.2413 - val_loss: 2.4051\n",
            "108/108 - 3s - loss: 2.2385 - val_loss: 2.4259\n",
            "108/108 - 3s - loss: 2.2254 - val_loss: 2.4114\n",
            "108/108 - 3s - loss: 2.2459 - val_loss: 2.5066\n",
            "108/108 - 4s - loss: 2.2313 - val_loss: 2.4242\n",
            "108/108 - 4s - loss: 2.2678 - val_loss: 2.4491\n",
            "108/108 - 4s - loss: 2.2063 - val_loss: 2.3716\n",
            "108/108 - 3s - loss: 2.2256 - val_loss: 2.3636\n",
            "108/108 - 3s - loss: 2.2159 - val_loss: 2.4300\n",
            "108/108 - 3s - loss: 2.2473 - val_loss: 2.4356\n",
            "108/108 - 3s - loss: 2.2154 - val_loss: 2.4504\n",
            "108/108 - 3s - loss: 2.2163 - val_loss: 2.4455\n",
            "108/108 - 3s - loss: 2.2225 - val_loss: 2.3956\n",
            "108/108 - 4s - loss: 2.2343 - val_loss: 2.4719\n",
            "108/108 - 3s - loss: 2.2259 - val_loss: 2.4257\n",
            "108/108 - 3s - loss: 2.2255 - val_loss: 2.5045\n",
            "108/108 - 3s - loss: 2.2152 - val_loss: 2.5064\n",
            "108/108 - 3s - loss: 2.2195 - val_loss: 2.3589\n",
            "108/108 - 3s - loss: 2.2327 - val_loss: 2.4368\n",
            "108/108 - 3s - loss: 2.2196 - val_loss: 2.4038\n",
            "108/108 - 3s - loss: 2.2236 - val_loss: 2.3934\n",
            "108/108 - 4s - loss: 2.2371 - val_loss: 2.4100\n",
            "108/108 - 3s - loss: 2.2486 - val_loss: 2.4650\n",
            "108/108 - 3s - loss: 2.2145 - val_loss: 2.4349\n",
            "108/108 - 3s - loss: 2.2298 - val_loss: 2.4600\n",
            "108/108 - 3s - loss: 2.2192 - val_loss: 2.4945\n",
            "108/108 - 3s - loss: 2.1990 - val_loss: 2.4666\n",
            "108/108 - 3s - loss: 2.2232 - val_loss: 2.4189\n",
            "108/108 - 3s - loss: 2.2261 - val_loss: 2.3989\n",
            "108/108 - 3s - loss: 2.1761 - val_loss: 2.4588\n",
            "108/108 - 3s - loss: 2.1810 - val_loss: 2.4214\n",
            "108/108 - 4s - loss: 2.1858 - val_loss: 2.4116\n",
            "108/108 - 4s - loss: 2.2326 - val_loss: 2.4224\n",
            "108/108 - 3s - loss: 2.1916 - val_loss: 2.4421\n",
            "108/108 - 4s - loss: 2.2161 - val_loss: 2.4298\n",
            "108/108 - 3s - loss: 2.1837 - val_loss: 2.4577\n",
            "108/108 - 3s - loss: 2.2653 - val_loss: 2.4769\n",
            "108/108 - 3s - loss: 2.2705 - val_loss: 2.4019\n",
            "108/108 - 4s - loss: 2.2127 - val_loss: 2.4551\n",
            "108/108 - 3s - loss: 2.2198 - val_loss: 2.4548\n",
            "108/108 - 3s - loss: 2.2189 - val_loss: 2.4279\n",
            "108/108 - 4s - loss: 2.2666 - val_loss: 2.4372\n",
            "108/108 - 4s - loss: 2.2290 - val_loss: 2.3826\n",
            "108/108 - 4s - loss: 2.2220 - val_loss: 2.4706\n",
            "108/108 - 4s - loss: 2.1809 - val_loss: 2.4190\n",
            "108/108 - 4s - loss: 2.1908 - val_loss: 2.4777\n",
            "108/108 - 4s - loss: 2.2006 - val_loss: 2.4775\n",
            "108/108 - 4s - loss: 2.2109 - val_loss: 2.4278\n",
            "108/108 - 4s - loss: 2.2041 - val_loss: 2.4483\n",
            "108/108 - 4s - loss: 2.1991 - val_loss: 2.4425\n",
            "108/108 - 3s - loss: 2.1863 - val_loss: 2.4645\n",
            "108/108 - 4s - loss: 2.1830 - val_loss: 2.4706\n",
            "108/108 - 4s - loss: 2.1865 - val_loss: 2.4742\n",
            "108/108 - 3s - loss: 2.2059 - val_loss: 2.3613\n",
            "108/108 - 3s - loss: 2.2242 - val_loss: 2.3860\n",
            "108/108 - 3s - loss: 2.2398 - val_loss: 2.3688\n",
            "108/108 - 3s - loss: 2.2054 - val_loss: 2.4453\n",
            "108/108 - 3s - loss: 2.1767 - val_loss: 2.3974\n",
            "108/108 - 3s - loss: 2.1802 - val_loss: 2.4116\n",
            "108/108 - 3s - loss: 2.1850 - val_loss: 2.4176\n",
            "108/108 - 3s - loss: 2.1875 - val_loss: 2.4265\n",
            "108/108 - 3s - loss: 2.2012 - val_loss: 2.3759\n",
            "108/108 - 3s - loss: 2.2218 - val_loss: 2.4841\n",
            "108/108 - 3s - loss: 2.2191 - val_loss: 2.4919\n",
            "108/108 - 4s - loss: 2.2023 - val_loss: 2.4464\n",
            "108/108 - 3s - loss: 2.1801 - val_loss: 2.5387\n",
            "108/108 - 3s - loss: 2.2113 - val_loss: 2.4233\n",
            "108/108 - 3s - loss: 2.1830 - val_loss: 2.3966\n",
            "108/108 - 3s - loss: 2.2070 - val_loss: 2.4502\n",
            "108/108 - 4s - loss: 2.1874 - val_loss: 2.3888\n",
            "108/108 - 4s - loss: 2.1805 - val_loss: 2.4523\n",
            "0.7000000000000001\n",
            "108/108 - 7s - loss: 11.5191 - val_loss: 9.5810\n",
            "108/108 - 3s - loss: 9.4889 - val_loss: 8.0005\n",
            "108/108 - 3s - loss: 8.1082 - val_loss: 6.8042\n",
            "108/108 - 4s - loss: 7.0029 - val_loss: 5.8645\n",
            "108/108 - 3s - loss: 6.1353 - val_loss: 5.1318\n",
            "108/108 - 3s - loss: 5.4356 - val_loss: 4.5833\n",
            "108/108 - 3s - loss: 4.9245 - val_loss: 4.1863\n",
            "108/108 - 3s - loss: 4.5716 - val_loss: 3.9179\n",
            "108/108 - 3s - loss: 4.2910 - val_loss: 3.7351\n",
            "108/108 - 3s - loss: 4.1048 - val_loss: 3.6157\n",
            "108/108 - 3s - loss: 4.0141 - val_loss: 3.5352\n",
            "108/108 - 3s - loss: 3.9003 - val_loss: 3.4831\n",
            "108/108 - 4s - loss: 3.8714 - val_loss: 3.4507\n",
            "108/108 - 4s - loss: 3.7684 - val_loss: 3.4328\n",
            "108/108 - 4s - loss: 3.7146 - val_loss: 3.4235\n",
            "108/108 - 3s - loss: 3.7485 - val_loss: 3.4199\n",
            "108/108 - 3s - loss: 3.6688 - val_loss: 3.4200\n",
            "108/108 - 3s - loss: 3.7293 - val_loss: 3.4217\n",
            "108/108 - 3s - loss: 3.6835 - val_loss: 3.4228\n",
            "108/108 - 3s - loss: 3.6354 - val_loss: 3.4256\n",
            "108/108 - 3s - loss: 3.7398 - val_loss: 3.4276\n",
            "108/108 - 3s - loss: 3.6906 - val_loss: 3.4282\n",
            "108/108 - 3s - loss: 3.6930 - val_loss: 3.4302\n",
            "108/108 - 3s - loss: 3.6928 - val_loss: 3.4328\n",
            "108/108 - 3s - loss: 3.6984 - val_loss: 3.4340\n",
            "108/108 - 3s - loss: 3.6774 - val_loss: 3.4347\n",
            "108/108 - 3s - loss: 3.7062 - val_loss: 3.4361\n",
            "108/108 - 3s - loss: 3.6741 - val_loss: 3.4368\n",
            "108/108 - 4s - loss: 3.6863 - val_loss: 3.4370\n",
            "108/108 - 3s - loss: 3.7081 - val_loss: 3.4359\n",
            "108/108 - 3s - loss: 3.6983 - val_loss: 3.4338\n",
            "108/108 - 4s - loss: 3.6930 - val_loss: 3.4366\n",
            "108/108 - 3s - loss: 3.7247 - val_loss: 3.4353\n",
            "108/108 - 3s - loss: 3.7335 - val_loss: 3.4341\n",
            "108/108 - 3s - loss: 3.7043 - val_loss: 3.4356\n",
            "108/108 - 3s - loss: 3.3798 - val_loss: 2.8370\n",
            "108/108 - 3s - loss: 3.0480 - val_loss: 2.7509\n",
            "108/108 - 3s - loss: 2.8949 - val_loss: 2.5979\n",
            "108/108 - 4s - loss: 2.7797 - val_loss: 2.6525\n",
            "108/108 - 3s - loss: 2.7198 - val_loss: 2.4540\n",
            "108/108 - 3s - loss: 2.6302 - val_loss: 2.4570\n",
            "108/108 - 4s - loss: 2.5962 - val_loss: 2.4156\n",
            "108/108 - 4s - loss: 2.5546 - val_loss: 2.3915\n",
            "108/108 - 4s - loss: 2.5478 - val_loss: 2.3277\n",
            "108/108 - 3s - loss: 2.5180 - val_loss: 2.2471\n",
            "108/108 - 3s - loss: 2.4751 - val_loss: 2.3892\n",
            "108/108 - 3s - loss: 2.5016 - val_loss: 2.2246\n",
            "108/108 - 3s - loss: 2.4245 - val_loss: 2.2109\n",
            "108/108 - 3s - loss: 2.3931 - val_loss: 2.2050\n",
            "108/108 - 3s - loss: 2.3773 - val_loss: 2.1921\n",
            "108/108 - 3s - loss: 2.4123 - val_loss: 2.1340\n",
            "108/108 - 3s - loss: 2.3420 - val_loss: 2.1723\n",
            "108/108 - 3s - loss: 2.3844 - val_loss: 2.1370\n",
            "108/108 - 3s - loss: 2.3223 - val_loss: 2.1150\n",
            "108/108 - 4s - loss: 2.3427 - val_loss: 2.1132\n",
            "108/108 - 4s - loss: 2.3260 - val_loss: 2.0817\n",
            "108/108 - 4s - loss: 2.3189 - val_loss: 2.0759\n",
            "108/108 - 3s - loss: 2.3136 - val_loss: 2.0612\n",
            "108/108 - 4s - loss: 2.2605 - val_loss: 2.1319\n",
            "108/108 - 3s - loss: 2.3230 - val_loss: 2.0945\n",
            "108/108 - 3s - loss: 2.3308 - val_loss: 2.0825\n",
            "108/108 - 4s - loss: 2.2628 - val_loss: 2.0864\n",
            "108/108 - 3s - loss: 2.2402 - val_loss: 2.0755\n",
            "108/108 - 3s - loss: 2.3145 - val_loss: 2.1375\n",
            "108/108 - 3s - loss: 2.2971 - val_loss: 2.0821\n",
            "108/108 - 3s - loss: 2.2596 - val_loss: 2.1345\n",
            "108/108 - 3s - loss: 2.2745 - val_loss: 2.0831\n",
            "108/108 - 3s - loss: 2.2401 - val_loss: 2.0699\n",
            "108/108 - 3s - loss: 2.2598 - val_loss: 2.1563\n",
            "108/108 - 4s - loss: 2.2546 - val_loss: 2.0756\n",
            "108/108 - 3s - loss: 2.3022 - val_loss: 2.0760\n",
            "108/108 - 3s - loss: 2.3206 - val_loss: 2.0697\n",
            "108/108 - 3s - loss: 2.2629 - val_loss: 2.0617\n",
            "108/108 - 3s - loss: 2.2464 - val_loss: 2.0728\n",
            "108/108 - 4s - loss: 2.2589 - val_loss: 2.1277\n",
            "108/108 - 3s - loss: 2.2775 - val_loss: 2.0519\n",
            "108/108 - 3s - loss: 2.2556 - val_loss: 2.0237\n",
            "108/108 - 3s - loss: 2.2173 - val_loss: 2.0657\n",
            "108/108 - 3s - loss: 2.2273 - val_loss: 2.0959\n",
            "108/108 - 3s - loss: 2.2369 - val_loss: 2.0734\n",
            "108/108 - 3s - loss: 2.2673 - val_loss: 2.0497\n",
            "108/108 - 3s - loss: 2.2465 - val_loss: 2.0342\n",
            "108/108 - 3s - loss: 2.2215 - val_loss: 2.0530\n",
            "108/108 - 3s - loss: 2.2465 - val_loss: 2.0371\n",
            "108/108 - 3s - loss: 2.2462 - val_loss: 2.0656\n",
            "108/108 - 3s - loss: 2.2112 - val_loss: 2.0255\n",
            "108/108 - 3s - loss: 2.2439 - val_loss: 2.0406\n",
            "108/108 - 3s - loss: 2.2442 - val_loss: 2.0463\n",
            "108/108 - 4s - loss: 2.2377 - val_loss: 2.0371\n",
            "108/108 - 3s - loss: 2.2037 - val_loss: 2.0579\n",
            "108/108 - 3s - loss: 2.2167 - val_loss: 2.0508\n",
            "108/108 - 4s - loss: 2.2328 - val_loss: 2.0734\n",
            "108/108 - 4s - loss: 2.2450 - val_loss: 2.0298\n",
            "108/108 - 3s - loss: 2.2223 - val_loss: 2.0523\n",
            "108/108 - 3s - loss: 2.2171 - val_loss: 2.0803\n",
            "108/108 - 3s - loss: 2.2422 - val_loss: 2.0369\n",
            "108/108 - 3s - loss: 2.2526 - val_loss: 2.0569\n",
            "108/108 - 3s - loss: 2.2604 - val_loss: 2.0365\n",
            "108/108 - 3s - loss: 2.2438 - val_loss: 2.0271\n",
            "108/108 - 3s - loss: 2.2441 - val_loss: 2.0738\n",
            "108/108 - 4s - loss: 2.2081 - val_loss: 2.0390\n",
            "108/108 - 4s - loss: 2.2236 - val_loss: 2.0293\n",
            "108/108 - 4s - loss: 2.2314 - val_loss: 2.0983\n",
            "108/108 - 3s - loss: 2.2542 - val_loss: 2.0234\n",
            "108/108 - 4s - loss: 2.1828 - val_loss: 2.0656\n",
            "108/108 - 3s - loss: 2.2458 - val_loss: 2.0869\n",
            "108/108 - 3s - loss: 2.1927 - val_loss: 2.0579\n",
            "108/108 - 3s - loss: 2.2071 - val_loss: 2.0704\n",
            "108/108 - 3s - loss: 2.1912 - val_loss: 2.0196\n",
            "108/108 - 3s - loss: 2.2216 - val_loss: 2.0365\n",
            "108/108 - 3s - loss: 2.2378 - val_loss: 2.0165\n",
            "108/108 - 3s - loss: 2.2141 - val_loss: 2.0363\n",
            "108/108 - 3s - loss: 2.2394 - val_loss: 2.0249\n",
            "108/108 - 3s - loss: 2.1987 - val_loss: 2.0934\n",
            "108/108 - 3s - loss: 2.2020 - val_loss: 2.0343\n",
            "108/108 - 3s - loss: 2.1902 - val_loss: 2.0265\n",
            "108/108 - 3s - loss: 2.2101 - val_loss: 2.0352\n",
            "108/108 - 3s - loss: 2.1870 - val_loss: 2.0420\n",
            "108/108 - 3s - loss: 2.1828 - val_loss: 2.0334\n",
            "108/108 - 3s - loss: 2.1918 - val_loss: 2.0193\n",
            "108/108 - 3s - loss: 2.2351 - val_loss: 2.0597\n",
            "108/108 - 3s - loss: 2.2109 - val_loss: 2.0453\n",
            "108/108 - 3s - loss: 2.1734 - val_loss: 2.0594\n",
            "108/108 - 3s - loss: 2.1866 - val_loss: 2.0192\n",
            "108/108 - 3s - loss: 2.2155 - val_loss: 2.0336\n",
            "108/108 - 3s - loss: 2.2257 - val_loss: 2.0850\n",
            "108/108 - 3s - loss: 2.2053 - val_loss: 2.0106\n",
            "108/108 - 4s - loss: 2.2093 - val_loss: 2.0402\n",
            "108/108 - 4s - loss: 2.2052 - val_loss: 2.0253\n",
            "108/108 - 3s - loss: 2.1894 - val_loss: 2.0661\n",
            "108/108 - 3s - loss: 2.1895 - val_loss: 2.0144\n",
            "108/108 - 3s - loss: 2.1953 - val_loss: 2.0317\n",
            "108/108 - 3s - loss: 2.1812 - val_loss: 2.0096\n",
            "108/108 - 3s - loss: 2.2017 - val_loss: 2.0167\n",
            "108/108 - 3s - loss: 2.1974 - val_loss: 2.0519\n",
            "108/108 - 3s - loss: 2.2265 - val_loss: 2.0261\n",
            "108/108 - 3s - loss: 2.2142 - val_loss: 2.0276\n",
            "108/108 - 4s - loss: 2.2087 - val_loss: 2.0172\n",
            "108/108 - 4s - loss: 2.1907 - val_loss: 2.0602\n",
            "108/108 - 4s - loss: 2.1941 - val_loss: 2.0098\n",
            "108/108 - 4s - loss: 2.1658 - val_loss: 2.0207\n",
            "108/108 - 4s - loss: 2.1934 - val_loss: 2.0291\n",
            "108/108 - 4s - loss: 2.1819 - val_loss: 2.0098\n",
            "108/108 - 4s - loss: 2.2075 - val_loss: 2.0609\n",
            "108/108 - 4s - loss: 2.1793 - val_loss: 2.0405\n",
            "108/108 - 3s - loss: 2.2158 - val_loss: 2.0473\n",
            "108/108 - 3s - loss: 2.1834 - val_loss: 2.0174\n",
            "108/108 - 4s - loss: 2.1747 - val_loss: 2.0425\n",
            "108/108 - 4s - loss: 2.1720 - val_loss: 2.0206\n",
            "108/108 - 3s - loss: 2.1597 - val_loss: 2.0210\n",
            "108/108 - 3s - loss: 2.1664 - val_loss: 2.0289\n",
            "108/108 - 3s - loss: 2.1743 - val_loss: 2.0003\n",
            "108/108 - 4s - loss: 2.1563 - val_loss: 2.0275\n",
            "108/108 - 4s - loss: 2.1944 - val_loss: 2.0138\n",
            "108/108 - 4s - loss: 2.1781 - val_loss: 2.0114\n",
            "108/108 - 4s - loss: 2.1528 - val_loss: 2.0549\n",
            "108/108 - 4s - loss: 2.1662 - val_loss: 1.9966\n",
            "108/108 - 4s - loss: 2.2296 - val_loss: 2.0436\n",
            "108/108 - 4s - loss: 2.1544 - val_loss: 2.0095\n",
            "108/108 - 4s - loss: 2.1623 - val_loss: 2.0463\n",
            "108/108 - 4s - loss: 2.1817 - val_loss: 2.0248\n",
            "108/108 - 4s - loss: 2.1965 - val_loss: 2.0619\n",
            "108/108 - 3s - loss: 2.1437 - val_loss: 2.0466\n",
            "108/108 - 3s - loss: 2.1604 - val_loss: 2.0234\n",
            "108/108 - 3s - loss: 2.1574 - val_loss: 2.0338\n",
            "108/108 - 3s - loss: 2.1757 - val_loss: 2.0262\n",
            "108/108 - 4s - loss: 2.1796 - val_loss: 2.0238\n",
            "108/108 - 3s - loss: 2.1386 - val_loss: 2.0315\n",
            "108/108 - 4s - loss: 2.2127 - val_loss: 2.0143\n",
            "108/108 - 4s - loss: 2.1905 - val_loss: 2.0354\n",
            "108/108 - 4s - loss: 2.1746 - val_loss: 2.0395\n",
            "108/108 - 4s - loss: 2.1388 - val_loss: 2.0308\n",
            "108/108 - 4s - loss: 2.1767 - val_loss: 2.0431\n",
            "108/108 - 3s - loss: 2.1618 - val_loss: 2.0723\n",
            "108/108 - 3s - loss: 2.1547 - val_loss: 2.0449\n",
            "108/108 - 3s - loss: 2.1500 - val_loss: 2.0507\n",
            "108/108 - 3s - loss: 2.1633 - val_loss: 2.0132\n",
            "108/108 - 4s - loss: 2.1404 - val_loss: 2.0006\n",
            "108/108 - 3s - loss: 2.1939 - val_loss: 2.0071\n",
            "108/108 - 3s - loss: 2.1436 - val_loss: 2.0191\n",
            "108/108 - 4s - loss: 2.1357 - val_loss: 2.0348\n",
            "108/108 - 3s - loss: 2.1565 - val_loss: 2.0487\n",
            "108/108 - 3s - loss: 2.1614 - val_loss: 2.0227\n",
            "108/108 - 4s - loss: 2.1597 - val_loss: 2.0320\n",
            "108/108 - 3s - loss: 2.1214 - val_loss: 2.0705\n",
            "108/108 - 4s - loss: 2.1611 - val_loss: 2.0147\n",
            "108/108 - 4s - loss: 2.1570 - val_loss: 2.0173\n",
            "108/108 - 4s - loss: 2.1446 - val_loss: 2.0017\n",
            "108/108 - 4s - loss: 2.1353 - val_loss: 2.0125\n",
            "108/108 - 4s - loss: 2.1362 - val_loss: 1.9953\n",
            "108/108 - 4s - loss: 2.1087 - val_loss: 2.0269\n",
            "108/108 - 4s - loss: 2.1215 - val_loss: 2.0417\n",
            "108/108 - 4s - loss: 2.1412 - val_loss: 2.0205\n",
            "108/108 - 4s - loss: 2.1508 - val_loss: 2.0422\n",
            "108/108 - 4s - loss: 2.1305 - val_loss: 1.9964\n",
            "108/108 - 4s - loss: 2.1594 - val_loss: 2.0268\n",
            "108/108 - 4s - loss: 2.1748 - val_loss: 2.0071\n",
            "108/108 - 4s - loss: 2.1456 - val_loss: 2.0297\n",
            "108/108 - 4s - loss: 2.1272 - val_loss: 2.0237\n",
            "108/108 - 4s - loss: 2.1403 - val_loss: 2.0873\n",
            "108/108 - 4s - loss: 2.1203 - val_loss: 2.0173\n",
            "108/108 - 4s - loss: 2.1077 - val_loss: 2.0215\n",
            "108/108 - 4s - loss: 2.1263 - val_loss: 2.0332\n",
            "108/108 - 4s - loss: 2.1152 - val_loss: 2.0439\n",
            "108/108 - 4s - loss: 2.1364 - val_loss: 2.0556\n",
            "108/108 - 4s - loss: 2.1222 - val_loss: 2.0845\n",
            "108/108 - 4s - loss: 2.0970 - val_loss: 2.0418\n",
            "108/108 - 4s - loss: 2.1652 - val_loss: 2.0214\n",
            "108/108 - 4s - loss: 2.0858 - val_loss: 2.0201\n",
            "108/108 - 4s - loss: 2.1177 - val_loss: 2.0294\n",
            "108/108 - 4s - loss: 2.0969 - val_loss: 2.0412\n",
            "108/108 - 4s - loss: 2.1035 - val_loss: 2.0309\n",
            "108/108 - 4s - loss: 2.0961 - val_loss: 2.0253\n",
            "108/108 - 4s - loss: 2.1451 - val_loss: 2.0303\n",
            "108/108 - 4s - loss: 2.1236 - val_loss: 1.9953\n",
            "108/108 - 4s - loss: 2.1151 - val_loss: 2.0763\n",
            "108/108 - 4s - loss: 2.1011 - val_loss: 2.0053\n",
            "108/108 - 3s - loss: 2.1458 - val_loss: 2.0226\n",
            "108/108 - 4s - loss: 2.1262 - val_loss: 2.0403\n",
            "108/108 - 4s - loss: 2.1612 - val_loss: 2.0203\n",
            "108/108 - 4s - loss: 2.0897 - val_loss: 1.9961\n",
            "108/108 - 4s - loss: 2.0927 - val_loss: 2.0017\n",
            "108/108 - 4s - loss: 2.1083 - val_loss: 2.0143\n",
            "108/108 - 3s - loss: 2.1159 - val_loss: 2.0416\n",
            "108/108 - 4s - loss: 2.0764 - val_loss: 2.0018\n",
            "108/108 - 3s - loss: 2.1368 - val_loss: 2.0420\n",
            "108/108 - 3s - loss: 2.0719 - val_loss: 1.9921\n",
            "108/108 - 4s - loss: 2.1005 - val_loss: 2.0161\n",
            "108/108 - 4s - loss: 2.0946 - val_loss: 2.0145\n",
            "108/108 - 3s - loss: 2.1204 - val_loss: 2.0401\n",
            "108/108 - 3s - loss: 2.1150 - val_loss: 2.0488\n",
            "108/108 - 3s - loss: 2.1057 - val_loss: 2.0201\n",
            "108/108 - 3s - loss: 2.1113 - val_loss: 2.0394\n",
            "108/108 - 4s - loss: 2.1363 - val_loss: 2.0209\n",
            "108/108 - 3s - loss: 2.0978 - val_loss: 2.0339\n",
            "108/108 - 3s - loss: 2.1056 - val_loss: 2.0618\n",
            "108/108 - 3s - loss: 2.0756 - val_loss: 2.0446\n",
            "108/108 - 3s - loss: 2.0932 - val_loss: 2.0750\n",
            "108/108 - 3s - loss: 2.1221 - val_loss: 2.0107\n",
            "108/108 - 3s - loss: 2.0797 - val_loss: 2.0116\n",
            "108/108 - 3s - loss: 2.1216 - val_loss: 2.0412\n",
            "108/108 - 3s - loss: 2.0649 - val_loss: 2.0224\n",
            "108/108 - 3s - loss: 2.0955 - val_loss: 2.0095\n",
            "108/108 - 3s - loss: 2.0516 - val_loss: 2.0423\n",
            "108/108 - 4s - loss: 2.0891 - val_loss: 2.0081\n",
            "108/108 - 3s - loss: 2.0695 - val_loss: 2.0402\n",
            "108/108 - 3s - loss: 2.0745 - val_loss: 2.0169\n",
            "108/108 - 3s - loss: 2.0912 - val_loss: 2.0021\n",
            "108/108 - 3s - loss: 2.1048 - val_loss: 2.0105\n",
            "108/108 - 3s - loss: 2.0649 - val_loss: 1.9888\n",
            "108/108 - 3s - loss: 2.0706 - val_loss: 2.0094\n",
            "108/108 - 3s - loss: 2.0929 - val_loss: 1.9937\n",
            "108/108 - 3s - loss: 2.0680 - val_loss: 2.0198\n",
            "108/108 - 4s - loss: 2.0528 - val_loss: 2.0023\n",
            "108/108 - 3s - loss: 2.0583 - val_loss: 2.0085\n",
            "108/108 - 3s - loss: 2.0540 - val_loss: 1.9990\n",
            "108/108 - 3s - loss: 2.0799 - val_loss: 1.9833\n",
            "108/108 - 3s - loss: 2.1010 - val_loss: 1.9938\n",
            "108/108 - 3s - loss: 2.0692 - val_loss: 2.0365\n",
            "108/108 - 3s - loss: 2.1123 - val_loss: 2.1264\n",
            "108/108 - 3s - loss: 2.0802 - val_loss: 2.0268\n",
            "108/108 - 3s - loss: 2.0648 - val_loss: 2.0635\n",
            "108/108 - 4s - loss: 2.0543 - val_loss: 2.0376\n",
            "108/108 - 4s - loss: 2.0387 - val_loss: 2.0331\n",
            "108/108 - 3s - loss: 2.0695 - val_loss: 2.0217\n",
            "108/108 - 3s - loss: 2.0489 - val_loss: 2.0698\n",
            "108/108 - 3s - loss: 2.0602 - val_loss: 2.0160\n",
            "108/108 - 4s - loss: 2.0550 - val_loss: 2.0089\n",
            "108/108 - 3s - loss: 2.0679 - val_loss: 2.0323\n",
            "108/108 - 4s - loss: 2.0659 - val_loss: 2.0534\n",
            "108/108 - 4s - loss: 2.0303 - val_loss: 2.0809\n",
            "108/108 - 3s - loss: 2.0597 - val_loss: 2.0617\n",
            "108/108 - 4s - loss: 2.0400 - val_loss: 2.0286\n",
            "108/108 - 4s - loss: 2.0608 - val_loss: 2.0669\n",
            "108/108 - 4s - loss: 2.0460 - val_loss: 2.0164\n",
            "108/108 - 4s - loss: 2.0581 - val_loss: 2.0314\n",
            "108/108 - 4s - loss: 2.0470 - val_loss: 2.0234\n",
            "108/108 - 4s - loss: 2.0882 - val_loss: 2.0404\n",
            "108/108 - 4s - loss: 2.0556 - val_loss: 2.0140\n",
            "108/108 - 4s - loss: 2.0542 - val_loss: 2.0074\n",
            "108/108 - 3s - loss: 2.0469 - val_loss: 2.0803\n",
            "108/108 - 3s - loss: 2.0523 - val_loss: 1.9954\n",
            "108/108 - 4s - loss: 2.0625 - val_loss: 2.0306\n",
            "108/108 - 3s - loss: 2.0363 - val_loss: 2.0103\n",
            "108/108 - 3s - loss: 2.0916 - val_loss: 2.0544\n",
            "108/108 - 3s - loss: 2.0946 - val_loss: 1.9961\n",
            "108/108 - 3s - loss: 2.0778 - val_loss: 1.9964\n",
            "108/108 - 3s - loss: 2.0538 - val_loss: 2.0211\n",
            "108/108 - 3s - loss: 2.0326 - val_loss: 2.0387\n",
            "108/108 - 3s - loss: 2.0302 - val_loss: 2.0980\n",
            "108/108 - 3s - loss: 2.0686 - val_loss: 2.0271\n",
            "108/108 - 3s - loss: 2.0335 - val_loss: 2.0215\n",
            "108/108 - 3s - loss: 2.0561 - val_loss: 2.0058\n",
            "108/108 - 3s - loss: 2.0480 - val_loss: 1.9882\n",
            "108/108 - 3s - loss: 2.0434 - val_loss: 2.0477\n",
            "108/108 - 3s - loss: 2.0754 - val_loss: 2.0881\n",
            "108/108 - 3s - loss: 2.0535 - val_loss: 2.0759\n",
            "108/108 - 4s - loss: 2.0294 - val_loss: 2.0725\n",
            "108/108 - 3s - loss: 2.0551 - val_loss: 2.0255\n",
            "108/108 - 3s - loss: 2.0566 - val_loss: 2.0735\n",
            "108/108 - 3s - loss: 2.0922 - val_loss: 2.0105\n",
            "108/108 - 3s - loss: 2.0442 - val_loss: 2.0504\n",
            "108/108 - 3s - loss: 2.0364 - val_loss: 2.0230\n",
            "108/108 - 3s - loss: 2.0382 - val_loss: 2.0234\n",
            "108/108 - 3s - loss: 2.0373 - val_loss: 2.0507\n",
            "108/108 - 3s - loss: 2.0732 - val_loss: 2.0582\n",
            "108/108 - 3s - loss: 2.0481 - val_loss: 2.0771\n",
            "108/108 - 3s - loss: 2.0512 - val_loss: 2.0761\n",
            "108/108 - 4s - loss: 2.0320 - val_loss: 2.0925\n",
            "108/108 - 4s - loss: 2.0635 - val_loss: 2.0184\n",
            "108/108 - 3s - loss: 2.0480 - val_loss: 2.0449\n",
            "108/108 - 3s - loss: 2.0352 - val_loss: 2.0423\n",
            "108/108 - 3s - loss: 2.0503 - val_loss: 2.0655\n",
            "108/108 - 3s - loss: 2.0226 - val_loss: 2.0185\n",
            "108/108 - 4s - loss: 2.0189 - val_loss: 2.0779\n",
            "108/108 - 4s - loss: 2.0107 - val_loss: 2.0531\n",
            "108/108 - 3s - loss: 2.0496 - val_loss: 2.0827\n",
            "108/108 - 3s - loss: 2.0461 - val_loss: 2.0909\n",
            "108/108 - 3s - loss: 2.0335 - val_loss: 2.0771\n",
            "108/108 - 3s - loss: 2.0613 - val_loss: 2.1206\n",
            "108/108 - 4s - loss: 2.0374 - val_loss: 2.0550\n",
            "108/108 - 3s - loss: 2.0495 - val_loss: 2.0463\n",
            "108/108 - 3s - loss: 2.0185 - val_loss: 2.0740\n",
            "108/108 - 3s - loss: 2.0089 - val_loss: 2.0325\n",
            "108/108 - 3s - loss: 2.0296 - val_loss: 2.0145\n",
            "108/108 - 3s - loss: 2.0019 - val_loss: 2.0777\n",
            "108/108 - 3s - loss: 2.0347 - val_loss: 2.0892\n",
            "108/108 - 3s - loss: 2.0244 - val_loss: 2.0595\n",
            "108/108 - 3s - loss: 2.0570 - val_loss: 2.0540\n",
            "108/108 - 4s - loss: 2.0400 - val_loss: 2.0543\n",
            "108/108 - 3s - loss: 2.0567 - val_loss: 2.0541\n",
            "108/108 - 3s - loss: 2.0179 - val_loss: 2.0314\n",
            "108/108 - 3s - loss: 2.0068 - val_loss: 1.9944\n",
            "108/108 - 3s - loss: 2.0431 - val_loss: 2.0669\n",
            "108/108 - 3s - loss: 2.0174 - val_loss: 2.1042\n",
            "108/108 - 3s - loss: 2.0455 - val_loss: 2.0584\n",
            "108/108 - 3s - loss: 2.0264 - val_loss: 2.0623\n",
            "108/108 - 3s - loss: 2.0128 - val_loss: 2.0320\n",
            "108/108 - 3s - loss: 2.0178 - val_loss: 2.0360\n",
            "108/108 - 3s - loss: 2.0040 - val_loss: 1.9979\n",
            "108/108 - 3s - loss: 2.0366 - val_loss: 2.0670\n",
            "108/108 - 3s - loss: 2.0236 - val_loss: 2.0284\n",
            "108/108 - 3s - loss: 2.0137 - val_loss: 2.0144\n",
            "108/108 - 3s - loss: 2.0082 - val_loss: 2.0163\n",
            "108/108 - 3s - loss: 2.0556 - val_loss: 2.0279\n",
            "108/108 - 3s - loss: 2.0299 - val_loss: 2.0113\n",
            "108/108 - 3s - loss: 2.0066 - val_loss: 2.0120\n",
            "108/108 - 3s - loss: 2.0095 - val_loss: 2.0393\n",
            "108/108 - 3s - loss: 1.9774 - val_loss: 1.9907\n",
            "108/108 - 3s - loss: 2.0646 - val_loss: 2.0087\n",
            "108/108 - 3s - loss: 2.0118 - val_loss: 2.0315\n",
            "108/108 - 3s - loss: 2.0477 - val_loss: 2.0328\n",
            "108/108 - 3s - loss: 2.0387 - val_loss: 2.0393\n",
            "108/108 - 3s - loss: 2.0156 - val_loss: 2.0227\n",
            "108/108 - 3s - loss: 2.0219 - val_loss: 2.0712\n",
            "108/108 - 3s - loss: 2.0321 - val_loss: 2.0151\n",
            "108/108 - 4s - loss: 2.0126 - val_loss: 2.0290\n",
            "108/108 - 3s - loss: 2.0097 - val_loss: 2.0456\n",
            "108/108 - 3s - loss: 2.0334 - val_loss: 2.0335\n",
            "108/108 - 3s - loss: 2.0215 - val_loss: 2.0529\n",
            "108/108 - 3s - loss: 2.0286 - val_loss: 2.0295\n",
            "108/108 - 3s - loss: 1.9962 - val_loss: 2.0173\n",
            "108/108 - 3s - loss: 2.0111 - val_loss: 2.0357\n",
            "108/108 - 3s - loss: 2.0086 - val_loss: 2.0387\n",
            "108/108 - 4s - loss: 2.0094 - val_loss: 2.0573\n",
            "108/108 - 4s - loss: 2.0112 - val_loss: 2.0348\n",
            "108/108 - 4s - loss: 1.9966 - val_loss: 2.0409\n",
            "108/108 - 3s - loss: 2.0135 - val_loss: 2.0397\n",
            "108/108 - 3s - loss: 1.9701 - val_loss: 2.0308\n",
            "108/108 - 3s - loss: 2.0050 - val_loss: 2.0085\n",
            "108/108 - 3s - loss: 1.9859 - val_loss: 2.0096\n",
            "108/108 - 3s - loss: 2.0044 - val_loss: 2.0424\n",
            "108/108 - 3s - loss: 1.9940 - val_loss: 2.0413\n",
            "108/108 - 3s - loss: 2.0130 - val_loss: 2.0067\n",
            "108/108 - 3s - loss: 2.0105 - val_loss: 2.0566\n",
            "108/108 - 3s - loss: 2.0309 - val_loss: 2.0639\n",
            "108/108 - 3s - loss: 1.9921 - val_loss: 2.0662\n",
            "108/108 - 3s - loss: 2.0224 - val_loss: 2.0644\n",
            "108/108 - 3s - loss: 2.0312 - val_loss: 2.0422\n",
            "108/108 - 3s - loss: 2.0133 - val_loss: 2.0350\n",
            "108/108 - 3s - loss: 2.0205 - val_loss: 2.0279\n",
            "108/108 - 4s - loss: 1.9849 - val_loss: 2.0097\n",
            "108/108 - 3s - loss: 1.9951 - val_loss: 2.0401\n",
            "108/108 - 3s - loss: 1.9716 - val_loss: 1.9988\n",
            "108/108 - 3s - loss: 2.0420 - val_loss: 2.1141\n",
            "108/108 - 3s - loss: 2.0432 - val_loss: 2.0458\n",
            "108/108 - 3s - loss: 2.0332 - val_loss: 2.0467\n",
            "108/108 - 3s - loss: 2.0083 - val_loss: 2.0274\n",
            "108/108 - 3s - loss: 1.9881 - val_loss: 2.0988\n",
            "108/108 - 3s - loss: 2.0030 - val_loss: 1.9992\n",
            "108/108 - 3s - loss: 1.9971 - val_loss: 2.0068\n",
            "108/108 - 4s - loss: 2.0153 - val_loss: 2.0736\n",
            "108/108 - 3s - loss: 2.0023 - val_loss: 2.0339\n",
            "108/108 - 3s - loss: 1.9881 - val_loss: 2.0760\n",
            "108/108 - 3s - loss: 2.0230 - val_loss: 2.0428\n",
            "108/108 - 3s - loss: 2.0104 - val_loss: 2.0258\n",
            "108/108 - 3s - loss: 2.0078 - val_loss: 2.0406\n",
            "108/108 - 3s - loss: 2.1077 - val_loss: 2.0347\n",
            "108/108 - 3s - loss: 2.0447 - val_loss: 2.0005\n",
            "108/108 - 3s - loss: 1.9934 - val_loss: 2.0128\n",
            "108/108 - 3s - loss: 2.0156 - val_loss: 2.0073\n",
            "108/108 - 3s - loss: 2.0190 - val_loss: 2.0139\n",
            "108/108 - 3s - loss: 1.9668 - val_loss: 2.0162\n",
            "108/108 - 3s - loss: 2.0052 - val_loss: 2.0114\n",
            "108/108 - 3s - loss: 1.9726 - val_loss: 1.9727\n",
            "108/108 - 3s - loss: 1.9876 - val_loss: 2.0329\n",
            "108/108 - 3s - loss: 1.9855 - val_loss: 2.0163\n",
            "108/108 - 3s - loss: 2.0077 - val_loss: 2.0160\n",
            "108/108 - 3s - loss: 2.0124 - val_loss: 2.0349\n",
            "108/108 - 3s - loss: 2.0369 - val_loss: 2.0103\n",
            "108/108 - 3s - loss: 1.9934 - val_loss: 2.0083\n",
            "108/108 - 3s - loss: 1.9848 - val_loss: 2.0273\n",
            "108/108 - 3s - loss: 1.9667 - val_loss: 2.0169\n",
            "108/108 - 3s - loss: 1.9921 - val_loss: 2.0565\n",
            "108/108 - 3s - loss: 1.9895 - val_loss: 2.0323\n",
            "108/108 - 3s - loss: 1.9914 - val_loss: 2.0391\n",
            "108/108 - 3s - loss: 2.0038 - val_loss: 2.0296\n",
            "108/108 - 3s - loss: 2.0229 - val_loss: 2.1111\n",
            "108/108 - 3s - loss: 2.0123 - val_loss: 2.0327\n",
            "108/108 - 3s - loss: 2.0003 - val_loss: 2.0014\n",
            "108/108 - 3s - loss: 1.9727 - val_loss: 2.0268\n",
            "108/108 - 4s - loss: 2.0117 - val_loss: 2.0476\n",
            "108/108 - 3s - loss: 1.9990 - val_loss: 2.0073\n",
            "108/108 - 3s - loss: 1.9956 - val_loss: 2.0227\n",
            "108/108 - 3s - loss: 1.9903 - val_loss: 2.0186\n",
            "108/108 - 3s - loss: 2.0108 - val_loss: 2.0356\n",
            "108/108 - 3s - loss: 1.9983 - val_loss: 2.0958\n",
            "108/108 - 3s - loss: 2.0057 - val_loss: 2.0588\n",
            "108/108 - 3s - loss: 1.9805 - val_loss: 2.0008\n",
            "108/108 - 3s - loss: 1.9953 - val_loss: 2.0182\n",
            "108/108 - 3s - loss: 1.9914 - val_loss: 2.0175\n",
            "108/108 - 3s - loss: 1.9450 - val_loss: 2.0789\n",
            "108/108 - 3s - loss: 1.9999 - val_loss: 2.0477\n",
            "108/108 - 3s - loss: 1.9891 - val_loss: 2.0477\n",
            "108/108 - 3s - loss: 2.0051 - val_loss: 2.0386\n",
            "108/108 - 3s - loss: 1.9851 - val_loss: 2.0169\n",
            "108/108 - 3s - loss: 1.9864 - val_loss: 2.0618\n",
            "108/108 - 3s - loss: 1.9900 - val_loss: 2.0696\n",
            "108/108 - 3s - loss: 1.9650 - val_loss: 2.0808\n",
            "108/108 - 3s - loss: 1.9852 - val_loss: 2.0200\n",
            "108/108 - 3s - loss: 2.0181 - val_loss: 2.0466\n",
            "108/108 - 3s - loss: 1.9888 - val_loss: 2.0863\n",
            "108/108 - 3s - loss: 1.9777 - val_loss: 2.0569\n",
            "108/108 - 3s - loss: 2.0147 - val_loss: 2.0771\n",
            "108/108 - 3s - loss: 1.9790 - val_loss: 2.0526\n",
            "108/108 - 3s - loss: 1.9881 - val_loss: 2.0561\n",
            "108/108 - 3s - loss: 1.9595 - val_loss: 2.0723\n",
            "108/108 - 3s - loss: 1.9955 - val_loss: 2.0894\n",
            "108/108 - 3s - loss: 1.9980 - val_loss: 2.0629\n",
            "108/108 - 3s - loss: 1.9705 - val_loss: 2.0485\n",
            "108/108 - 3s - loss: 1.9903 - val_loss: 2.0519\n",
            "108/108 - 3s - loss: 1.9928 - val_loss: 2.0401\n",
            "108/108 - 3s - loss: 1.9724 - val_loss: 2.0633\n",
            "108/108 - 3s - loss: 1.9836 - val_loss: 2.0469\n",
            "108/108 - 4s - loss: 2.0037 - val_loss: 2.0845\n",
            "108/108 - 4s - loss: 1.9879 - val_loss: 2.0881\n",
            "108/108 - 4s - loss: 2.0047 - val_loss: 2.0601\n",
            "108/108 - 3s - loss: 1.9804 - val_loss: 2.1009\n",
            "108/108 - 3s - loss: 1.9909 - val_loss: 2.0361\n",
            "108/108 - 3s - loss: 1.9784 - val_loss: 2.0509\n",
            "108/108 - 3s - loss: 1.9806 - val_loss: 2.0395\n",
            "108/108 - 3s - loss: 1.9790 - val_loss: 2.0550\n",
            "108/108 - 3s - loss: 1.9676 - val_loss: 2.0404\n",
            "108/108 - 3s - loss: 1.9441 - val_loss: 2.0541\n",
            "108/108 - 3s - loss: 1.9692 - val_loss: 2.0104\n",
            "108/108 - 3s - loss: 1.9373 - val_loss: 2.0755\n",
            "108/108 - 3s - loss: 1.9886 - val_loss: 2.0457\n",
            "108/108 - 3s - loss: 1.9721 - val_loss: 2.0865\n",
            "108/108 - 3s - loss: 1.9915 - val_loss: 2.0636\n",
            "108/108 - 3s - loss: 1.9731 - val_loss: 2.0274\n",
            "108/108 - 3s - loss: 2.0009 - val_loss: 2.0685\n",
            "108/108 - 3s - loss: 1.9771 - val_loss: 2.0376\n",
            "108/108 - 3s - loss: 1.9739 - val_loss: 2.0636\n",
            "108/108 - 3s - loss: 1.9716 - val_loss: 2.0427\n",
            "108/108 - 3s - loss: 1.9528 - val_loss: 2.0422\n",
            "108/108 - 3s - loss: 1.9384 - val_loss: 2.0341\n",
            "108/108 - 3s - loss: 1.9834 - val_loss: 2.0086\n",
            "108/108 - 3s - loss: 1.9615 - val_loss: 2.0514\n",
            "108/108 - 4s - loss: 1.9663 - val_loss: 1.9953\n",
            "108/108 - 3s - loss: 1.9410 - val_loss: 1.9998\n",
            "108/108 - 3s - loss: 1.9616 - val_loss: 2.0643\n",
            "108/108 - 3s - loss: 1.9446 - val_loss: 2.0617\n",
            "108/108 - 3s - loss: 1.9958 - val_loss: 2.0421\n",
            "108/108 - 3s - loss: 1.9795 - val_loss: 2.0436\n",
            "108/108 - 3s - loss: 1.9631 - val_loss: 2.0481\n",
            "108/108 - 3s - loss: 1.9680 - val_loss: 1.9989\n",
            "108/108 - 3s - loss: 1.9659 - val_loss: 2.0027\n",
            "108/108 - 3s - loss: 1.9778 - val_loss: 2.0132\n",
            "108/108 - 3s - loss: 1.9555 - val_loss: 1.9937\n",
            "108/108 - 3s - loss: 1.9686 - val_loss: 2.0490\n",
            "108/108 - 4s - loss: 1.9820 - val_loss: 2.0019\n",
            "108/108 - 3s - loss: 1.9939 - val_loss: 1.9760\n",
            "108/108 - 3s - loss: 1.9471 - val_loss: 2.0276\n",
            "108/108 - 3s - loss: 1.9789 - val_loss: 1.9981\n",
            "108/108 - 3s - loss: 1.9597 - val_loss: 2.0145\n",
            "108/108 - 3s - loss: 1.9350 - val_loss: 2.0640\n",
            "108/108 - 3s - loss: 1.9664 - val_loss: 2.0572\n",
            "108/108 - 3s - loss: 1.9464 - val_loss: 2.0374\n",
            "108/108 - 3s - loss: 1.9491 - val_loss: 2.0563\n",
            "108/108 - 3s - loss: 1.9233 - val_loss: 2.0689\n",
            "0.8\n",
            "108/108 - 8s - loss: 13.0516 - val_loss: 10.8669\n",
            "108/108 - 3s - loss: 10.6908 - val_loss: 8.9885\n",
            "108/108 - 3s - loss: 9.0089 - val_loss: 7.5090\n",
            "108/108 - 3s - loss: 7.6338 - val_loss: 6.3226\n",
            "108/108 - 3s - loss: 6.5128 - val_loss: 5.3634\n",
            "108/108 - 3s - loss: 5.6113 - val_loss: 4.6180\n",
            "108/108 - 3s - loss: 4.9522 - val_loss: 4.0576\n",
            "108/108 - 3s - loss: 4.4402 - val_loss: 3.6527\n",
            "108/108 - 4s - loss: 3.9965 - val_loss: 3.3590\n",
            "108/108 - 3s - loss: 3.7115 - val_loss: 3.1442\n",
            "108/108 - 4s - loss: 3.4901 - val_loss: 2.9897\n",
            "108/108 - 4s - loss: 3.3071 - val_loss: 2.8825\n",
            "108/108 - 3s - loss: 3.2055 - val_loss: 2.8077\n",
            "108/108 - 3s - loss: 3.0857 - val_loss: 2.7592\n",
            "108/108 - 3s - loss: 3.0520 - val_loss: 2.7262\n",
            "108/108 - 3s - loss: 3.0169 - val_loss: 2.7101\n",
            "108/108 - 4s - loss: 2.9865 - val_loss: 2.7044\n",
            "108/108 - 4s - loss: 2.9234 - val_loss: 2.7030\n",
            "108/108 - 4s - loss: 2.9072 - val_loss: 2.7046\n",
            "108/108 - 4s - loss: 2.9113 - val_loss: 2.7084\n",
            "108/108 - 4s - loss: 2.8762 - val_loss: 2.7112\n",
            "108/108 - 4s - loss: 2.9251 - val_loss: 2.7145\n",
            "108/108 - 4s - loss: 2.8632 - val_loss: 2.7168\n",
            "108/108 - 4s - loss: 2.8876 - val_loss: 2.7216\n",
            "108/108 - 4s - loss: 2.8578 - val_loss: 2.7230\n",
            "108/108 - 3s - loss: 2.9025 - val_loss: 2.7276\n",
            "108/108 - 3s - loss: 2.9091 - val_loss: 2.7287\n",
            "108/108 - 3s - loss: 2.8763 - val_loss: 2.7281\n",
            "108/108 - 3s - loss: 2.8959 - val_loss: 2.7266\n",
            "108/108 - 3s - loss: 2.8912 - val_loss: 2.7279\n",
            "108/108 - 3s - loss: 2.9066 - val_loss: 2.7267\n",
            "108/108 - 3s - loss: 2.9377 - val_loss: 2.7290\n",
            "108/108 - 3s - loss: 2.9222 - val_loss: 2.7311\n",
            "108/108 - 3s - loss: 2.9293 - val_loss: 2.7293\n",
            "108/108 - 3s - loss: 2.9064 - val_loss: 2.7303\n",
            "108/108 - 3s - loss: 2.9229 - val_loss: 2.7316\n",
            "108/108 - 3s - loss: 2.9364 - val_loss: 2.7297\n",
            "108/108 - 3s - loss: 2.8982 - val_loss: 2.7318\n",
            "108/108 - 3s - loss: 2.8690 - val_loss: 2.7336\n",
            "108/108 - 3s - loss: 2.9112 - val_loss: 2.7326\n",
            "108/108 - 4s - loss: 2.9274 - val_loss: 2.7338\n",
            "108/108 - 4s - loss: 2.8552 - val_loss: 2.7335\n",
            "108/108 - 4s - loss: 2.8974 - val_loss: 2.5034\n",
            "108/108 - 3s - loss: 2.5289 - val_loss: 2.2468\n",
            "108/108 - 3s - loss: 2.2793 - val_loss: 1.9589\n",
            "108/108 - 3s - loss: 2.1371 - val_loss: 1.8777\n",
            "108/108 - 3s - loss: 2.0770 - val_loss: 1.9228\n",
            "108/108 - 3s - loss: 2.0329 - val_loss: 1.8194\n",
            "108/108 - 3s - loss: 2.0217 - val_loss: 1.9138\n",
            "108/108 - 3s - loss: 1.9709 - val_loss: 1.8201\n",
            "108/108 - 3s - loss: 1.9486 - val_loss: 1.7216\n",
            "108/108 - 3s - loss: 1.9115 - val_loss: 1.6957\n",
            "108/108 - 4s - loss: 1.9079 - val_loss: 1.7246\n",
            "108/108 - 3s - loss: 1.8954 - val_loss: 1.7362\n",
            "108/108 - 3s - loss: 1.8854 - val_loss: 1.6523\n",
            "108/108 - 3s - loss: 1.8609 - val_loss: 1.6338\n",
            "108/108 - 3s - loss: 1.8455 - val_loss: 1.6175\n",
            "108/108 - 4s - loss: 1.8005 - val_loss: 1.5994\n",
            "108/108 - 4s - loss: 1.7855 - val_loss: 1.6059\n",
            "108/108 - 3s - loss: 1.8172 - val_loss: 1.5815\n",
            "108/108 - 3s - loss: 1.8086 - val_loss: 1.6413\n",
            "108/108 - 3s - loss: 1.7651 - val_loss: 1.5790\n",
            "108/108 - 3s - loss: 1.8137 - val_loss: 1.5802\n",
            "108/108 - 3s - loss: 1.7895 - val_loss: 1.5628\n",
            "108/108 - 3s - loss: 1.8023 - val_loss: 1.6054\n",
            "108/108 - 3s - loss: 1.7634 - val_loss: 1.5920\n",
            "108/108 - 3s - loss: 1.7742 - val_loss: 1.6076\n",
            "108/108 - 3s - loss: 1.7671 - val_loss: 1.5367\n",
            "108/108 - 4s - loss: 1.7557 - val_loss: 1.5419\n",
            "108/108 - 4s - loss: 1.7738 - val_loss: 1.5930\n",
            "108/108 - 4s - loss: 1.7663 - val_loss: 1.5713\n",
            "108/108 - 3s - loss: 1.7934 - val_loss: 1.5761\n",
            "108/108 - 3s - loss: 1.7598 - val_loss: 1.5439\n",
            "108/108 - 3s - loss: 1.7318 - val_loss: 1.6015\n",
            "108/108 - 3s - loss: 1.7709 - val_loss: 1.5471\n",
            "108/108 - 4s - loss: 1.7681 - val_loss: 1.5254\n",
            "108/108 - 3s - loss: 1.7440 - val_loss: 1.5080\n",
            "108/108 - 3s - loss: 1.7487 - val_loss: 1.5399\n",
            "108/108 - 3s - loss: 1.7459 - val_loss: 1.5469\n",
            "108/108 - 3s - loss: 1.7492 - val_loss: 1.5543\n",
            "108/108 - 3s - loss: 1.7634 - val_loss: 1.5333\n",
            "108/108 - 4s - loss: 1.7447 - val_loss: 1.4648\n",
            "108/108 - 4s - loss: 1.7294 - val_loss: 1.4937\n",
            "108/108 - 3s - loss: 1.7321 - val_loss: 1.5153\n",
            "108/108 - 3s - loss: 1.7259 - val_loss: 1.5046\n",
            "108/108 - 4s - loss: 1.7570 - val_loss: 1.5598\n",
            "108/108 - 3s - loss: 1.7080 - val_loss: 1.5079\n",
            "108/108 - 3s - loss: 1.7393 - val_loss: 1.5295\n",
            "108/108 - 3s - loss: 1.7125 - val_loss: 1.5573\n",
            "108/108 - 3s - loss: 1.7158 - val_loss: 1.5478\n",
            "108/108 - 3s - loss: 1.7333 - val_loss: 1.5042\n",
            "108/108 - 3s - loss: 1.6903 - val_loss: 1.5063\n",
            "108/108 - 3s - loss: 1.6946 - val_loss: 1.4809\n",
            "108/108 - 3s - loss: 1.6928 - val_loss: 1.5232\n",
            "108/108 - 3s - loss: 1.7293 - val_loss: 1.5240\n",
            "108/108 - 3s - loss: 1.6925 - val_loss: 1.5238\n",
            "108/108 - 3s - loss: 1.7190 - val_loss: 1.4979\n",
            "108/108 - 3s - loss: 1.7341 - val_loss: 1.4711\n",
            "108/108 - 3s - loss: 1.7056 - val_loss: 1.5766\n",
            "108/108 - 3s - loss: 1.7164 - val_loss: 1.4947\n",
            "108/108 - 3s - loss: 1.7071 - val_loss: 1.5471\n",
            "108/108 - 4s - loss: 1.6980 - val_loss: 1.5273\n",
            "108/108 - 4s - loss: 1.7457 - val_loss: 1.5056\n",
            "108/108 - 3s - loss: 1.7075 - val_loss: 1.5606\n",
            "108/108 - 3s - loss: 1.7137 - val_loss: 1.4852\n",
            "108/108 - 4s - loss: 1.6921 - val_loss: 1.4802\n",
            "108/108 - 4s - loss: 1.7348 - val_loss: 1.5443\n",
            "108/108 - 3s - loss: 1.6665 - val_loss: 1.5205\n",
            "108/108 - 3s - loss: 1.6837 - val_loss: 1.4740\n",
            "108/108 - 3s - loss: 1.7146 - val_loss: 1.4848\n",
            "108/108 - 3s - loss: 1.6936 - val_loss: 1.5593\n",
            "108/108 - 3s - loss: 1.6915 - val_loss: 1.5014\n",
            "108/108 - 3s - loss: 1.7067 - val_loss: 1.5118\n",
            "108/108 - 3s - loss: 1.7220 - val_loss: 1.5083\n",
            "108/108 - 3s - loss: 1.6893 - val_loss: 1.5330\n",
            "108/108 - 3s - loss: 1.6869 - val_loss: 1.5142\n",
            "108/108 - 3s - loss: 1.6778 - val_loss: 1.5010\n",
            "108/108 - 4s - loss: 1.6776 - val_loss: 1.4995\n",
            "108/108 - 3s - loss: 1.6972 - val_loss: 1.4978\n",
            "108/108 - 4s - loss: 1.7264 - val_loss: 1.4846\n",
            "108/108 - 3s - loss: 1.6973 - val_loss: 1.4909\n",
            "108/108 - 3s - loss: 1.6907 - val_loss: 1.5312\n",
            "108/108 - 3s - loss: 1.6824 - val_loss: 1.5113\n",
            "108/108 - 3s - loss: 1.7213 - val_loss: 1.4959\n",
            "108/108 - 3s - loss: 1.6877 - val_loss: 1.5708\n",
            "108/108 - 3s - loss: 1.6803 - val_loss: 1.5152\n",
            "108/108 - 3s - loss: 1.7061 - val_loss: 1.5302\n",
            "108/108 - 3s - loss: 1.6396 - val_loss: 1.5016\n",
            "108/108 - 3s - loss: 1.6735 - val_loss: 1.5155\n",
            "108/108 - 4s - loss: 1.7119 - val_loss: 1.4915\n",
            "108/108 - 4s - loss: 1.6863 - val_loss: 1.5391\n",
            "108/108 - 4s - loss: 1.6871 - val_loss: 1.4945\n",
            "108/108 - 3s - loss: 1.7106 - val_loss: 1.4852\n",
            "108/108 - 3s - loss: 1.6646 - val_loss: 1.4803\n",
            "108/108 - 3s - loss: 1.6846 - val_loss: 1.4891\n",
            "108/108 - 3s - loss: 1.6779 - val_loss: 1.5309\n",
            "108/108 - 3s - loss: 1.6962 - val_loss: 1.4794\n",
            "108/108 - 3s - loss: 1.6739 - val_loss: 1.5295\n",
            "108/108 - 3s - loss: 1.7003 - val_loss: 1.4916\n",
            "108/108 - 3s - loss: 1.6795 - val_loss: 1.5319\n",
            "108/108 - 3s - loss: 1.7067 - val_loss: 1.5266\n",
            "108/108 - 3s - loss: 1.6754 - val_loss: 1.5335\n",
            "108/108 - 3s - loss: 1.6791 - val_loss: 1.5369\n",
            "108/108 - 3s - loss: 1.6870 - val_loss: 1.6051\n",
            "108/108 - 3s - loss: 1.6591 - val_loss: 1.5023\n",
            "108/108 - 3s - loss: 1.6485 - val_loss: 1.4883\n",
            "108/108 - 3s - loss: 1.7053 - val_loss: 1.5048\n",
            "108/108 - 3s - loss: 1.6518 - val_loss: 1.5088\n",
            "108/108 - 3s - loss: 1.6655 - val_loss: 1.4855\n",
            "108/108 - 3s - loss: 1.6917 - val_loss: 1.4890\n",
            "108/108 - 3s - loss: 1.6637 - val_loss: 1.4956\n",
            "108/108 - 3s - loss: 1.6807 - val_loss: 1.5224\n",
            "108/108 - 3s - loss: 1.6806 - val_loss: 1.5097\n",
            "108/108 - 4s - loss: 1.6699 - val_loss: 1.4939\n",
            "108/108 - 4s - loss: 1.6578 - val_loss: 1.4787\n",
            "108/108 - 3s - loss: 1.6870 - val_loss: 1.5241\n",
            "108/108 - 3s - loss: 1.6847 - val_loss: 1.4905\n",
            "108/108 - 3s - loss: 1.6819 - val_loss: 1.4809\n",
            "108/108 - 3s - loss: 1.6730 - val_loss: 1.5285\n",
            "108/108 - 3s - loss: 1.6826 - val_loss: 1.4985\n",
            "108/108 - 3s - loss: 1.6442 - val_loss: 1.5058\n",
            "108/108 - 3s - loss: 1.6755 - val_loss: 1.5181\n",
            "108/108 - 3s - loss: 1.6719 - val_loss: 1.4644\n",
            "108/108 - 3s - loss: 1.6670 - val_loss: 1.5291\n",
            "108/108 - 3s - loss: 1.6879 - val_loss: 1.4703\n",
            "108/108 - 3s - loss: 1.6720 - val_loss: 1.4520\n",
            "108/108 - 3s - loss: 1.6660 - val_loss: 1.5411\n",
            "108/108 - 3s - loss: 1.6868 - val_loss: 1.5444\n",
            "108/108 - 3s - loss: 1.6900 - val_loss: 1.5160\n",
            "108/108 - 3s - loss: 1.6361 - val_loss: 1.5019\n",
            "108/108 - 3s - loss: 1.6459 - val_loss: 1.4762\n",
            "108/108 - 3s - loss: 1.6814 - val_loss: 1.5369\n",
            "108/108 - 3s - loss: 1.6542 - val_loss: 1.4783\n",
            "108/108 - 3s - loss: 1.6545 - val_loss: 1.4990\n",
            "108/108 - 3s - loss: 1.6860 - val_loss: 1.4603\n",
            "108/108 - 3s - loss: 1.6415 - val_loss: 1.4553\n",
            "108/108 - 3s - loss: 1.6255 - val_loss: 1.4807\n",
            "108/108 - 3s - loss: 1.6691 - val_loss: 1.4615\n",
            "108/108 - 3s - loss: 1.6431 - val_loss: 1.5584\n",
            "108/108 - 3s - loss: 1.6317 - val_loss: 1.5014\n",
            "108/108 - 3s - loss: 1.6424 - val_loss: 1.4962\n",
            "108/108 - 3s - loss: 1.6682 - val_loss: 1.4956\n",
            "108/108 - 3s - loss: 1.6756 - val_loss: 1.4715\n",
            "108/108 - 4s - loss: 1.6738 - val_loss: 1.5053\n",
            "108/108 - 3s - loss: 1.6484 - val_loss: 1.5107\n",
            "108/108 - 3s - loss: 1.6587 - val_loss: 1.4910\n",
            "108/108 - 3s - loss: 1.6503 - val_loss: 1.5115\n",
            "108/108 - 3s - loss: 1.6804 - val_loss: 1.5277\n",
            "108/108 - 3s - loss: 1.6483 - val_loss: 1.5025\n",
            "108/108 - 3s - loss: 1.6273 - val_loss: 1.5108\n",
            "108/108 - 3s - loss: 1.6192 - val_loss: 1.5008\n",
            "108/108 - 3s - loss: 1.6323 - val_loss: 1.4902\n",
            "108/108 - 3s - loss: 1.6793 - val_loss: 1.5768\n",
            "108/108 - 3s - loss: 1.6727 - val_loss: 1.4990\n",
            "108/108 - 3s - loss: 1.6368 - val_loss: 1.4827\n",
            "108/108 - 3s - loss: 1.6759 - val_loss: 1.4974\n",
            "108/108 - 3s - loss: 1.6261 - val_loss: 1.4646\n",
            "108/108 - 3s - loss: 1.6357 - val_loss: 1.5037\n",
            "108/108 - 3s - loss: 1.6625 - val_loss: 1.5025\n",
            "108/108 - 3s - loss: 1.6500 - val_loss: 1.5321\n",
            "108/108 - 3s - loss: 1.6496 - val_loss: 1.5446\n",
            "108/108 - 3s - loss: 1.6499 - val_loss: 1.5233\n",
            "108/108 - 3s - loss: 1.6282 - val_loss: 1.4808\n",
            "108/108 - 3s - loss: 1.6505 - val_loss: 1.4972\n",
            "108/108 - 3s - loss: 1.6355 - val_loss: 1.5109\n",
            "108/108 - 3s - loss: 1.6461 - val_loss: 1.5710\n",
            "108/108 - 3s - loss: 1.6280 - val_loss: 1.5100\n",
            "108/108 - 3s - loss: 1.6341 - val_loss: 1.4942\n",
            "108/108 - 3s - loss: 1.6122 - val_loss: 1.4866\n",
            "108/108 - 3s - loss: 1.6556 - val_loss: 1.5624\n",
            "108/108 - 3s - loss: 1.6020 - val_loss: 1.5253\n",
            "108/108 - 3s - loss: 1.6555 - val_loss: 1.4907\n",
            "108/108 - 3s - loss: 1.6430 - val_loss: 1.5300\n",
            "108/108 - 3s - loss: 1.6117 - val_loss: 1.5232\n",
            "108/108 - 3s - loss: 1.6170 - val_loss: 1.5296\n",
            "108/108 - 3s - loss: 1.6446 - val_loss: 1.5264\n",
            "108/108 - 3s - loss: 1.6505 - val_loss: 1.5441\n",
            "108/108 - 3s - loss: 1.6219 - val_loss: 1.5217\n",
            "108/108 - 3s - loss: 1.6416 - val_loss: 1.5630\n",
            "108/108 - 4s - loss: 1.6378 - val_loss: 1.5117\n",
            "108/108 - 4s - loss: 1.6246 - val_loss: 1.5117\n",
            "108/108 - 4s - loss: 1.6326 - val_loss: 1.5120\n",
            "108/108 - 3s - loss: 1.6200 - val_loss: 1.5184\n",
            "108/108 - 3s - loss: 1.6423 - val_loss: 1.5168\n",
            "108/108 - 3s - loss: 1.6267 - val_loss: 1.5409\n",
            "108/108 - 4s - loss: 1.6136 - val_loss: 1.5111\n",
            "108/108 - 3s - loss: 1.6462 - val_loss: 1.5446\n",
            "108/108 - 3s - loss: 1.6373 - val_loss: 1.5383\n",
            "108/108 - 3s - loss: 1.6321 - val_loss: 1.5159\n",
            "108/108 - 3s - loss: 1.6441 - val_loss: 1.5127\n",
            "108/108 - 3s - loss: 1.6495 - val_loss: 1.5229\n",
            "108/108 - 3s - loss: 1.6250 - val_loss: 1.5095\n",
            "108/108 - 3s - loss: 1.6472 - val_loss: 1.5403\n",
            "108/108 - 3s - loss: 1.6343 - val_loss: 1.5123\n",
            "108/108 - 3s - loss: 1.6202 - val_loss: 1.5338\n",
            "108/108 - 3s - loss: 1.6387 - val_loss: 1.5644\n",
            "108/108 - 3s - loss: 1.6459 - val_loss: 1.5237\n",
            "108/108 - 3s - loss: 1.6109 - val_loss: 1.5210\n",
            "108/108 - 3s - loss: 1.6425 - val_loss: 1.5782\n",
            "108/108 - 3s - loss: 1.6344 - val_loss: 1.5075\n",
            "108/108 - 4s - loss: 1.6387 - val_loss: 1.5253\n",
            "108/108 - 3s - loss: 1.6417 - val_loss: 1.5621\n",
            "108/108 - 3s - loss: 1.6181 - val_loss: 1.5269\n",
            "108/108 - 3s - loss: 1.6424 - val_loss: 1.4895\n",
            "108/108 - 3s - loss: 1.5998 - val_loss: 1.5330\n",
            "108/108 - 3s - loss: 1.6193 - val_loss: 1.5433\n",
            "108/108 - 3s - loss: 1.6416 - val_loss: 1.5534\n",
            "108/108 - 3s - loss: 1.6326 - val_loss: 1.5190\n",
            "108/108 - 3s - loss: 1.5845 - val_loss: 1.4997\n",
            "108/108 - 3s - loss: 1.6183 - val_loss: 1.5148\n",
            "108/108 - 3s - loss: 1.6104 - val_loss: 1.5144\n",
            "108/108 - 3s - loss: 1.6103 - val_loss: 1.5619\n",
            "108/108 - 3s - loss: 1.5813 - val_loss: 1.5090\n",
            "108/108 - 3s - loss: 1.6289 - val_loss: 1.5383\n",
            "108/108 - 3s - loss: 1.6232 - val_loss: 1.4975\n",
            "108/108 - 3s - loss: 1.5989 - val_loss: 1.5187\n",
            "108/108 - 3s - loss: 1.5934 - val_loss: 1.5078\n",
            "108/108 - 3s - loss: 1.5845 - val_loss: 1.4952\n",
            "108/108 - 3s - loss: 1.5741 - val_loss: 1.5144\n",
            "108/108 - 3s - loss: 1.5991 - val_loss: 1.5551\n",
            "108/108 - 3s - loss: 1.6243 - val_loss: 1.5179\n",
            "108/108 - 3s - loss: 1.6116 - val_loss: 1.5449\n",
            "108/108 - 3s - loss: 1.6015 - val_loss: 1.5127\n",
            "108/108 - 3s - loss: 1.5725 - val_loss: 1.5242\n",
            "108/108 - 3s - loss: 1.5921 - val_loss: 1.5575\n",
            "108/108 - 3s - loss: 1.6071 - val_loss: 1.5190\n",
            "108/108 - 3s - loss: 1.6310 - val_loss: 1.5290\n",
            "108/108 - 3s - loss: 1.6318 - val_loss: 1.4909\n",
            "108/108 - 3s - loss: 1.6358 - val_loss: 1.4979\n",
            "108/108 - 3s - loss: 1.6102 - val_loss: 1.5000\n",
            "108/108 - 3s - loss: 1.6010 - val_loss: 1.4984\n",
            "108/108 - 3s - loss: 1.6034 - val_loss: 1.5478\n",
            "108/108 - 3s - loss: 1.6158 - val_loss: 1.5511\n",
            "108/108 - 3s - loss: 1.5970 - val_loss: 1.5102\n",
            "108/108 - 3s - loss: 1.5930 - val_loss: 1.5242\n",
            "108/108 - 3s - loss: 1.6092 - val_loss: 1.5004\n",
            "108/108 - 4s - loss: 1.5937 - val_loss: 1.5233\n",
            "108/108 - 3s - loss: 1.6108 - val_loss: 1.5071\n",
            "108/108 - 3s - loss: 1.6090 - val_loss: 1.5667\n",
            "108/108 - 3s - loss: 1.6019 - val_loss: 1.5300\n",
            "108/108 - 3s - loss: 1.6236 - val_loss: 1.5488\n",
            "108/108 - 3s - loss: 1.5864 - val_loss: 1.5206\n",
            "108/108 - 3s - loss: 1.5925 - val_loss: 1.5025\n",
            "108/108 - 3s - loss: 1.6101 - val_loss: 1.5208\n",
            "108/108 - 3s - loss: 1.6389 - val_loss: 1.4679\n",
            "108/108 - 3s - loss: 1.6396 - val_loss: 1.4946\n",
            "108/108 - 3s - loss: 1.6132 - val_loss: 1.5294\n",
            "108/108 - 3s - loss: 1.5872 - val_loss: 1.5279\n",
            "108/108 - 3s - loss: 1.6072 - val_loss: 1.4997\n",
            "108/108 - 3s - loss: 1.6148 - val_loss: 1.5366\n",
            "108/108 - 3s - loss: 1.5876 - val_loss: 1.5120\n",
            "108/108 - 3s - loss: 1.5989 - val_loss: 1.5095\n",
            "108/108 - 3s - loss: 1.5866 - val_loss: 1.5152\n",
            "108/108 - 3s - loss: 1.6038 - val_loss: 1.5171\n",
            "108/108 - 3s - loss: 1.5915 - val_loss: 1.5517\n",
            "108/108 - 3s - loss: 1.5809 - val_loss: 1.5238\n",
            "108/108 - 3s - loss: 1.6142 - val_loss: 1.5318\n",
            "108/108 - 3s - loss: 1.5955 - val_loss: 1.5358\n",
            "108/108 - 3s - loss: 1.5889 - val_loss: 1.5546\n",
            "108/108 - 3s - loss: 1.6137 - val_loss: 1.5092\n",
            "108/108 - 3s - loss: 1.6222 - val_loss: 1.5281\n",
            "108/108 - 3s - loss: 1.6236 - val_loss: 1.5417\n",
            "108/108 - 3s - loss: 1.6056 - val_loss: 1.5143\n",
            "108/108 - 3s - loss: 1.5897 - val_loss: 1.5439\n",
            "108/108 - 3s - loss: 1.5848 - val_loss: 1.5143\n",
            "108/108 - 3s - loss: 1.6119 - val_loss: 1.5455\n",
            "108/108 - 3s - loss: 1.6043 - val_loss: 1.5468\n",
            "108/108 - 3s - loss: 1.5672 - val_loss: 1.5292\n",
            "108/108 - 3s - loss: 1.5760 - val_loss: 1.5177\n",
            "108/108 - 3s - loss: 1.6070 - val_loss: 1.4837\n",
            "108/108 - 3s - loss: 1.6286 - val_loss: 1.4971\n",
            "108/108 - 4s - loss: 1.5894 - val_loss: 1.5064\n",
            "108/108 - 4s - loss: 1.6019 - val_loss: 1.4714\n",
            "108/108 - 3s - loss: 1.5741 - val_loss: 1.4908\n",
            "108/108 - 3s - loss: 1.6018 - val_loss: 1.5333\n",
            "108/108 - 3s - loss: 1.5574 - val_loss: 1.5609\n",
            "108/108 - 3s - loss: 1.5785 - val_loss: 1.5300\n",
            "108/108 - 3s - loss: 1.5956 - val_loss: 1.4771\n",
            "108/108 - 3s - loss: 1.5839 - val_loss: 1.5413\n",
            "108/108 - 3s - loss: 1.5924 - val_loss: 1.4974\n",
            "108/108 - 3s - loss: 1.5663 - val_loss: 1.4791\n",
            "108/108 - 3s - loss: 1.5762 - val_loss: 1.5299\n",
            "108/108 - 3s - loss: 1.5698 - val_loss: 1.5016\n",
            "108/108 - 3s - loss: 1.5659 - val_loss: 1.5452\n",
            "108/108 - 3s - loss: 1.5790 - val_loss: 1.4973\n",
            "108/108 - 3s - loss: 1.6006 - val_loss: 1.5009\n",
            "108/108 - 3s - loss: 1.5813 - val_loss: 1.5179\n",
            "108/108 - 3s - loss: 1.5696 - val_loss: 1.5363\n",
            "108/108 - 3s - loss: 1.5852 - val_loss: 1.5197\n",
            "108/108 - 3s - loss: 1.5812 - val_loss: 1.5341\n",
            "108/108 - 3s - loss: 1.6094 - val_loss: 1.5344\n",
            "108/108 - 3s - loss: 1.5698 - val_loss: 1.5510\n",
            "108/108 - 3s - loss: 1.5868 - val_loss: 1.5418\n",
            "108/108 - 4s - loss: 1.5617 - val_loss: 1.5137\n",
            "108/108 - 4s - loss: 1.5797 - val_loss: 1.5271\n",
            "108/108 - 3s - loss: 1.6001 - val_loss: 1.5308\n",
            "108/108 - 3s - loss: 1.5873 - val_loss: 1.5051\n",
            "108/108 - 3s - loss: 1.5893 - val_loss: 1.5050\n",
            "108/108 - 3s - loss: 1.5748 - val_loss: 1.5022\n",
            "108/108 - 4s - loss: 1.5944 - val_loss: 1.5099\n",
            "108/108 - 3s - loss: 1.5641 - val_loss: 1.4998\n",
            "108/108 - 4s - loss: 1.5814 - val_loss: 1.5752\n",
            "108/108 - 4s - loss: 1.6036 - val_loss: 1.5406\n",
            "108/108 - 4s - loss: 1.5672 - val_loss: 1.5417\n",
            "108/108 - 3s - loss: 1.6185 - val_loss: 1.5208\n",
            "108/108 - 3s - loss: 1.5638 - val_loss: 1.5316\n",
            "108/108 - 3s - loss: 1.6024 - val_loss: 1.5304\n",
            "108/108 - 3s - loss: 1.6085 - val_loss: 1.5250\n",
            "108/108 - 3s - loss: 1.5954 - val_loss: 1.5216\n",
            "108/108 - 4s - loss: 1.5630 - val_loss: 1.5176\n",
            "108/108 - 3s - loss: 1.5588 - val_loss: 1.5398\n",
            "108/108 - 3s - loss: 1.5930 - val_loss: 1.4953\n",
            "108/108 - 3s - loss: 1.5424 - val_loss: 1.5062\n",
            "108/108 - 3s - loss: 1.5532 - val_loss: 1.5287\n",
            "108/108 - 3s - loss: 1.5861 - val_loss: 1.5260\n",
            "108/108 - 3s - loss: 1.5777 - val_loss: 1.5066\n",
            "108/108 - 3s - loss: 1.5512 - val_loss: 1.5195\n",
            "108/108 - 3s - loss: 1.5669 - val_loss: 1.5076\n",
            "108/108 - 3s - loss: 1.5629 - val_loss: 1.4957\n",
            "108/108 - 4s - loss: 1.5630 - val_loss: 1.5382\n",
            "108/108 - 3s - loss: 1.5367 - val_loss: 1.5518\n",
            "108/108 - 3s - loss: 1.5783 - val_loss: 1.5036\n",
            "108/108 - 3s - loss: 1.5567 - val_loss: 1.4933\n",
            "108/108 - 3s - loss: 1.5949 - val_loss: 1.5038\n",
            "108/108 - 3s - loss: 1.5745 - val_loss: 1.5451\n",
            "108/108 - 3s - loss: 1.5662 - val_loss: 1.5287\n",
            "108/108 - 3s - loss: 1.5354 - val_loss: 1.5181\n",
            "108/108 - 3s - loss: 1.5810 - val_loss: 1.5608\n",
            "108/108 - 3s - loss: 1.5599 - val_loss: 1.5355\n",
            "108/108 - 3s - loss: 1.5543 - val_loss: 1.4925\n",
            "108/108 - 3s - loss: 1.5776 - val_loss: 1.5445\n",
            "108/108 - 3s - loss: 1.5696 - val_loss: 1.4770\n",
            "108/108 - 3s - loss: 1.5649 - val_loss: 1.5206\n",
            "108/108 - 3s - loss: 1.5912 - val_loss: 1.5643\n",
            "108/108 - 3s - loss: 1.5833 - val_loss: 1.5362\n",
            "108/108 - 3s - loss: 1.5596 - val_loss: 1.5549\n",
            "108/108 - 3s - loss: 1.5599 - val_loss: 1.5209\n",
            "108/108 - 3s - loss: 1.6093 - val_loss: 1.5450\n",
            "108/108 - 3s - loss: 1.5537 - val_loss: 1.5215\n",
            "108/108 - 4s - loss: 1.5638 - val_loss: 1.5244\n",
            "108/108 - 3s - loss: 1.5477 - val_loss: 1.5564\n",
            "108/108 - 3s - loss: 1.5748 - val_loss: 1.5333\n",
            "108/108 - 3s - loss: 1.5689 - val_loss: 1.4831\n",
            "108/108 - 3s - loss: 1.5785 - val_loss: 1.5121\n",
            "108/108 - 3s - loss: 1.5758 - val_loss: 1.4598\n",
            "108/108 - 3s - loss: 1.5704 - val_loss: 1.4831\n",
            "108/108 - 3s - loss: 1.5574 - val_loss: 1.4698\n",
            "108/108 - 3s - loss: 1.5622 - val_loss: 1.5255\n",
            "108/108 - 3s - loss: 1.5339 - val_loss: 1.4526\n",
            "108/108 - 3s - loss: 1.5552 - val_loss: 1.5048\n",
            "108/108 - 3s - loss: 1.5410 - val_loss: 1.4901\n",
            "108/108 - 3s - loss: 1.5727 - val_loss: 1.5394\n",
            "108/108 - 3s - loss: 1.5654 - val_loss: 1.4892\n",
            "108/108 - 3s - loss: 1.5504 - val_loss: 1.5550\n",
            "108/108 - 3s - loss: 1.5868 - val_loss: 1.5299\n",
            "108/108 - 3s - loss: 1.5962 - val_loss: 1.5220\n",
            "108/108 - 4s - loss: 1.6019 - val_loss: 1.4840\n",
            "108/108 - 3s - loss: 1.5591 - val_loss: 1.5135\n",
            "108/108 - 4s - loss: 1.5524 - val_loss: 1.5417\n",
            "108/108 - 4s - loss: 1.5856 - val_loss: 1.5780\n",
            "108/108 - 4s - loss: 1.5770 - val_loss: 1.5400\n",
            "108/108 - 3s - loss: 1.5472 - val_loss: 1.5791\n",
            "108/108 - 3s - loss: 1.5563 - val_loss: 1.5339\n",
            "108/108 - 3s - loss: 1.5559 - val_loss: 1.5503\n",
            "108/108 - 3s - loss: 1.5490 - val_loss: 1.5345\n",
            "108/108 - 3s - loss: 1.5396 - val_loss: 1.4971\n",
            "108/108 - 3s - loss: 1.5672 - val_loss: 1.5307\n",
            "108/108 - 3s - loss: 1.5512 - val_loss: 1.5400\n",
            "108/108 - 3s - loss: 1.5272 - val_loss: 1.5257\n",
            "108/108 - 3s - loss: 1.5303 - val_loss: 1.5057\n",
            "108/108 - 3s - loss: 1.5370 - val_loss: 1.4814\n",
            "108/108 - 3s - loss: 1.5522 - val_loss: 1.5203\n",
            "108/108 - 3s - loss: 1.5523 - val_loss: 1.5413\n",
            "108/108 - 3s - loss: 1.5591 - val_loss: 1.4959\n",
            "108/108 - 4s - loss: 1.5432 - val_loss: 1.4842\n",
            "108/108 - 3s - loss: 1.5418 - val_loss: 1.5170\n",
            "108/108 - 3s - loss: 1.5491 - val_loss: 1.5309\n",
            "108/108 - 3s - loss: 1.5392 - val_loss: 1.4905\n",
            "108/108 - 3s - loss: 1.5237 - val_loss: 1.5517\n",
            "108/108 - 3s - loss: 1.5327 - val_loss: 1.5363\n",
            "108/108 - 3s - loss: 1.5470 - val_loss: 1.5325\n",
            "108/108 - 3s - loss: 1.5380 - val_loss: 1.5380\n",
            "108/108 - 3s - loss: 1.5483 - val_loss: 1.5320\n",
            "108/108 - 3s - loss: 1.5458 - val_loss: 1.5306\n",
            "108/108 - 3s - loss: 1.5336 - val_loss: 1.5237\n",
            "108/108 - 4s - loss: 1.5372 - val_loss: 1.5327\n",
            "108/108 - 3s - loss: 1.5241 - val_loss: 1.5247\n",
            "108/108 - 3s - loss: 1.5450 - val_loss: 1.5268\n",
            "108/108 - 3s - loss: 1.5142 - val_loss: 1.4998\n",
            "108/108 - 3s - loss: 1.5185 - val_loss: 1.5173\n",
            "108/108 - 3s - loss: 1.5403 - val_loss: 1.4875\n",
            "108/108 - 3s - loss: 1.5474 - val_loss: 1.5201\n",
            "108/108 - 3s - loss: 1.5546 - val_loss: 1.5219\n",
            "108/108 - 3s - loss: 1.5480 - val_loss: 1.5079\n",
            "108/108 - 4s - loss: 1.5036 - val_loss: 1.5163\n",
            "108/108 - 3s - loss: 1.5347 - val_loss: 1.5156\n",
            "108/108 - 3s - loss: 1.5314 - val_loss: 1.5384\n",
            "108/108 - 3s - loss: 1.5375 - val_loss: 1.4920\n",
            "108/108 - 3s - loss: 1.5572 - val_loss: 1.5140\n",
            "108/108 - 3s - loss: 1.5417 - val_loss: 1.4966\n",
            "108/108 - 3s - loss: 1.5511 - val_loss: 1.4935\n",
            "108/108 - 3s - loss: 1.5203 - val_loss: 1.5139\n",
            "108/108 - 3s - loss: 1.5295 - val_loss: 1.4571\n",
            "108/108 - 3s - loss: 1.5412 - val_loss: 1.4727\n",
            "108/108 - 3s - loss: 1.5205 - val_loss: 1.5114\n",
            "108/108 - 3s - loss: 1.5380 - val_loss: 1.4790\n",
            "108/108 - 3s - loss: 1.5773 - val_loss: 1.5029\n",
            "108/108 - 3s - loss: 1.5570 - val_loss: 1.5375\n",
            "108/108 - 3s - loss: 1.5503 - val_loss: 1.5144\n",
            "108/108 - 3s - loss: 1.5653 - val_loss: 1.5509\n",
            "108/108 - 3s - loss: 1.5551 - val_loss: 1.5136\n",
            "108/108 - 3s - loss: 1.5534 - val_loss: 1.5383\n",
            "108/108 - 3s - loss: 1.5405 - val_loss: 1.5183\n",
            "108/108 - 3s - loss: 1.5489 - val_loss: 1.4996\n",
            "108/108 - 3s - loss: 1.5344 - val_loss: 1.5298\n",
            "108/108 - 3s - loss: 1.5153 - val_loss: 1.4825\n",
            "108/108 - 3s - loss: 1.5394 - val_loss: 1.4918\n",
            "108/108 - 3s - loss: 1.5289 - val_loss: 1.5101\n",
            "108/108 - 4s - loss: 1.5159 - val_loss: 1.5140\n",
            "108/108 - 3s - loss: 1.5392 - val_loss: 1.5029\n",
            "108/108 - 3s - loss: 1.5254 - val_loss: 1.5638\n",
            "108/108 - 3s - loss: 1.5286 - val_loss: 1.5351\n",
            "108/108 - 3s - loss: 1.5320 - val_loss: 1.5317\n",
            "108/108 - 3s - loss: 1.5085 - val_loss: 1.5507\n",
            "108/108 - 3s - loss: 1.5330 - val_loss: 1.5146\n",
            "108/108 - 3s - loss: 1.5396 - val_loss: 1.4696\n",
            "108/108 - 3s - loss: 1.5192 - val_loss: 1.5378\n",
            "108/108 - 3s - loss: 1.5130 - val_loss: 1.5321\n",
            "108/108 - 3s - loss: 1.5024 - val_loss: 1.4926\n",
            "108/108 - 3s - loss: 1.5361 - val_loss: 1.5227\n",
            "108/108 - 4s - loss: 1.4944 - val_loss: 1.4827\n",
            "108/108 - 3s - loss: 1.5366 - val_loss: 1.5461\n",
            "108/108 - 3s - loss: 1.5358 - val_loss: 1.5300\n",
            "108/108 - 3s - loss: 1.5314 - val_loss: 1.5548\n",
            "108/108 - 3s - loss: 1.5257 - val_loss: 1.5552\n",
            "108/108 - 3s - loss: 1.5265 - val_loss: 1.5156\n",
            "108/108 - 3s - loss: 1.4950 - val_loss: 1.5138\n",
            "108/108 - 3s - loss: 1.5027 - val_loss: 1.5382\n",
            "108/108 - 3s - loss: 1.5385 - val_loss: 1.4909\n",
            "108/108 - 3s - loss: 1.5480 - val_loss: 1.5193\n",
            "108/108 - 3s - loss: 1.5004 - val_loss: 1.5089\n",
            "108/108 - 3s - loss: 1.5318 - val_loss: 1.5156\n",
            "108/108 - 4s - loss: 1.5094 - val_loss: 1.5609\n",
            "108/108 - 4s - loss: 1.5484 - val_loss: 1.5514\n",
            "108/108 - 4s - loss: 1.5348 - val_loss: 1.5252\n",
            "108/108 - 3s - loss: 1.5536 - val_loss: 1.4899\n",
            "108/108 - 3s - loss: 1.5274 - val_loss: 1.4795\n",
            "108/108 - 4s - loss: 1.5070 - val_loss: 1.5219\n",
            "108/108 - 4s - loss: 1.4878 - val_loss: 1.5108\n",
            "108/108 - 4s - loss: 1.5289 - val_loss: 1.5055\n",
            "108/108 - 3s - loss: 1.4962 - val_loss: 1.4995\n",
            "108/108 - 3s - loss: 1.5206 - val_loss: 1.4674\n",
            "108/108 - 3s - loss: 1.5173 - val_loss: 1.4799\n",
            "108/108 - 3s - loss: 1.5211 - val_loss: 1.4945\n",
            "108/108 - 3s - loss: 1.5003 - val_loss: 1.5171\n",
            "108/108 - 3s - loss: 1.5149 - val_loss: 1.4867\n",
            "108/108 - 3s - loss: 1.5610 - val_loss: 1.4454\n",
            "108/108 - 3s - loss: 1.5151 - val_loss: 1.4383\n",
            "108/108 - 3s - loss: 1.4913 - val_loss: 1.4722\n",
            "108/108 - 3s - loss: 1.4972 - val_loss: 1.4713\n",
            "0.9\n",
            "108/108 - 7s - loss: 14.5954 - val_loss: 12.1169\n",
            "108/108 - 3s - loss: 11.8533 - val_loss: 9.9075\n",
            "108/108 - 3s - loss: 9.8293 - val_loss: 8.1298\n",
            "108/108 - 3s - loss: 8.1476 - val_loss: 6.6699\n",
            "108/108 - 3s - loss: 6.8081 - val_loss: 5.4698\n",
            "108/108 - 3s - loss: 5.6125 - val_loss: 4.5097\n",
            "108/108 - 3s - loss: 4.7249 - val_loss: 3.7642\n",
            "108/108 - 3s - loss: 4.0337 - val_loss: 3.1942\n",
            "108/108 - 3s - loss: 3.5339 - val_loss: 2.7609\n",
            "108/108 - 3s - loss: 3.0743 - val_loss: 2.4335\n",
            "108/108 - 3s - loss: 2.7520 - val_loss: 2.1857\n",
            "108/108 - 4s - loss: 2.4583 - val_loss: 1.9986\n",
            "108/108 - 3s - loss: 2.2637 - val_loss: 1.8598\n",
            "108/108 - 3s - loss: 2.1197 - val_loss: 1.7564\n",
            "108/108 - 4s - loss: 1.9743 - val_loss: 1.6927\n",
            "108/108 - 3s - loss: 1.9047 - val_loss: 1.6515\n",
            "108/108 - 3s - loss: 1.8306 - val_loss: 1.6239\n",
            "108/108 - 3s - loss: 1.8351 - val_loss: 1.6092\n",
            "108/108 - 3s - loss: 1.7593 - val_loss: 1.6019\n",
            "108/108 - 3s - loss: 1.8111 - val_loss: 1.6002\n",
            "108/108 - 3s - loss: 1.7364 - val_loss: 1.5992\n",
            "108/108 - 3s - loss: 1.7421 - val_loss: 1.6034\n",
            "108/108 - 3s - loss: 1.7628 - val_loss: 1.6069\n",
            "108/108 - 3s - loss: 1.7218 - val_loss: 1.6102\n",
            "108/108 - 3s - loss: 1.7399 - val_loss: 1.6128\n",
            "108/108 - 3s - loss: 1.7319 - val_loss: 1.6180\n",
            "108/108 - 3s - loss: 1.6995 - val_loss: 1.6194\n",
            "108/108 - 3s - loss: 1.7099 - val_loss: 1.6202\n",
            "108/108 - 3s - loss: 1.6722 - val_loss: 1.6179\n",
            "108/108 - 3s - loss: 1.6599 - val_loss: 1.6165\n",
            "108/108 - 4s - loss: 1.7027 - val_loss: 1.6199\n",
            "108/108 - 4s - loss: 1.7055 - val_loss: 1.6247\n",
            "108/108 - 3s - loss: 1.7122 - val_loss: 1.6243\n",
            "108/108 - 3s - loss: 1.6997 - val_loss: 1.6241\n",
            "108/108 - 3s - loss: 1.6935 - val_loss: 1.6216\n",
            "108/108 - 3s - loss: 1.7344 - val_loss: 1.6201\n",
            "108/108 - 3s - loss: 1.6985 - val_loss: 1.6180\n",
            "108/108 - 3s - loss: 1.7167 - val_loss: 1.6175\n",
            "108/108 - 3s - loss: 1.7029 - val_loss: 1.6211\n",
            "108/108 - 3s - loss: 1.7159 - val_loss: 1.6217\n",
            "108/108 - 3s - loss: 1.7274 - val_loss: 1.6221\n",
            "108/108 - 3s - loss: 1.6874 - val_loss: 1.6167\n",
            "108/108 - 3s - loss: 1.7221 - val_loss: 1.6198\n",
            "108/108 - 3s - loss: 1.7307 - val_loss: 1.6234\n",
            "108/108 - 3s - loss: 1.7083 - val_loss: 1.6242\n",
            "108/108 - 3s - loss: 1.6942 - val_loss: 1.6191\n",
            "108/108 - 3s - loss: 1.6921 - val_loss: 1.6160\n",
            "108/108 - 3s - loss: 1.7076 - val_loss: 1.6238\n",
            "108/108 - 3s - loss: 1.7113 - val_loss: 1.6255\n",
            "108/108 - 3s - loss: 1.7198 - val_loss: 1.6269\n",
            "108/108 - 4s - loss: 1.7312 - val_loss: 1.6281\n",
            "108/108 - 3s - loss: 1.6952 - val_loss: 1.6227\n",
            "108/108 - 3s - loss: 1.7614 - val_loss: 1.4739\n",
            "108/108 - 3s - loss: 1.5521 - val_loss: 1.3922\n",
            "108/108 - 4s - loss: 1.3479 - val_loss: 1.2126\n",
            "108/108 - 3s - loss: 1.2762 - val_loss: 1.1257\n",
            "108/108 - 3s - loss: 1.2504 - val_loss: 1.1236\n",
            "108/108 - 3s - loss: 1.2493 - val_loss: 1.1516\n",
            "108/108 - 3s - loss: 1.2253 - val_loss: 1.0391\n",
            "108/108 - 3s - loss: 1.2059 - val_loss: 1.0498\n",
            "108/108 - 3s - loss: 1.1824 - val_loss: 1.0481\n",
            "108/108 - 3s - loss: 1.1433 - val_loss: 1.0671\n",
            "108/108 - 3s - loss: 1.1443 - val_loss: 1.0037\n",
            "108/108 - 3s - loss: 1.1334 - val_loss: 0.9807\n",
            "108/108 - 3s - loss: 1.1526 - val_loss: 0.9997\n",
            "108/108 - 3s - loss: 1.1266 - val_loss: 0.9255\n",
            "108/108 - 4s - loss: 1.0838 - val_loss: 0.9352\n",
            "108/108 - 4s - loss: 1.0967 - val_loss: 0.9346\n",
            "108/108 - 3s - loss: 1.0750 - val_loss: 0.9321\n",
            "108/108 - 3s - loss: 1.0997 - val_loss: 0.9943\n",
            "108/108 - 3s - loss: 1.0823 - val_loss: 0.9205\n",
            "108/108 - 4s - loss: 1.0859 - val_loss: 0.9092\n",
            "108/108 - 4s - loss: 1.0687 - val_loss: 0.9366\n",
            "108/108 - 4s - loss: 1.0615 - val_loss: 0.9031\n",
            "108/108 - 4s - loss: 1.0609 - val_loss: 0.9125\n",
            "108/108 - 4s - loss: 1.0573 - val_loss: 0.9920\n",
            "108/108 - 4s - loss: 1.0762 - val_loss: 0.9448\n",
            "108/108 - 3s - loss: 1.0620 - val_loss: 0.9341\n",
            "108/108 - 3s - loss: 1.0804 - val_loss: 0.9159\n",
            "108/108 - 3s - loss: 1.0452 - val_loss: 0.9211\n",
            "108/108 - 4s - loss: 1.0347 - val_loss: 0.8940\n",
            "108/108 - 3s - loss: 1.0355 - val_loss: 0.9198\n",
            "108/108 - 4s - loss: 1.0418 - val_loss: 0.8948\n",
            "108/108 - 4s - loss: 1.0478 - val_loss: 0.8669\n",
            "108/108 - 3s - loss: 1.0350 - val_loss: 0.9624\n",
            "108/108 - 3s - loss: 1.0298 - val_loss: 0.8392\n",
            "108/108 - 3s - loss: 1.0397 - val_loss: 0.8616\n",
            "108/108 - 3s - loss: 1.0461 - val_loss: 0.8736\n",
            "108/108 - 3s - loss: 1.0772 - val_loss: 0.9074\n",
            "108/108 - 3s - loss: 0.9981 - val_loss: 0.8689\n",
            "108/108 - 3s - loss: 1.0076 - val_loss: 0.8789\n",
            "108/108 - 3s - loss: 1.0086 - val_loss: 0.8630\n",
            "108/108 - 3s - loss: 1.0361 - val_loss: 0.8694\n",
            "108/108 - 3s - loss: 1.0220 - val_loss: 0.8807\n",
            "108/108 - 3s - loss: 1.0113 - val_loss: 0.8482\n",
            "108/108 - 3s - loss: 1.0172 - val_loss: 0.8479\n",
            "108/108 - 3s - loss: 1.0244 - val_loss: 0.8817\n",
            "108/108 - 3s - loss: 1.0178 - val_loss: 0.8816\n",
            "108/108 - 4s - loss: 1.0196 - val_loss: 0.8646\n",
            "108/108 - 4s - loss: 1.0196 - val_loss: 0.8897\n",
            "108/108 - 4s - loss: 1.0228 - val_loss: 0.8962\n",
            "108/108 - 3s - loss: 1.0264 - val_loss: 0.8628\n",
            "108/108 - 3s - loss: 1.0185 - val_loss: 0.8547\n",
            "108/108 - 3s - loss: 0.9968 - val_loss: 0.8380\n",
            "108/108 - 4s - loss: 1.0088 - val_loss: 0.8761\n",
            "108/108 - 3s - loss: 1.0065 - val_loss: 0.8446\n",
            "108/108 - 3s - loss: 1.0111 - val_loss: 0.9139\n",
            "108/108 - 3s - loss: 1.0323 - val_loss: 0.9224\n",
            "108/108 - 3s - loss: 1.0290 - val_loss: 0.8599\n",
            "108/108 - 3s - loss: 0.9946 - val_loss: 0.8533\n",
            "108/108 - 3s - loss: 1.0100 - val_loss: 0.8618\n",
            "108/108 - 3s - loss: 1.0047 - val_loss: 0.8630\n",
            "108/108 - 3s - loss: 1.0043 - val_loss: 0.8737\n",
            "108/108 - 3s - loss: 1.0094 - val_loss: 0.8774\n",
            "108/108 - 3s - loss: 0.9984 - val_loss: 0.8418\n",
            "108/108 - 3s - loss: 0.9972 - val_loss: 0.8694\n",
            "108/108 - 3s - loss: 1.0042 - val_loss: 0.8725\n",
            "108/108 - 4s - loss: 1.0276 - val_loss: 0.8862\n",
            "108/108 - 3s - loss: 0.9889 - val_loss: 0.8300\n",
            "108/108 - 3s - loss: 0.9812 - val_loss: 0.8528\n",
            "108/108 - 3s - loss: 1.0138 - val_loss: 0.8486\n",
            "108/108 - 3s - loss: 1.0321 - val_loss: 0.8467\n",
            "108/108 - 3s - loss: 1.0353 - val_loss: 0.8667\n",
            "108/108 - 3s - loss: 1.0078 - val_loss: 0.8546\n",
            "108/108 - 3s - loss: 1.0051 - val_loss: 0.8510\n",
            "108/108 - 3s - loss: 0.9995 - val_loss: 0.9208\n",
            "108/108 - 3s - loss: 1.0473 - val_loss: 0.9050\n",
            "108/108 - 3s - loss: 1.0150 - val_loss: 0.8748\n",
            "108/108 - 3s - loss: 1.0313 - val_loss: 0.9000\n",
            "108/108 - 3s - loss: 1.0103 - val_loss: 0.9115\n",
            "108/108 - 3s - loss: 1.0044 - val_loss: 0.8713\n",
            "108/108 - 3s - loss: 1.0175 - val_loss: 0.8775\n",
            "108/108 - 3s - loss: 1.0291 - val_loss: 0.8506\n",
            "108/108 - 3s - loss: 1.0297 - val_loss: 0.8890\n",
            "108/108 - 3s - loss: 1.0169 - val_loss: 0.8818\n",
            "108/108 - 3s - loss: 0.9911 - val_loss: 0.8525\n",
            "108/108 - 3s - loss: 1.0219 - val_loss: 0.8668\n",
            "108/108 - 4s - loss: 1.0070 - val_loss: 0.8415\n",
            "108/108 - 3s - loss: 0.9831 - val_loss: 0.8792\n",
            "108/108 - 3s - loss: 0.9725 - val_loss: 0.8402\n",
            "108/108 - 3s - loss: 0.9933 - val_loss: 0.8873\n",
            "108/108 - 4s - loss: 1.0040 - val_loss: 0.8516\n",
            "108/108 - 3s - loss: 0.9861 - val_loss: 0.8447\n",
            "108/108 - 3s - loss: 0.9963 - val_loss: 0.8363\n",
            "108/108 - 3s - loss: 0.9924 - val_loss: 0.8568\n",
            "108/108 - 3s - loss: 1.0134 - val_loss: 0.8814\n",
            "108/108 - 3s - loss: 0.9939 - val_loss: 0.8560\n",
            "108/108 - 3s - loss: 0.9974 - val_loss: 0.8691\n",
            "108/108 - 3s - loss: 0.9942 - val_loss: 0.8666\n",
            "108/108 - 3s - loss: 0.9762 - val_loss: 0.8698\n",
            "108/108 - 3s - loss: 0.9935 - val_loss: 0.8421\n",
            "108/108 - 3s - loss: 0.9926 - val_loss: 0.8784\n",
            "108/108 - 4s - loss: 1.0104 - val_loss: 0.8507\n",
            "108/108 - 4s - loss: 1.0022 - val_loss: 0.8640\n",
            "108/108 - 4s - loss: 0.9869 - val_loss: 0.8569\n",
            "108/108 - 3s - loss: 0.9838 - val_loss: 0.9042\n",
            "108/108 - 3s - loss: 1.0017 - val_loss: 0.8464\n",
            "108/108 - 3s - loss: 0.9918 - val_loss: 0.8648\n",
            "108/108 - 3s - loss: 0.9843 - val_loss: 0.8492\n",
            "108/108 - 4s - loss: 0.9821 - val_loss: 0.8492\n",
            "108/108 - 4s - loss: 0.9800 - val_loss: 0.8640\n",
            "108/108 - 4s - loss: 1.0184 - val_loss: 0.8343\n",
            "108/108 - 4s - loss: 0.9945 - val_loss: 0.9018\n",
            "108/108 - 4s - loss: 0.9878 - val_loss: 0.8599\n",
            "108/108 - 4s - loss: 1.0006 - val_loss: 0.8247\n",
            "108/108 - 4s - loss: 0.9661 - val_loss: 0.8736\n",
            "108/108 - 4s - loss: 0.9888 - val_loss: 0.8904\n",
            "108/108 - 4s - loss: 0.9851 - val_loss: 0.8421\n",
            "108/108 - 4s - loss: 1.0088 - val_loss: 0.8689\n",
            "108/108 - 4s - loss: 1.0206 - val_loss: 0.8511\n",
            "108/108 - 4s - loss: 0.9867 - val_loss: 0.8479\n",
            "108/108 - 4s - loss: 0.9877 - val_loss: 0.8396\n",
            "108/108 - 4s - loss: 0.9744 - val_loss: 0.8563\n",
            "108/108 - 4s - loss: 0.9901 - val_loss: 0.8987\n",
            "108/108 - 4s - loss: 1.0015 - val_loss: 0.9018\n",
            "108/108 - 4s - loss: 0.9802 - val_loss: 0.8600\n",
            "108/108 - 4s - loss: 0.9964 - val_loss: 0.8451\n",
            "108/108 - 4s - loss: 0.9899 - val_loss: 0.8555\n",
            "108/108 - 3s - loss: 0.9991 - val_loss: 0.8602\n",
            "108/108 - 3s - loss: 1.0031 - val_loss: 0.8614\n",
            "108/108 - 4s - loss: 0.9930 - val_loss: 0.8555\n",
            "108/108 - 4s - loss: 0.9943 - val_loss: 0.8812\n",
            "108/108 - 4s - loss: 0.9736 - val_loss: 0.8728\n",
            "108/108 - 3s - loss: 0.9984 - val_loss: 0.8251\n",
            "108/108 - 4s - loss: 0.9882 - val_loss: 0.8427\n",
            "108/108 - 4s - loss: 0.9737 - val_loss: 0.8601\n",
            "108/108 - 4s - loss: 0.9704 - val_loss: 0.8433\n",
            "108/108 - 4s - loss: 1.0339 - val_loss: 0.8491\n",
            "108/108 - 4s - loss: 0.9997 - val_loss: 0.8782\n",
            "108/108 - 4s - loss: 1.0092 - val_loss: 0.8749\n",
            "108/108 - 4s - loss: 1.0003 - val_loss: 0.9152\n",
            "108/108 - 4s - loss: 0.9978 - val_loss: 0.8552\n",
            "108/108 - 3s - loss: 0.9754 - val_loss: 0.8500\n",
            "108/108 - 3s - loss: 0.9897 - val_loss: 0.8769\n",
            "108/108 - 3s - loss: 0.9982 - val_loss: 0.8719\n",
            "108/108 - 3s - loss: 0.9886 - val_loss: 0.8728\n",
            "108/108 - 3s - loss: 1.0027 - val_loss: 0.8615\n",
            "108/108 - 4s - loss: 0.9845 - val_loss: 0.8431\n",
            "108/108 - 4s - loss: 0.9846 - val_loss: 0.9017\n",
            "108/108 - 4s - loss: 0.9869 - val_loss: 0.8342\n",
            "108/108 - 3s - loss: 0.9727 - val_loss: 0.8508\n",
            "108/108 - 3s - loss: 1.0047 - val_loss: 0.8544\n",
            "108/108 - 4s - loss: 1.0055 - val_loss: 0.8362\n",
            "108/108 - 4s - loss: 0.9861 - val_loss: 0.8795\n",
            "108/108 - 4s - loss: 0.9800 - val_loss: 0.8728\n",
            "108/108 - 4s - loss: 0.9926 - val_loss: 0.8801\n",
            "108/108 - 4s - loss: 0.9857 - val_loss: 0.8730\n",
            "108/108 - 4s - loss: 1.0170 - val_loss: 0.8668\n",
            "108/108 - 4s - loss: 0.9720 - val_loss: 0.8547\n",
            "108/108 - 4s - loss: 0.9776 - val_loss: 0.8716\n",
            "108/108 - 3s - loss: 1.0013 - val_loss: 0.8423\n",
            "108/108 - 4s - loss: 0.9814 - val_loss: 0.8471\n",
            "108/108 - 4s - loss: 0.9747 - val_loss: 0.8700\n",
            "108/108 - 3s - loss: 0.9794 - val_loss: 0.8912\n",
            "108/108 - 4s - loss: 1.0072 - val_loss: 0.8410\n",
            "108/108 - 4s - loss: 0.9698 - val_loss: 0.8471\n",
            "108/108 - 4s - loss: 0.9767 - val_loss: 0.8501\n",
            "108/108 - 4s - loss: 0.9665 - val_loss: 0.8890\n",
            "108/108 - 4s - loss: 0.9919 - val_loss: 0.8440\n",
            "108/108 - 4s - loss: 1.0032 - val_loss: 0.8389\n",
            "108/108 - 3s - loss: 1.0013 - val_loss: 0.8973\n",
            "108/108 - 3s - loss: 0.9986 - val_loss: 0.8473\n",
            "108/108 - 3s - loss: 0.9937 - val_loss: 0.8826\n",
            "108/108 - 3s - loss: 0.9818 - val_loss: 0.8675\n",
            "108/108 - 3s - loss: 0.9820 - val_loss: 0.8676\n",
            "108/108 - 4s - loss: 1.0136 - val_loss: 0.8638\n",
            "108/108 - 4s - loss: 0.9847 - val_loss: 0.8471\n",
            "108/108 - 4s - loss: 0.9836 - val_loss: 0.8967\n",
            "108/108 - 3s - loss: 0.9746 - val_loss: 0.8878\n",
            "108/108 - 3s - loss: 0.9955 - val_loss: 0.8458\n",
            "108/108 - 3s - loss: 0.9825 - val_loss: 0.8701\n",
            "108/108 - 3s - loss: 0.9776 - val_loss: 0.8646\n",
            "108/108 - 3s - loss: 0.9868 - val_loss: 0.9132\n",
            "108/108 - 3s - loss: 0.9764 - val_loss: 0.8640\n",
            "108/108 - 4s - loss: 0.9759 - val_loss: 0.8150\n",
            "108/108 - 4s - loss: 0.9748 - val_loss: 0.8565\n",
            "108/108 - 4s - loss: 0.9796 - val_loss: 0.8886\n",
            "108/108 - 4s - loss: 1.0108 - val_loss: 0.8609\n",
            "108/108 - 4s - loss: 1.0145 - val_loss: 0.8457\n",
            "108/108 - 4s - loss: 1.0129 - val_loss: 0.8547\n",
            "108/108 - 4s - loss: 1.0158 - val_loss: 0.8670\n",
            "108/108 - 3s - loss: 0.9824 - val_loss: 0.8450\n",
            "108/108 - 4s - loss: 1.0041 - val_loss: 0.8863\n",
            "108/108 - 4s - loss: 0.9854 - val_loss: 0.8451\n",
            "108/108 - 3s - loss: 0.9754 - val_loss: 0.8331\n",
            "108/108 - 3s - loss: 0.9851 - val_loss: 0.8282\n",
            "108/108 - 3s - loss: 0.9796 - val_loss: 0.8247\n",
            "108/108 - 3s - loss: 0.9866 - val_loss: 0.8785\n",
            "108/108 - 3s - loss: 0.9950 - val_loss: 0.8645\n",
            "108/108 - 4s - loss: 0.9940 - val_loss: 0.9021\n",
            "108/108 - 4s - loss: 0.9726 - val_loss: 0.8597\n",
            "108/108 - 4s - loss: 0.9933 - val_loss: 0.8525\n",
            "108/108 - 4s - loss: 0.9869 - val_loss: 0.8518\n",
            "108/108 - 3s - loss: 1.0128 - val_loss: 0.8588\n",
            "108/108 - 3s - loss: 0.9890 - val_loss: 0.8758\n",
            "108/108 - 3s - loss: 0.9720 - val_loss: 0.8327\n",
            "108/108 - 3s - loss: 0.9866 - val_loss: 0.8469\n",
            "108/108 - 3s - loss: 0.9971 - val_loss: 0.8970\n",
            "108/108 - 3s - loss: 1.0143 - val_loss: 0.8531\n",
            "108/108 - 4s - loss: 0.9571 - val_loss: 0.8560\n",
            "108/108 - 4s - loss: 0.9949 - val_loss: 0.8807\n",
            "108/108 - 4s - loss: 0.9765 - val_loss: 0.8306\n",
            "108/108 - 3s - loss: 1.0103 - val_loss: 0.8768\n",
            "108/108 - 3s - loss: 0.9733 - val_loss: 0.8330\n",
            "108/108 - 3s - loss: 0.9723 - val_loss: 0.8664\n",
            "108/108 - 3s - loss: 0.9735 - val_loss: 0.8600\n",
            "108/108 - 3s - loss: 0.9796 - val_loss: 0.8379\n",
            "108/108 - 3s - loss: 0.9882 - val_loss: 0.8744\n",
            "108/108 - 4s - loss: 0.9801 - val_loss: 0.8948\n",
            "108/108 - 4s - loss: 0.9728 - val_loss: 0.8801\n",
            "108/108 - 4s - loss: 0.9553 - val_loss: 0.8837\n",
            "108/108 - 4s - loss: 0.9604 - val_loss: 0.8663\n",
            "108/108 - 3s - loss: 0.9831 - val_loss: 0.8628\n",
            "108/108 - 3s - loss: 0.9892 - val_loss: 0.8285\n",
            "108/108 - 3s - loss: 0.9749 - val_loss: 0.8632\n",
            "108/108 - 3s - loss: 0.9905 - val_loss: 0.8580\n",
            "108/108 - 3s - loss: 0.9728 - val_loss: 0.8619\n",
            "108/108 - 3s - loss: 0.9826 - val_loss: 0.8807\n",
            "108/108 - 3s - loss: 0.9651 - val_loss: 0.8720\n",
            "108/108 - 3s - loss: 0.9700 - val_loss: 0.8351\n",
            "108/108 - 4s - loss: 0.9719 - val_loss: 0.8343\n",
            "108/108 - 4s - loss: 0.9616 - val_loss: 0.8549\n",
            "108/108 - 3s - loss: 0.9782 - val_loss: 0.8581\n",
            "108/108 - 3s - loss: 0.9486 - val_loss: 0.8436\n",
            "108/108 - 3s - loss: 0.9851 - val_loss: 0.8357\n",
            "108/108 - 3s - loss: 0.9688 - val_loss: 0.8542\n",
            "108/108 - 4s - loss: 0.9816 - val_loss: 0.8298\n",
            "108/108 - 3s - loss: 1.0028 - val_loss: 0.8899\n",
            "108/108 - 3s - loss: 0.9862 - val_loss: 0.8312\n",
            "108/108 - 3s - loss: 0.9899 - val_loss: 0.8620\n",
            "108/108 - 3s - loss: 0.9772 - val_loss: 0.8576\n",
            "108/108 - 3s - loss: 0.9857 - val_loss: 0.8353\n",
            "108/108 - 4s - loss: 0.9768 - val_loss: 0.8485\n",
            "108/108 - 3s - loss: 0.9619 - val_loss: 0.8750\n",
            "108/108 - 3s - loss: 0.9623 - val_loss: 0.8230\n",
            "108/108 - 3s - loss: 0.9872 - val_loss: 0.8399\n",
            "108/108 - 4s - loss: 0.9739 - val_loss: 0.8429\n",
            "108/108 - 3s - loss: 0.9539 - val_loss: 0.8411\n",
            "108/108 - 3s - loss: 0.9832 - val_loss: 0.8772\n",
            "108/108 - 4s - loss: 0.9494 - val_loss: 0.8463\n",
            "108/108 - 4s - loss: 0.9687 - val_loss: 0.8202\n",
            "108/108 - 3s - loss: 0.9990 - val_loss: 0.8483\n",
            "108/108 - 3s - loss: 0.9765 - val_loss: 0.8705\n",
            "108/108 - 4s - loss: 0.9826 - val_loss: 0.8841\n",
            "108/108 - 3s - loss: 0.9779 - val_loss: 0.8817\n",
            "108/108 - 3s - loss: 0.9804 - val_loss: 0.8627\n",
            "108/108 - 3s - loss: 0.9706 - val_loss: 0.8831\n",
            "108/108 - 3s - loss: 0.9741 - val_loss: 0.8719\n",
            "108/108 - 3s - loss: 0.9352 - val_loss: 0.8537\n",
            "108/108 - 3s - loss: 0.9578 - val_loss: 0.8623\n",
            "108/108 - 4s - loss: 0.9833 - val_loss: 0.8549\n",
            "108/108 - 4s - loss: 0.9851 - val_loss: 0.8337\n",
            "108/108 - 4s - loss: 0.9963 - val_loss: 0.8780\n",
            "108/108 - 3s - loss: 0.9716 - val_loss: 0.8779\n",
            "108/108 - 3s - loss: 0.9794 - val_loss: 0.8432\n",
            "108/108 - 3s - loss: 0.9773 - val_loss: 0.8224\n",
            "108/108 - 3s - loss: 0.9658 - val_loss: 0.8279\n",
            "108/108 - 3s - loss: 0.9576 - val_loss: 0.8659\n",
            "108/108 - 3s - loss: 0.9763 - val_loss: 0.8411\n",
            "108/108 - 3s - loss: 0.9857 - val_loss: 0.8487\n",
            "108/108 - 4s - loss: 0.9514 - val_loss: 0.8746\n",
            "108/108 - 4s - loss: 0.9700 - val_loss: 0.8567\n",
            "108/108 - 4s - loss: 0.9730 - val_loss: 0.8421\n",
            "108/108 - 4s - loss: 0.9794 - val_loss: 0.8407\n",
            "108/108 - 4s - loss: 0.9528 - val_loss: 0.8416\n",
            "108/108 - 3s - loss: 0.9775 - val_loss: 0.8355\n",
            "108/108 - 3s - loss: 0.9588 - val_loss: 0.8477\n",
            "108/108 - 3s - loss: 0.9631 - val_loss: 0.8365\n",
            "108/108 - 3s - loss: 0.9888 - val_loss: 0.8612\n",
            "108/108 - 3s - loss: 0.9664 - val_loss: 0.8393\n",
            "108/108 - 3s - loss: 0.9611 - val_loss: 0.8906\n",
            "108/108 - 3s - loss: 0.9522 - val_loss: 0.8589\n",
            "108/108 - 4s - loss: 0.9530 - val_loss: 0.8082\n",
            "108/108 - 3s - loss: 0.9861 - val_loss: 0.8790\n",
            "108/108 - 3s - loss: 0.9631 - val_loss: 0.8501\n",
            "108/108 - 3s - loss: 0.9641 - val_loss: 0.8549\n",
            "108/108 - 3s - loss: 0.9730 - val_loss: 0.8754\n",
            "108/108 - 4s - loss: 0.9891 - val_loss: 0.8328\n",
            "108/108 - 4s - loss: 0.9512 - val_loss: 0.8127\n",
            "108/108 - 4s - loss: 0.9890 - val_loss: 0.8681\n",
            "108/108 - 4s - loss: 0.9585 - val_loss: 0.8438\n",
            "108/108 - 3s - loss: 0.9743 - val_loss: 0.8662\n",
            "108/108 - 3s - loss: 0.9494 - val_loss: 0.8803\n",
            "108/108 - 3s - loss: 0.9516 - val_loss: 0.8470\n",
            "108/108 - 4s - loss: 0.9967 - val_loss: 0.8729\n",
            "108/108 - 3s - loss: 0.9574 - val_loss: 0.8167\n",
            "108/108 - 3s - loss: 0.9612 - val_loss: 0.8585\n",
            "108/108 - 3s - loss: 1.0229 - val_loss: 0.8517\n",
            "108/108 - 4s - loss: 0.9794 - val_loss: 0.8356\n",
            "108/108 - 4s - loss: 0.9537 - val_loss: 0.8512\n",
            "108/108 - 4s - loss: 0.9878 - val_loss: 0.8717\n",
            "108/108 - 4s - loss: 0.9569 - val_loss: 0.8549\n",
            "108/108 - 3s - loss: 0.9665 - val_loss: 0.8199\n",
            "108/108 - 3s - loss: 0.9759 - val_loss: 0.8378\n",
            "108/108 - 4s - loss: 0.9720 - val_loss: 0.8663\n",
            "108/108 - 3s - loss: 0.9580 - val_loss: 0.8420\n",
            "108/108 - 4s - loss: 0.9593 - val_loss: 0.8437\n",
            "108/108 - 4s - loss: 0.9656 - val_loss: 0.8601\n",
            "108/108 - 3s - loss: 0.9724 - val_loss: 0.9064\n",
            "108/108 - 3s - loss: 0.9841 - val_loss: 0.8492\n",
            "108/108 - 3s - loss: 0.9843 - val_loss: 0.8857\n",
            "108/108 - 3s - loss: 0.9853 - val_loss: 0.8695\n",
            "108/108 - 3s - loss: 0.9930 - val_loss: 0.8282\n",
            "108/108 - 3s - loss: 0.9714 - val_loss: 0.8175\n",
            "108/108 - 3s - loss: 0.9516 - val_loss: 0.8266\n",
            "108/108 - 3s - loss: 0.9750 - val_loss: 0.8499\n",
            "108/108 - 3s - loss: 0.9552 - val_loss: 0.8267\n",
            "108/108 - 4s - loss: 0.9546 - val_loss: 0.8591\n",
            "108/108 - 3s - loss: 0.9444 - val_loss: 0.8105\n",
            "108/108 - 3s - loss: 0.9822 - val_loss: 0.8507\n",
            "108/108 - 3s - loss: 0.9614 - val_loss: 0.8467\n",
            "108/108 - 4s - loss: 0.9436 - val_loss: 0.8668\n",
            "108/108 - 3s - loss: 0.9511 - val_loss: 0.8504\n",
            "108/108 - 3s - loss: 0.9584 - val_loss: 0.8630\n",
            "108/108 - 3s - loss: 0.9533 - val_loss: 0.8102\n",
            "108/108 - 3s - loss: 0.9573 - val_loss: 0.8359\n",
            "108/108 - 3s - loss: 0.9597 - val_loss: 0.8452\n",
            "108/108 - 3s - loss: 0.9686 - val_loss: 0.8405\n",
            "108/108 - 3s - loss: 0.9514 - val_loss: 0.8429\n",
            "108/108 - 3s - loss: 0.9452 - val_loss: 0.8551\n",
            "108/108 - 3s - loss: 0.9624 - val_loss: 0.8200\n",
            "108/108 - 3s - loss: 0.9635 - val_loss: 0.8368\n",
            "108/108 - 3s - loss: 0.9722 - val_loss: 0.8421\n",
            "108/108 - 3s - loss: 0.9594 - val_loss: 0.8862\n",
            "108/108 - 3s - loss: 0.9756 - val_loss: 0.8568\n",
            "108/108 - 3s - loss: 0.9918 - val_loss: 0.8677\n",
            "108/108 - 3s - loss: 0.9722 - val_loss: 0.8709\n",
            "108/108 - 3s - loss: 0.9699 - val_loss: 0.8288\n",
            "108/108 - 3s - loss: 0.9791 - val_loss: 0.8961\n",
            "108/108 - 3s - loss: 0.9688 - val_loss: 0.8761\n",
            "108/108 - 3s - loss: 0.9527 - val_loss: 0.8439\n",
            "108/108 - 3s - loss: 0.9396 - val_loss: 0.8400\n",
            "108/108 - 3s - loss: 0.9505 - val_loss: 0.8501\n",
            "108/108 - 3s - loss: 0.9421 - val_loss: 0.8329\n",
            "108/108 - 3s - loss: 0.9711 - val_loss: 0.8673\n",
            "108/108 - 3s - loss: 0.9439 - val_loss: 0.8464\n",
            "108/108 - 3s - loss: 0.9669 - val_loss: 0.8384\n",
            "108/108 - 4s - loss: 0.9614 - val_loss: 0.8429\n",
            "108/108 - 3s - loss: 0.9713 - val_loss: 0.8523\n",
            "108/108 - 3s - loss: 0.9964 - val_loss: 0.8498\n",
            "108/108 - 3s - loss: 0.9817 - val_loss: 0.8883\n",
            "108/108 - 3s - loss: 0.9511 - val_loss: 0.8264\n",
            "108/108 - 3s - loss: 0.9628 - val_loss: 0.8661\n",
            "108/108 - 3s - loss: 0.9509 - val_loss: 0.8268\n",
            "108/108 - 3s - loss: 0.9558 - val_loss: 0.8493\n",
            "108/108 - 3s - loss: 0.9505 - val_loss: 0.8834\n",
            "108/108 - 3s - loss: 0.9603 - val_loss: 0.8372\n",
            "108/108 - 4s - loss: 0.9669 - val_loss: 0.8151\n",
            "108/108 - 3s - loss: 0.9517 - val_loss: 0.8633\n",
            "108/108 - 3s - loss: 0.9630 - val_loss: 0.8355\n",
            "108/108 - 3s - loss: 0.9501 - val_loss: 0.8258\n",
            "108/108 - 3s - loss: 0.9500 - val_loss: 0.8696\n",
            "108/108 - 3s - loss: 0.9516 - val_loss: 0.8397\n",
            "108/108 - 3s - loss: 0.9498 - val_loss: 0.8729\n",
            "108/108 - 3s - loss: 0.9486 - val_loss: 0.8238\n",
            "108/108 - 3s - loss: 0.9578 - val_loss: 0.8964\n",
            "108/108 - 3s - loss: 0.9614 - val_loss: 0.8429\n",
            "108/108 - 3s - loss: 0.9523 - val_loss: 0.8531\n",
            "108/108 - 3s - loss: 0.9638 - val_loss: 0.8600\n",
            "108/108 - 3s - loss: 0.9487 - val_loss: 0.8251\n",
            "108/108 - 3s - loss: 0.9715 - val_loss: 0.8308\n",
            "108/108 - 3s - loss: 0.9449 - val_loss: 0.8519\n",
            "108/108 - 3s - loss: 0.9675 - val_loss: 0.8341\n",
            "108/108 - 3s - loss: 0.9768 - val_loss: 0.8451\n",
            "108/108 - 3s - loss: 0.9662 - val_loss: 0.8900\n",
            "108/108 - 3s - loss: 0.9474 - val_loss: 0.8486\n",
            "108/108 - 3s - loss: 0.9527 - val_loss: 0.8510\n",
            "108/108 - 3s - loss: 0.9509 - val_loss: 0.8541\n",
            "108/108 - 3s - loss: 0.9515 - val_loss: 0.8415\n",
            "108/108 - 3s - loss: 0.9694 - val_loss: 0.8536\n",
            "108/108 - 3s - loss: 0.9808 - val_loss: 0.8422\n",
            "108/108 - 3s - loss: 0.9531 - val_loss: 0.8557\n",
            "108/108 - 3s - loss: 0.9424 - val_loss: 0.8413\n",
            "108/108 - 3s - loss: 0.9597 - val_loss: 0.8524\n",
            "108/108 - 3s - loss: 0.9528 - val_loss: 0.8440\n",
            "108/108 - 3s - loss: 0.9331 - val_loss: 0.8378\n",
            "108/108 - 3s - loss: 0.9755 - val_loss: 0.8401\n",
            "108/108 - 3s - loss: 0.9411 - val_loss: 0.8369\n",
            "108/108 - 3s - loss: 0.9413 - val_loss: 0.8403\n",
            "108/108 - 3s - loss: 0.9668 - val_loss: 0.8103\n",
            "108/108 - 3s - loss: 0.9364 - val_loss: 0.8263\n",
            "108/108 - 3s - loss: 0.9753 - val_loss: 0.8372\n",
            "108/108 - 3s - loss: 0.9509 - val_loss: 0.8318\n",
            "108/108 - 3s - loss: 0.9607 - val_loss: 0.8487\n",
            "108/108 - 3s - loss: 0.9500 - val_loss: 0.8569\n",
            "108/108 - 3s - loss: 0.9369 - val_loss: 0.8313\n",
            "108/108 - 3s - loss: 0.9541 - val_loss: 0.8602\n",
            "108/108 - 3s - loss: 0.9614 - val_loss: 0.8443\n",
            "108/108 - 3s - loss: 0.9652 - val_loss: 0.8913\n",
            "108/108 - 3s - loss: 0.9528 - val_loss: 0.8534\n",
            "108/108 - 3s - loss: 0.9345 - val_loss: 0.8241\n",
            "108/108 - 3s - loss: 0.9460 - val_loss: 0.8493\n",
            "108/108 - 3s - loss: 0.9702 - val_loss: 0.8670\n",
            "108/108 - 3s - loss: 0.9534 - val_loss: 0.8630\n",
            "108/108 - 4s - loss: 0.9664 - val_loss: 0.8909\n",
            "108/108 - 3s - loss: 0.9606 - val_loss: 0.8568\n",
            "108/108 - 3s - loss: 0.9774 - val_loss: 0.8976\n",
            "108/108 - 3s - loss: 0.9433 - val_loss: 0.8578\n",
            "108/108 - 3s - loss: 0.9359 - val_loss: 0.8346\n",
            "108/108 - 3s - loss: 0.9425 - val_loss: 0.8394\n",
            "108/108 - 3s - loss: 0.9493 - val_loss: 0.8263\n",
            "108/108 - 3s - loss: 0.9482 - val_loss: 0.8453\n",
            "108/108 - 3s - loss: 0.9515 - val_loss: 0.8409\n",
            "108/108 - 3s - loss: 0.9630 - val_loss: 0.8345\n",
            "108/108 - 3s - loss: 0.9712 - val_loss: 0.8398\n",
            "108/108 - 3s - loss: 0.9504 - val_loss: 0.8806\n",
            "108/108 - 3s - loss: 0.9493 - val_loss: 0.8266\n",
            "108/108 - 3s - loss: 0.9249 - val_loss: 0.8118\n",
            "108/108 - 3s - loss: 0.9589 - val_loss: 0.8116\n",
            "108/108 - 3s - loss: 0.9635 - val_loss: 0.8438\n",
            "108/108 - 3s - loss: 0.9527 - val_loss: 0.8338\n",
            "108/108 - 3s - loss: 0.9658 - val_loss: 0.8155\n",
            "108/108 - 3s - loss: 0.9534 - val_loss: 0.8438\n",
            "108/108 - 3s - loss: 0.9529 - val_loss: 0.8325\n",
            "108/108 - 3s - loss: 0.9401 - val_loss: 0.8323\n",
            "108/108 - 3s - loss: 0.9525 - val_loss: 0.8680\n",
            "108/108 - 3s - loss: 0.9587 - val_loss: 0.8257\n",
            "108/108 - 3s - loss: 0.9709 - val_loss: 0.8390\n",
            "108/108 - 3s - loss: 0.9602 - val_loss: 0.8273\n",
            "108/108 - 3s - loss: 0.9600 - val_loss: 0.8256\n",
            "108/108 - 3s - loss: 0.9629 - val_loss: 0.8416\n",
            "108/108 - 3s - loss: 0.9501 - val_loss: 0.8810\n",
            "108/108 - 3s - loss: 0.9560 - val_loss: 0.8105\n",
            "108/108 - 3s - loss: 0.9716 - val_loss: 0.8376\n",
            "108/108 - 3s - loss: 0.9470 - val_loss: 0.8280\n",
            "108/108 - 3s - loss: 0.9433 - val_loss: 0.8546\n",
            "108/108 - 3s - loss: 0.9508 - val_loss: 0.8343\n",
            "108/108 - 3s - loss: 0.9619 - val_loss: 0.8408\n",
            "108/108 - 3s - loss: 0.9503 - val_loss: 0.8409\n",
            "108/108 - 3s - loss: 0.9511 - val_loss: 0.8231\n",
            "108/108 - 4s - loss: 0.9518 - val_loss: 0.8612\n",
            "108/108 - 3s - loss: 0.9451 - val_loss: 0.8429\n",
            "108/108 - 3s - loss: 0.9484 - val_loss: 0.8118\n",
            "108/108 - 3s - loss: 0.9627 - val_loss: 0.8196\n",
            "108/108 - 3s - loss: 0.9620 - val_loss: 0.8494\n",
            "108/108 - 4s - loss: 0.9721 - val_loss: 0.8491\n",
            "108/108 - 3s - loss: 0.9449 - val_loss: 0.8483\n",
            "108/108 - 3s - loss: 0.9637 - val_loss: 0.8448\n",
            "108/108 - 3s - loss: 0.9605 - val_loss: 0.8535\n",
            "108/108 - 3s - loss: 0.9548 - val_loss: 0.8210\n",
            "finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELO9wznnSAy3"
      },
      "source": [
        "n_batch = 8\r\n",
        "early_stopping = EarlyStopping(monitor='val_loss',patience = 20)\r\n",
        "\r\n",
        "for l, q in enumerate(np.arange(0.1, 1, 0.1)):\r\n",
        "  print(q)\r\n",
        "  model = Sequential()\r\n",
        "  for i in range(2):\r\n",
        "      model.add(LSTM(64, batch_input_shape=(8, 336, 8), stateful=True, return_sequences=True))\r\n",
        "      model.add(Dropout(0.3))\r\n",
        "  model.add(LSTM(64, batch_input_shape=(8, 336, 8), stateful=True))\r\n",
        "  model.add(Dropout(0.3))\r\n",
        "  model.add(Dense(96))\r\n",
        "  model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "  for j in range(100):\r\n",
        "    # model.fit(train_x, train_y, epochs=1, batch_size=1, shuffle=False, callbacks=[custom_hist], validation_data=(val_x, val_y))\r\n",
        "    model.fit(train_x, train_y, epochs=1, batch_size=n_batch, shuffle=False, validation_data=(val_x, val_y), callbacks=[early_stopping], verbose=2)\r\n",
        "    model.reset_states()\r\n",
        "  weights = model.get_weights()\r\n",
        "  \r\n",
        "  single_item_model = Sequential()\r\n",
        "  for i in range(2):\r\n",
        "      single_item_model.add(LSTM(64, batch_input_shape=(1, 336, 8), stateful=True, return_sequences=True))\r\n",
        "      single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(LSTM(64, batch_input_shape=(1, 336, 8), stateful=True))\r\n",
        "  single_item_model.add(Dropout(0.3))\r\n",
        "  single_item_model.add(Dense(96))\r\n",
        "  single_item_model.set_weights(weights)\r\n",
        "  single_item_model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "  \r\n",
        "  predictions = []\r\n",
        "  for k in range(81):\r\n",
        "    prediction = single_item_model.predict(np.array([test[k]]), batch_size=1)\r\n",
        "    predictions.append(sun_rise(np.array([test[k]]),prediction))\r\n",
        "  predictions = np.reshape(np.concatenate(np.array(predictions), axis=0),(81*96))\r\n",
        "  submission.iloc[:,l+1] = predictions\r\n",
        "submission.to_csv(f'submission.csv_3', index=False)\r\n",
        "print('finish')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLUCCEMOoGt7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5Wq-jD_x2m6",
        "outputId": "9dcb6d1a-8dd3-4f07-f52e-f851e7871a0f"
      },
      "source": [
        "model = Sequential()\r\n",
        "for i in range(2):\r\n",
        "    model.add(LSTM(64, batch_input_shape=(8, 336, 8), stateful=True, return_sequences=True))\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "model.add(LSTM(64, batch_input_shape=(8, 336, 8), stateful=True))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "model.add(Dense(96))\r\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\r\n",
        "for i in range(500):\r\n",
        "    # model.fit(train_x, train_y, epochs=1, batch_size=1, shuffle=False, validation_data=(val_x, val_y))\r\n",
        "    model.fit(train_x, train_y, epochs=1, batch_size=8, shuffle=False, validation_data=(val_x, val_y))\r\n",
        "    model.reset_states()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108/108 [==============================] - 7s 41ms/step - loss: 859.9651 - val_loss: 1022.6951\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 657.7905 - val_loss: 828.4251\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 525.1588 - val_loss: 684.2260\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 430.4552 - val_loss: 575.6992\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 361.1811 - val_loss: 493.5324\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 307.9523 - val_loss: 431.9735\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 273.2994 - val_loss: 385.5602\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 246.7000 - val_loss: 351.9617\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 230.4998 - val_loss: 327.2189\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 220.4721 - val_loss: 308.5563\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 213.9492 - val_loss: 295.9988\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 208.1882 - val_loss: 286.0696\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 203.2014 - val_loss: 279.2719\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 202.0149 - val_loss: 274.2828\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 203.9424 - val_loss: 271.0159\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 200.4201 - val_loss: 268.0067\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 203.0058 - val_loss: 266.3394\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 201.3937 - val_loss: 264.9567\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 199.7646 - val_loss: 263.7782\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 201.6994 - val_loss: 263.3318\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 198.6279 - val_loss: 262.5056\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 201.4967 - val_loss: 262.2181\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 199.5972 - val_loss: 262.3137\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 201.1424 - val_loss: 262.1779\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 201.4853 - val_loss: 262.2569\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 200.5434 - val_loss: 262.0359\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 197.3235 - val_loss: 262.2540\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 198.7345 - val_loss: 261.9176\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 200.7065 - val_loss: 262.4185\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 201.2199 - val_loss: 262.2558\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 199.4677 - val_loss: 261.8260\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 199.6114 - val_loss: 261.3956\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 202.5371 - val_loss: 262.6147\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 201.8652 - val_loss: 264.3016\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 201.3932 - val_loss: 263.6063\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 199.6406 - val_loss: 262.8414\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 196.8138 - val_loss: 260.0692\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 191.7503 - val_loss: 255.1165\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 186.9429 - val_loss: 250.5672\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 186.9249 - val_loss: 247.3459\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 180.4353 - val_loss: 243.3583\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 180.8669 - val_loss: 241.5345\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 175.7989 - val_loss: 238.6568\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 170.2499 - val_loss: 236.3720\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 165.0194 - val_loss: 234.9928\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 159.3993 - val_loss: 228.9878\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 154.7075 - val_loss: 227.3630\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 151.2549 - val_loss: 226.9280\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 152.7727 - val_loss: 219.2358\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 152.6898 - val_loss: 219.6397\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 151.0862 - val_loss: 217.4733\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 147.4376 - val_loss: 215.8473\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 144.5465 - val_loss: 216.2288\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 146.0462 - val_loss: 216.3044\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 147.4839 - val_loss: 213.9315\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 145.5001 - val_loss: 212.9104\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 142.6212 - val_loss: 214.2949\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 140.4913 - val_loss: 215.5007\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 141.7469 - val_loss: 212.0372\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 141.0668 - val_loss: 205.5183\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 140.8304 - val_loss: 215.7335\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 140.8619 - val_loss: 209.7388\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 142.1111 - val_loss: 214.6340\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 139.4365 - val_loss: 216.0751\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 136.8890 - val_loss: 216.1364\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 136.6453 - val_loss: 215.7719\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 138.9545 - val_loss: 213.6123\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 137.9221 - val_loss: 215.3310\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 137.2407 - val_loss: 211.7935\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 135.0260 - val_loss: 209.3952\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 138.5582 - val_loss: 213.7062\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 132.2302 - val_loss: 208.3923\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 133.0959 - val_loss: 205.2996\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 137.3127 - val_loss: 211.1142\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 136.6839 - val_loss: 212.0952\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 137.5183 - val_loss: 211.2361\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 136.2919 - val_loss: 209.1730\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 135.4033 - val_loss: 209.0534\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 134.7150 - val_loss: 212.9424\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 135.6460 - val_loss: 212.4734\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 135.8020 - val_loss: 212.4485\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 134.9470 - val_loss: 208.2655\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 130.7317 - val_loss: 207.3828\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 134.1566 - val_loss: 209.7847\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 133.0737 - val_loss: 210.5090\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 136.3683 - val_loss: 206.7833\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 135.4561 - val_loss: 209.0398\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 135.9967 - val_loss: 209.6701\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 134.7682 - val_loss: 208.1613\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 134.0625 - val_loss: 205.3179\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 135.1438 - val_loss: 210.0502\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 133.0922 - val_loss: 206.1773\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 133.2565 - val_loss: 207.3361\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 137.6890 - val_loss: 209.8458\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 133.8142 - val_loss: 208.5797\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 134.4520 - val_loss: 209.4836\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 130.6577 - val_loss: 209.1804\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 132.5011 - val_loss: 206.6481\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 134.8240 - val_loss: 205.1615\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 131.6037 - val_loss: 207.1144\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 132.7843 - val_loss: 206.8399\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 135.3175 - val_loss: 205.4927\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 130.0273 - val_loss: 202.9012\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 132.4184 - val_loss: 205.3784\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 133.2397 - val_loss: 204.8323\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 132.9344 - val_loss: 201.6136\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 130.9069 - val_loss: 203.7822\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 130.6996 - val_loss: 205.2978\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 131.8709 - val_loss: 204.8477\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 131.7577 - val_loss: 207.3963\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 130.4843 - val_loss: 207.3525\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 129.6118 - val_loss: 205.2939\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 129.0995 - val_loss: 208.8569\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 131.6929 - val_loss: 207.1327\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 132.0413 - val_loss: 204.8755\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 129.8862 - val_loss: 205.2711\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 132.2896 - val_loss: 208.1080\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 131.4878 - val_loss: 202.0280\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 128.8789 - val_loss: 201.7228\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 127.8240 - val_loss: 201.6789\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 128.0975 - val_loss: 209.6519\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 130.2764 - val_loss: 202.9505\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 129.7680 - val_loss: 208.9658\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 129.9084 - val_loss: 202.6183\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 127.3833 - val_loss: 205.9720\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 128.7825 - val_loss: 204.8008\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 128.9634 - val_loss: 203.0011\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 127.1395 - val_loss: 205.7285\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 128.3358 - val_loss: 202.3458\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 125.7544 - val_loss: 201.1748\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 129.9825 - val_loss: 205.6891\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 128.0625 - val_loss: 206.6795\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 128.2672 - val_loss: 213.0417\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 130.0670 - val_loss: 208.2218\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 127.2250 - val_loss: 206.5393\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 128.2248 - val_loss: 208.0655\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 133.3088 - val_loss: 209.1583\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 129.2767 - val_loss: 203.0922\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 127.1673 - val_loss: 208.0582\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 123.3329 - val_loss: 202.0119\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 128.8227 - val_loss: 204.3268\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 127.1508 - val_loss: 210.9681\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 126.4403 - val_loss: 198.9117\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 125.9180 - val_loss: 207.4338\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 127.5137 - val_loss: 205.9360\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 125.6080 - val_loss: 201.0309\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 122.9807 - val_loss: 201.9865\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 126.0646 - val_loss: 210.7017\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 123.8273 - val_loss: 206.6184\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 125.6643 - val_loss: 207.1230\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 128.5538 - val_loss: 202.8897\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 125.0388 - val_loss: 205.1939\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 125.9636 - val_loss: 207.0096\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 125.7868 - val_loss: 208.6474\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 123.4629 - val_loss: 203.4383\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 124.1681 - val_loss: 209.3107\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 123.6038 - val_loss: 199.0393\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 125.4046 - val_loss: 205.8887\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 121.6795 - val_loss: 214.7217\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 122.5798 - val_loss: 196.2656\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 123.8341 - val_loss: 218.1172\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 124.3725 - val_loss: 203.8315\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 124.2837 - val_loss: 205.6160\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 124.4493 - val_loss: 209.6847\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 123.1868 - val_loss: 216.0600\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 124.7827 - val_loss: 203.2442\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 123.6587 - val_loss: 214.4582\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 122.6052 - val_loss: 200.5891\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 121.0302 - val_loss: 214.2003\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 121.9515 - val_loss: 206.0014\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 120.9394 - val_loss: 205.0688\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 120.9602 - val_loss: 210.4209\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 125.6129 - val_loss: 218.3235\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 124.3450 - val_loss: 206.5236\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 126.3969 - val_loss: 211.3652\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 124.6317 - val_loss: 204.3800\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 122.6566 - val_loss: 206.7776\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 123.2025 - val_loss: 203.9035\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 122.3676 - val_loss: 205.5976\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 121.8234 - val_loss: 214.1060\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 124.9194 - val_loss: 205.9756\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 122.3498 - val_loss: 206.2187\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 120.1337 - val_loss: 201.6897\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 121.4531 - val_loss: 205.2658\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 121.9526 - val_loss: 201.8900\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 117.6682 - val_loss: 201.7681\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 120.1116 - val_loss: 207.1313\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 120.2087 - val_loss: 204.0978\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 122.4967 - val_loss: 205.2449\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 123.3319 - val_loss: 199.9534\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.9554 - val_loss: 208.6759\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.9186 - val_loss: 205.9644\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 120.2699 - val_loss: 203.1255\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 120.9161 - val_loss: 203.3273\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.1492 - val_loss: 202.2135\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 119.5168 - val_loss: 206.8219\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.0647 - val_loss: 204.0122\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.1058 - val_loss: 199.1028\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 120.2084 - val_loss: 201.1121\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.5470 - val_loss: 205.9075\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.2734 - val_loss: 210.2859\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.8367 - val_loss: 205.5141\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.2077 - val_loss: 208.8631\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 117.6209 - val_loss: 205.2067\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.0814 - val_loss: 204.1191\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.6268 - val_loss: 206.1275\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 117.4759 - val_loss: 203.2364\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.1930 - val_loss: 200.6470\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.2725 - val_loss: 209.8317\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.9898 - val_loss: 202.1415\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.4610 - val_loss: 202.6566\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.2777 - val_loss: 212.4205\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.3088 - val_loss: 203.5943\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.9720 - val_loss: 203.0264\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.8212 - val_loss: 206.1392\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.3809 - val_loss: 212.2397\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.6203 - val_loss: 204.6673\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.2346 - val_loss: 214.2802\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 117.8981 - val_loss: 216.0379\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 117.8884 - val_loss: 203.2974\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 117.1917 - val_loss: 208.7745\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.9805 - val_loss: 213.2525\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.9207 - val_loss: 208.9305\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 114.3005 - val_loss: 208.2418\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 113.9456 - val_loss: 214.2439\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 114.4302 - val_loss: 213.7063\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.7217 - val_loss: 209.7751\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 113.5334 - val_loss: 217.0433\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 114.3120 - val_loss: 208.3637\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 112.3553 - val_loss: 216.4082\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 118.4192 - val_loss: 211.0172\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 114.8749 - val_loss: 201.8559\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 114.2102 - val_loss: 232.2856\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 119.1798 - val_loss: 209.8862\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 112.2464 - val_loss: 207.5892\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.5181 - val_loss: 209.7446\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 112.0447 - val_loss: 208.1903\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.2803 - val_loss: 211.7450\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 112.4278 - val_loss: 218.4080\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.3435 - val_loss: 215.5501\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 112.1587 - val_loss: 212.3282\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 114.0731 - val_loss: 221.4671\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.9200 - val_loss: 207.4227\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 115.6850 - val_loss: 213.9371\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.3940 - val_loss: 204.2932\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 113.7485 - val_loss: 203.3004\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 112.3609 - val_loss: 210.9766\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 112.7863 - val_loss: 209.3333\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 110.8737 - val_loss: 213.5617\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 113.7069 - val_loss: 212.2293\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.0419 - val_loss: 205.2051\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.7200 - val_loss: 209.3090\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 112.6345 - val_loss: 209.0839\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 113.9322 - val_loss: 201.9485\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 113.5266 - val_loss: 214.8943\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 113.0515 - val_loss: 209.9521\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.3812 - val_loss: 210.6265\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.1318 - val_loss: 208.7383\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 111.2547 - val_loss: 211.0148\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.6022 - val_loss: 208.7785\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 110.7514 - val_loss: 215.2365\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 112.3308 - val_loss: 211.0542\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 111.2282 - val_loss: 216.1279\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 114.4963 - val_loss: 206.4607\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 111.2152 - val_loss: 212.2432\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.0879 - val_loss: 209.8050\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.5301 - val_loss: 201.3472\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.0879 - val_loss: 215.4642\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.3942 - val_loss: 212.5108\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.1834 - val_loss: 207.4594\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 110.6936 - val_loss: 202.4212\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.6336 - val_loss: 211.4864\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 110.3270 - val_loss: 206.8305\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 108.2280 - val_loss: 205.0042\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.1023 - val_loss: 209.4272\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.6401 - val_loss: 212.2599\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.6727 - val_loss: 212.1854\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.7039 - val_loss: 215.2870\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 107.4925 - val_loss: 210.9609\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.3208 - val_loss: 216.7461\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.8130 - val_loss: 209.0926\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.2423 - val_loss: 214.0816\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.9516 - val_loss: 212.5574\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.9197 - val_loss: 215.0730\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 110.0083 - val_loss: 210.1973\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.6285 - val_loss: 213.9649\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.0573 - val_loss: 216.1398\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.9193 - val_loss: 203.2056\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 108.3084 - val_loss: 199.4683\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.3114 - val_loss: 215.2296\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.2462 - val_loss: 215.3944\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 104.5550 - val_loss: 206.8651\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.7050 - val_loss: 212.1842\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.1267 - val_loss: 206.9747\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.5696 - val_loss: 209.1484\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.4077 - val_loss: 215.6708\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.4026 - val_loss: 204.7547\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.4882 - val_loss: 212.0745\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 106.0894 - val_loss: 208.8682\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.0483 - val_loss: 202.1247\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.5781 - val_loss: 206.4769\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.2062 - val_loss: 211.9182\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.5521 - val_loss: 217.5726\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 111.1132 - val_loss: 211.0556\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 109.4142 - val_loss: 204.0619\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 108.1741 - val_loss: 206.7870\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.9151 - val_loss: 203.3497\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.5755 - val_loss: 209.6728\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 104.5642 - val_loss: 207.7007\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 104.1416 - val_loss: 200.2662\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.8489 - val_loss: 204.0418\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.5137 - val_loss: 199.5572\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.6967 - val_loss: 206.4765\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.3149 - val_loss: 207.5183\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 104.1481 - val_loss: 205.9817\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 104.5784 - val_loss: 203.5760\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.8901 - val_loss: 204.5355\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.2334 - val_loss: 204.7179\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 107.6161 - val_loss: 199.3689\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.9747 - val_loss: 204.0046\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.5279 - val_loss: 210.4779\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 104.3794 - val_loss: 207.2088\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.8292 - val_loss: 207.0982\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.5628 - val_loss: 206.6840\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.0597 - val_loss: 201.5157\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.9432 - val_loss: 203.2559\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.1287 - val_loss: 205.9749\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.3598 - val_loss: 206.4745\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 116.5947 - val_loss: 206.7827\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 110.9336 - val_loss: 204.4686\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.8032 - val_loss: 211.0885\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.9126 - val_loss: 211.5514\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 107.0519 - val_loss: 217.4867\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 106.9117 - val_loss: 206.9096\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.3612 - val_loss: 218.9250\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 104.9638 - val_loss: 212.9633\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.7798 - val_loss: 207.7161\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 102.6579 - val_loss: 213.4529\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 101.4171 - val_loss: 216.1117\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 101.7777 - val_loss: 216.0378\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 102.5048 - val_loss: 213.0779\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 101.6146 - val_loss: 209.6638\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 103.0290 - val_loss: 205.2574\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 102.3885 - val_loss: 207.6820\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 103.2088 - val_loss: 208.0166\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 101.1974 - val_loss: 210.0749\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 102.1370 - val_loss: 209.4342\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 100.5277 - val_loss: 207.0075\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.7777 - val_loss: 203.8460\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.4400 - val_loss: 211.1292\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 104.6340 - val_loss: 217.2846\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.6187 - val_loss: 212.2184\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.7285 - val_loss: 208.4099\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.0734 - val_loss: 211.3340\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 102.4160 - val_loss: 217.5274\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.0013 - val_loss: 212.3901\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.6315 - val_loss: 206.9925\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.0020 - val_loss: 207.1712\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 102.6775 - val_loss: 214.3876\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 101.2040 - val_loss: 210.9760\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 105.0285 - val_loss: 197.0009\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 105.8398 - val_loss: 210.9253\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 105.3327 - val_loss: 202.1720\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 100.4437 - val_loss: 214.7521\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 102.0210 - val_loss: 213.4675\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.4715 - val_loss: 210.2735\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.8142 - val_loss: 210.4614\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.2073 - val_loss: 216.0927\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.7573 - val_loss: 214.5076\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.1376 - val_loss: 209.7690\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.5760 - val_loss: 204.7313\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.9423 - val_loss: 201.7780\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.0659 - val_loss: 211.4903\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 101.7668 - val_loss: 216.7803\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 102.3754 - val_loss: 211.6507\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 100.3219 - val_loss: 205.6725\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.7136 - val_loss: 218.3202\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 101.0554 - val_loss: 210.6244\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 100.4238 - val_loss: 204.1638\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 102.6660 - val_loss: 206.7486\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.0401 - val_loss: 210.6242\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 102.3748 - val_loss: 220.0034\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 100.9733 - val_loss: 211.0648\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 100.6321 - val_loss: 205.9954\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 98.2353 - val_loss: 208.9274\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.2217 - val_loss: 212.1165\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.3238 - val_loss: 209.1020\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.3632 - val_loss: 210.5781\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.8006 - val_loss: 205.5989\n",
            "108/108 [==============================] - 3s 31ms/step - loss: 99.6073 - val_loss: 201.9891\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.8649 - val_loss: 207.9827\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.2553 - val_loss: 207.9209\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 98.5323 - val_loss: 208.7183\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 98.4638 - val_loss: 220.8983\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 100.0016 - val_loss: 212.9894\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 99.7339 - val_loss: 216.6509\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 100.0075 - val_loss: 214.3055\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 98.2545 - val_loss: 208.3457\n",
            "108/108 [==============================] - 4s 35ms/step - loss: 98.0409 - val_loss: 206.5497\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 98.5965 - val_loss: 203.5496\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 99.0588 - val_loss: 210.8444\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 102.5424 - val_loss: 210.0165\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 98.2222 - val_loss: 211.5554\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 99.0907 - val_loss: 216.8052\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.6246 - val_loss: 213.3869\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.2400 - val_loss: 208.0352\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.1561 - val_loss: 218.2005\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.9382 - val_loss: 207.8958\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.2720 - val_loss: 213.9450\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.7523 - val_loss: 215.1298\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.5096 - val_loss: 213.8440\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.1857 - val_loss: 212.4024\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.2802 - val_loss: 211.4549\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.3374 - val_loss: 217.0624\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.0211 - val_loss: 210.0867\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.9572 - val_loss: 215.0886\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.7032 - val_loss: 210.1034\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.5364 - val_loss: 208.6744\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 97.1303 - val_loss: 215.3036\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.7814 - val_loss: 203.1730\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 103.0353 - val_loss: 208.4204\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.6037 - val_loss: 216.2549\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.6840 - val_loss: 212.1489\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 100.0733 - val_loss: 223.3821\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.4778 - val_loss: 223.3869\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.3901 - val_loss: 202.1365\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.8055 - val_loss: 209.2166\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.7435 - val_loss: 210.4256\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.7464 - val_loss: 216.5970\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.0153 - val_loss: 205.8242\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.6446 - val_loss: 211.6694\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.7230 - val_loss: 218.9524\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.8258 - val_loss: 218.1261\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.1150 - val_loss: 216.7562\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.5120 - val_loss: 219.5367\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.2048 - val_loss: 213.3337\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.7862 - val_loss: 210.5365\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.1927 - val_loss: 210.2959\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 98.3169 - val_loss: 200.9281\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.4630 - val_loss: 213.3520\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 97.9980 - val_loss: 214.7769\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 97.5287 - val_loss: 207.3425\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 96.4003 - val_loss: 202.7490\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.7112 - val_loss: 212.5323\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.9135 - val_loss: 217.9975\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.1367 - val_loss: 209.0777\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.9665 - val_loss: 205.6181\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.6781 - val_loss: 202.6409\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.8012 - val_loss: 201.8852\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.3356 - val_loss: 202.0204\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.7064 - val_loss: 210.6799\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 99.3806 - val_loss: 200.7771\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.6012 - val_loss: 207.7558\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.1438 - val_loss: 215.3229\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.3995 - val_loss: 207.4613\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.0713 - val_loss: 212.5150\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.0254 - val_loss: 205.3490\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.2756 - val_loss: 219.3003\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 102.1684 - val_loss: 203.3691\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.4176 - val_loss: 210.3609\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.5618 - val_loss: 206.1192\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.7517 - val_loss: 219.3237\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 94.5044 - val_loss: 218.7483\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.8708 - val_loss: 206.3886\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 98.6596 - val_loss: 211.2130\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.9901 - val_loss: 211.8046\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.7595 - val_loss: 215.4093\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.6207 - val_loss: 215.4430\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 94.9082 - val_loss: 216.0576\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.5436 - val_loss: 216.1343\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 93.7400 - val_loss: 215.1615\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 94.6334 - val_loss: 217.8672\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.8800 - val_loss: 210.2760\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.7784 - val_loss: 196.7756\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 94.5136 - val_loss: 212.1273\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.8880 - val_loss: 211.2331\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.7151 - val_loss: 208.9024\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 100.3113 - val_loss: 209.8999\n",
            "108/108 [==============================] - 4s 33ms/step - loss: 97.1483 - val_loss: 214.2530\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.6450 - val_loss: 218.1215\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.3422 - val_loss: 217.4662\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 94.8062 - val_loss: 204.5787\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.2180 - val_loss: 214.3150\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 93.7076 - val_loss: 201.1180\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 93.5150 - val_loss: 219.7747\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.3648 - val_loss: 214.8577\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 93.1521 - val_loss: 213.1200\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.8147 - val_loss: 207.4779\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.0450 - val_loss: 203.3144\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.5517 - val_loss: 202.6660\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 93.2330 - val_loss: 213.8838\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 92.7806 - val_loss: 219.7321\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 93.2755 - val_loss: 208.6139\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.2048 - val_loss: 218.5752\n",
            "108/108 [==============================] - 4s 32ms/step - loss: 97.0195 - val_loss: 214.9752\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 95.5677 - val_loss: 206.8572\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 94.7710 - val_loss: 198.9467\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 97.6469 - val_loss: 207.1528\n",
            "108/108 [==============================] - 4s 34ms/step - loss: 98.1667 - val_loss: 205.3533\n",
            "108/108 [==============================] - 3s 32ms/step - loss: 96.2586 - val_loss: 205.4209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW3upYC_PAIO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tm1QXF2q7cPY",
        "outputId": "691a2cc8-41ea-4e4f-e6d8-acbaaf23bb05"
      },
      "source": [
        "# 모델 사용하기\r\n",
        "xhat = test_x[5]\r\n",
        "print(xhat)\r\n",
        "print(xhat.shape)\r\n",
        "weights = model.get_weights()\r\n",
        "single_item_model = Sequential()\r\n",
        "for l in range(2):\r\n",
        "    single_item_model.add(LSTM(64, batch_input_shape=(1, 336, 8), stateful=True, return_sequences=True))\r\n",
        "    single_item_model.add(Dropout(0.3))\r\n",
        "single_item_model.add(LSTM(64, batch_input_shape=(1, 336, 8), stateful=True))\r\n",
        "single_item_model.add(Dropout(0.3))\r\n",
        "single_item_model.add(Dense(96))  \r\n",
        "single_item_model.set_weights(weights)\r\n",
        "single_item_model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "\r\n",
        "prediction = single_item_model.predict(np.array([xhat]), batch_size=1)\r\n",
        "print(prediction)\r\n",
        "print(prediction.shape)\r\n",
        "print(max(prediction[0]))\r\n",
        "\r\n",
        "plt.plot(np.reshape(prediction,(96)))\r\n",
        "plt.plot(test_y[5])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.          0.          0.         ...  2.         -6.97958958\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  2.         -6.03649269\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  2.         -6.8010495\n",
            "   0.        ]\n",
            " ...\n",
            " [ 0.          0.          0.         ...  8.         -6.19348764\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  8.         -6.87535672\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  8.         -6.87535672\n",
            "   0.        ]]\n",
            "(336, 8)\n",
            "[[ 1.0400398e-02  1.0220265e-02 -1.4259620e-03  8.6406991e-03\n",
            "   1.1003657e-05 -1.2589195e-03 -3.8408507e-03  3.4533988e-04\n",
            "   3.5727402e-04  6.4742300e-03 -8.4702790e-02 -2.1103913e-01\n",
            "  -1.3211393e-01  1.1354380e+00  5.1237760e+00  1.1949825e+01\n",
            "   1.9845173e+01  2.8047077e+01  3.5804272e+01  4.2385468e+01\n",
            "   4.8985622e+01  5.4488998e+01  5.8077351e+01  5.9215370e+01\n",
            "   5.9276745e+01  5.7831165e+01  5.5809174e+01  5.2795124e+01\n",
            "   4.8466644e+01  4.2964100e+01  3.6847786e+01  2.9137970e+01\n",
            "   2.2038338e+01  1.4502323e+01  7.3436098e+00  2.3277428e+00\n",
            "   7.5787902e-02 -2.2383639e-01 -1.1414035e-01 -1.3801586e-03\n",
            "   2.9116459e-03 -2.7425822e-03 -4.2894902e-03  8.9382995e-03\n",
            "  -4.0182327e-03  1.7160038e-04  1.5630922e-03  1.6656337e-02\n",
            "   3.1947577e-03  1.9714730e-03 -3.8288215e-03 -2.0267344e-03\n",
            "  -3.8092365e-04  2.1635670e-02  1.1170528e-02 -1.5084597e-02\n",
            "  -3.0873690e-04  3.8788693e-03 -8.5881710e-02 -2.1077293e-01\n",
            "  -1.2749046e-01  1.1470952e+00  5.1321597e+00  1.1701321e+01\n",
            "   1.9564472e+01  2.7552557e+01  3.5290409e+01  4.1389542e+01\n",
            "   4.7856621e+01  5.3202423e+01  5.6643291e+01  5.8095985e+01\n",
            "   5.8547314e+01  5.6893696e+01  5.4616898e+01  5.1225552e+01\n",
            "   4.7021931e+01  4.1290596e+01  3.4953945e+01  2.8108711e+01\n",
            "   2.1519293e+01  1.3950696e+01  7.2278862e+00  2.3680501e+00\n",
            "   6.1583072e-02 -2.2265515e-01 -9.7680457e-02  1.3742230e-03\n",
            "  -2.1564127e-05  1.1310115e-04 -1.4088640e-03  1.6344383e-03\n",
            "  -7.7638775e-03  4.8369329e-06  1.0763811e-02 -5.5866537e-04]]\n",
            "(1, 96)\n",
            "59.276745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f09ff3a4ac8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxQ2dP_bJ_jT",
        "outputId": "d67fb649-4211-4603-d0d2-f0e0a8b3c6b2"
      },
      "source": [
        "model = Sequential()\r\n",
        "for i in range(2):\r\n",
        "    model.add(LSTM(128, batch_input_shape=(8, 336, 8), stateful=True, return_sequences=True))\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "model.add(LSTM(128, batch_input_shape=(8, 336, 8), stateful=True))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "model.add(Dense(96))\r\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\r\n",
        "for i in range(500):\r\n",
        "    # model.fit(train_x, train_y, epochs=1, batch_size=1, shuffle=False, validation_data=(val_x, val_y))\r\n",
        "    model.fit(train_x, train_y, epochs=1, batch_size=8, shuffle=False, validation_data=(val_x, val_y))\r\n",
        "    model.reset_states()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108/108 [==============================] - 9s 48ms/step - loss: 797.8747 - val_loss: 823.1141\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 470.0626 - val_loss: 569.1917\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 329.7916 - val_loss: 427.5873\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 257.4648 - val_loss: 350.2960\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 225.1470 - val_loss: 309.0214\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 211.1353 - val_loss: 287.4733\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 203.6599 - val_loss: 275.3723\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 202.1613 - val_loss: 269.6462\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 201.8714 - val_loss: 266.5604\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 200.8511 - val_loss: 265.0192\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 199.8038 - val_loss: 263.5818\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 199.8558 - val_loss: 262.8353\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 201.2488 - val_loss: 263.0153\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 197.2195 - val_loss: 263.0013\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 199.8562 - val_loss: 262.6386\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 200.4070 - val_loss: 262.7330\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 200.8947 - val_loss: 262.7742\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 198.4311 - val_loss: 262.6438\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 200.9937 - val_loss: 262.7369\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 200.3066 - val_loss: 263.3845\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 200.6561 - val_loss: 262.9546\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 200.8413 - val_loss: 263.4729\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 202.1788 - val_loss: 260.5580\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 195.3961 - val_loss: 261.1837\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 193.8508 - val_loss: 265.0963\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 193.2599 - val_loss: 260.0015\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 185.5220 - val_loss: 254.8879\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 183.9309 - val_loss: 252.9146\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 220.9703 - val_loss: 250.8012\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 182.8037 - val_loss: 241.7653\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 181.4564 - val_loss: 239.6996\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 181.9769 - val_loss: 238.8329\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 179.0582 - val_loss: 235.4366\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 177.2991 - val_loss: 237.5542\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 177.3555 - val_loss: 235.1676\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 177.6097 - val_loss: 232.3377\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 177.7572 - val_loss: 233.9461\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 173.4206 - val_loss: 234.2982\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 174.0482 - val_loss: 234.0679\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 177.5605 - val_loss: 236.3291\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 174.2393 - val_loss: 235.9735\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 170.6576 - val_loss: 233.9085\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 171.5760 - val_loss: 232.0979\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 169.0292 - val_loss: 231.8817\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 162.6732 - val_loss: 231.4592\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 170.9374 - val_loss: 229.6828\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 158.7537 - val_loss: 227.3165\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 165.2270 - val_loss: 224.9859\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 157.1260 - val_loss: 221.5439\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 154.8260 - val_loss: 220.8286\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 153.2951 - val_loss: 220.9778\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 149.4817 - val_loss: 223.1447\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 148.5770 - val_loss: 214.5982\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 143.5224 - val_loss: 217.4339\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 142.4800 - val_loss: 214.1853\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 143.2803 - val_loss: 213.9969\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 140.9590 - val_loss: 209.7338\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 141.4323 - val_loss: 207.5470\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 141.1669 - val_loss: 208.7432\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 143.4914 - val_loss: 206.7633\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 139.0144 - val_loss: 205.5593\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 138.6168 - val_loss: 206.4704\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 138.1790 - val_loss: 203.8773\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 136.8775 - val_loss: 203.5965\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 137.8664 - val_loss: 209.5157\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 137.1108 - val_loss: 203.9703\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 136.8363 - val_loss: 203.2357\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 138.9735 - val_loss: 208.1291\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 142.1584 - val_loss: 204.1958\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 136.2219 - val_loss: 206.7375\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 138.8817 - val_loss: 205.1126\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 138.3853 - val_loss: 204.0379\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 134.0722 - val_loss: 205.5033\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 136.5617 - val_loss: 200.5214\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 133.9579 - val_loss: 204.4198\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 136.5226 - val_loss: 205.0327\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 134.1272 - val_loss: 201.6134\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 133.5425 - val_loss: 201.1237\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.9883 - val_loss: 204.8340\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 133.4620 - val_loss: 201.7858\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.2835 - val_loss: 203.3636\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.5228 - val_loss: 204.7614\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.6331 - val_loss: 202.8409\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.2571 - val_loss: 203.8241\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.9841 - val_loss: 206.3059\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 132.6483 - val_loss: 212.5670\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.9880 - val_loss: 202.1356\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 132.1436 - val_loss: 200.7413\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 131.5455 - val_loss: 202.7470\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.8313 - val_loss: 200.1162\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 133.3177 - val_loss: 201.0593\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.7912 - val_loss: 202.4218\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 129.2777 - val_loss: 207.0328\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.7692 - val_loss: 206.1913\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 135.2307 - val_loss: 202.1937\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 130.4124 - val_loss: 201.7909\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 130.0082 - val_loss: 200.3321\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 129.0726 - val_loss: 201.5192\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 127.3813 - val_loss: 202.1706\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 127.0966 - val_loss: 202.0104\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 128.9720 - val_loss: 203.0068\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.5614 - val_loss: 204.2178\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 127.9714 - val_loss: 202.0972\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 127.6664 - val_loss: 204.5592\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 128.5749 - val_loss: 198.1412\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.9181 - val_loss: 203.1973\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 125.6460 - val_loss: 210.9374\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.8651 - val_loss: 204.2716\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.9797 - val_loss: 204.4145\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 127.7206 - val_loss: 201.8611\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.0449 - val_loss: 206.3859\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.5292 - val_loss: 200.8937\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 125.4280 - val_loss: 206.1317\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 125.0053 - val_loss: 205.0800\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 124.9378 - val_loss: 203.9697\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.7727 - val_loss: 208.5680\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 127.4923 - val_loss: 201.7038\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 123.1838 - val_loss: 200.5106\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 123.4082 - val_loss: 204.6680\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 123.3908 - val_loss: 203.9963\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 127.0538 - val_loss: 201.8192\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.0568 - val_loss: 202.0656\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 123.0043 - val_loss: 201.2648\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 126.8658 - val_loss: 203.2837\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 123.0586 - val_loss: 203.9877\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 123.2678 - val_loss: 199.2827\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 122.4367 - val_loss: 203.4301\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 124.7746 - val_loss: 201.6288\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 124.0238 - val_loss: 203.1557\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 127.9518 - val_loss: 198.6037\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 125.9760 - val_loss: 201.6803\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 121.9201 - val_loss: 200.8605\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 125.7965 - val_loss: 203.6673\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 122.2376 - val_loss: 204.2102\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 119.9961 - val_loss: 203.9553\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 122.0726 - val_loss: 206.8384\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 122.0869 - val_loss: 205.2461\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 122.3442 - val_loss: 205.6723\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 122.8765 - val_loss: 203.1672\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 123.1049 - val_loss: 212.0384\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 123.1086 - val_loss: 202.7111\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 122.8431 - val_loss: 200.8192\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 123.1986 - val_loss: 210.3991\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 124.9332 - val_loss: 199.2381\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 121.2381 - val_loss: 202.2713\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 121.0149 - val_loss: 201.9814\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 124.5525 - val_loss: 203.6479\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 121.5900 - val_loss: 204.0214\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 120.2701 - val_loss: 205.2482\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 118.3328 - val_loss: 198.8582\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 117.2324 - val_loss: 204.2805\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 116.8498 - val_loss: 209.8830\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 119.3817 - val_loss: 202.4050\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 120.9509 - val_loss: 199.6184\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 118.0570 - val_loss: 210.7025\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 119.9551 - val_loss: 207.7459\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 117.9525 - val_loss: 205.1295\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 125.0241 - val_loss: 203.7354\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 119.9497 - val_loss: 202.0782\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 116.2213 - val_loss: 195.2817\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 115.6424 - val_loss: 203.1463\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 115.0292 - val_loss: 199.9066\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 116.6703 - val_loss: 207.5125\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 114.5495 - val_loss: 200.8924\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 116.0631 - val_loss: 195.5924\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 117.2914 - val_loss: 198.2162\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 114.2886 - val_loss: 196.3621\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 115.1568 - val_loss: 197.4911\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 116.8259 - val_loss: 195.1382\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 113.8187 - val_loss: 202.9126\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 118.0396 - val_loss: 200.8229\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 115.8038 - val_loss: 198.4332\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 115.2581 - val_loss: 206.6156\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 121.5769 - val_loss: 197.4149\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 117.4415 - val_loss: 201.4610\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 113.5075 - val_loss: 204.5947\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 114.8726 - val_loss: 214.4892\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 114.7606 - val_loss: 212.4121\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 115.9169 - val_loss: 214.6458\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 119.4034 - val_loss: 201.6087\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 113.2985 - val_loss: 200.7540\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 115.5037 - val_loss: 203.5870\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 114.7710 - val_loss: 208.2402\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 113.4440 - val_loss: 199.7184\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 112.7614 - val_loss: 208.4303\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 114.1128 - val_loss: 208.0097\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 112.3729 - val_loss: 206.0262\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.6958 - val_loss: 198.7968\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.6297 - val_loss: 206.2238\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 111.3512 - val_loss: 209.7022\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 108.7100 - val_loss: 203.4476\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.7831 - val_loss: 205.7330\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 114.4967 - val_loss: 203.0210\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 115.5429 - val_loss: 222.4038\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 113.6106 - val_loss: 205.8212\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 113.3758 - val_loss: 206.9495\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 118.0909 - val_loss: 210.5156\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 112.1920 - val_loss: 203.3303\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.8532 - val_loss: 202.3999\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.3907 - val_loss: 204.7704\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 110.8876 - val_loss: 211.9062\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 109.4637 - val_loss: 202.8919\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 108.3497 - val_loss: 207.7068\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 112.3177 - val_loss: 220.8976\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.2707 - val_loss: 208.8192\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 109.0984 - val_loss: 212.9686\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 106.5929 - val_loss: 222.0774\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.3271 - val_loss: 218.3342\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 109.2649 - val_loss: 207.9238\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 110.7212 - val_loss: 217.2334\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 114.2430 - val_loss: 206.8773\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 111.2397 - val_loss: 216.2096\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 109.7732 - val_loss: 210.3508\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 109.1188 - val_loss: 223.3034\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 106.4989 - val_loss: 211.4619\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 107.5077 - val_loss: 207.8388\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 106.0785 - val_loss: 216.3253\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 109.8070 - val_loss: 222.2691\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 108.6443 - val_loss: 210.1199\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 107.3859 - val_loss: 216.6531\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 106.3812 - val_loss: 211.1264\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 107.4662 - val_loss: 217.2187\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 106.2275 - val_loss: 205.8841\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 107.3425 - val_loss: 219.0250\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 106.5544 - val_loss: 214.1105\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 108.2266 - val_loss: 205.0626\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.1610 - val_loss: 212.6385\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 105.8315 - val_loss: 206.2577\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 109.5997 - val_loss: 204.1503\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 107.9918 - val_loss: 207.1628\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.4925 - val_loss: 206.8925\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 109.6032 - val_loss: 212.7549\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 108.5252 - val_loss: 221.8068\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.1327 - val_loss: 201.4151\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 104.4398 - val_loss: 206.9684\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 104.6336 - val_loss: 206.0810\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 103.3919 - val_loss: 212.0622\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 105.5535 - val_loss: 214.8577\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 105.2866 - val_loss: 221.6823\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 103.7146 - val_loss: 219.5191\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.4240 - val_loss: 206.5674\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 103.3448 - val_loss: 208.1577\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 102.6386 - val_loss: 209.3030\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.6939 - val_loss: 219.4571\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 101.4061 - val_loss: 207.2518\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 106.2526 - val_loss: 212.6335\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.6385 - val_loss: 203.1628\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 103.7554 - val_loss: 203.2346\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 102.9116 - val_loss: 208.9005\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 102.7418 - val_loss: 223.9457\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.1200 - val_loss: 209.4364\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 102.8644 - val_loss: 231.0309\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 102.9908 - val_loss: 210.4523\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.4008 - val_loss: 222.5205\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 105.5403 - val_loss: 206.9826\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 102.2016 - val_loss: 212.2692\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 100.4604 - val_loss: 216.7994\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.2942 - val_loss: 211.8051\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 101.0052 - val_loss: 215.0805\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 99.7057 - val_loss: 217.9538\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 100.9390 - val_loss: 207.6802\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 103.9461 - val_loss: 204.5553\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 105.0142 - val_loss: 202.3357\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 101.5701 - val_loss: 214.2383\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.0477 - val_loss: 222.1066\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 104.9355 - val_loss: 209.4024\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 103.5480 - val_loss: 206.4175\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 105.2500 - val_loss: 211.1875\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 110.9277 - val_loss: 206.1218\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 101.5510 - val_loss: 220.0466\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 100.4955 - val_loss: 213.4320\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.9065 - val_loss: 221.7682\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 99.8540 - val_loss: 219.5860\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.5114 - val_loss: 230.3175\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.8920 - val_loss: 214.4038\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 101.4086 - val_loss: 211.9492\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 104.0128 - val_loss: 218.6047\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 99.9648 - val_loss: 204.4821\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.8855 - val_loss: 216.8384\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.9899 - val_loss: 229.7309\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 99.4118 - val_loss: 214.6867\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.5297 - val_loss: 216.4742\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 96.1157 - val_loss: 218.0836\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.0315 - val_loss: 219.6348\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 96.5872 - val_loss: 209.4159\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 96.6570 - val_loss: 210.2852\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.5202 - val_loss: 228.8635\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.8636 - val_loss: 212.4073\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.6786 - val_loss: 210.2136\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 95.3547 - val_loss: 213.8063\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 96.4705 - val_loss: 223.2306\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 96.0572 - val_loss: 230.3258\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.2671 - val_loss: 215.7046\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 97.3534 - val_loss: 216.2867\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.8748 - val_loss: 211.0846\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 99.6865 - val_loss: 206.8314\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.6349 - val_loss: 217.7927\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.0163 - val_loss: 221.3875\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.3590 - val_loss: 217.8397\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.0208 - val_loss: 217.0961\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 95.2587 - val_loss: 212.8110\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 96.0315 - val_loss: 219.9799\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 95.2540 - val_loss: 217.6603\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 97.6280 - val_loss: 218.9982\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 96.4198 - val_loss: 217.2092\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 96.8313 - val_loss: 229.4050\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 99.1543 - val_loss: 225.4045\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 95.0123 - val_loss: 216.5060\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 95.0345 - val_loss: 218.1237\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 93.6734 - val_loss: 223.0222\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 95.2292 - val_loss: 221.9974\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 96.1693 - val_loss: 222.5318\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 98.4782 - val_loss: 206.2090\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 99.3304 - val_loss: 222.5253\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 95.8983 - val_loss: 212.7185\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 95.5242 - val_loss: 205.9710\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 94.3919 - val_loss: 210.3076\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 95.3898 - val_loss: 215.4718\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 93.8046 - val_loss: 210.5604\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 93.0313 - val_loss: 226.7972\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 92.9170 - val_loss: 228.8082\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 94.1415 - val_loss: 220.0989\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 92.8573 - val_loss: 216.6263\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 92.2750 - val_loss: 217.3610\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 91.7801 - val_loss: 214.6651\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 94.2538 - val_loss: 221.8721\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 93.8561 - val_loss: 218.0799\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 91.5998 - val_loss: 212.4781\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 94.7581 - val_loss: 220.6803\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 92.4638 - val_loss: 236.9458\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 93.4664 - val_loss: 231.3032\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 93.6517 - val_loss: 208.6965\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 93.0759 - val_loss: 219.5740\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 92.8750 - val_loss: 213.4635\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 94.5781 - val_loss: 227.9915\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 94.5382 - val_loss: 232.4645\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 95.7512 - val_loss: 233.0763\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 94.1913 - val_loss: 216.4824\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 92.2608 - val_loss: 225.4203\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 91.4082 - val_loss: 225.2313\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 92.8373 - val_loss: 212.5888\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 90.8963 - val_loss: 216.4805\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 90.5601 - val_loss: 240.4295\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 91.1768 - val_loss: 238.2544\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 89.6940 - val_loss: 225.5169\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 89.8503 - val_loss: 226.3096\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 92.3667 - val_loss: 231.7677\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 91.9546 - val_loss: 225.9299\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 90.3965 - val_loss: 223.1778\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 88.7067 - val_loss: 225.7981\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 89.5882 - val_loss: 218.6996\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 87.9063 - val_loss: 226.8763\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 88.5815 - val_loss: 233.2000\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 89.7652 - val_loss: 217.6973\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 90.1385 - val_loss: 236.0677\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 89.1992 - val_loss: 223.1909\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 90.0863 - val_loss: 241.0986\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 89.5695 - val_loss: 220.1220\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 88.0869 - val_loss: 209.7699\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 88.7550 - val_loss: 237.6899\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 91.4153 - val_loss: 230.6277\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 86.5054 - val_loss: 222.9315\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 88.3205 - val_loss: 240.7667\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 90.9526 - val_loss: 213.6835\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 89.3568 - val_loss: 227.3327\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 86.5654 - val_loss: 214.4328\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 87.8882 - val_loss: 231.1940\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 87.8300 - val_loss: 225.4137\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 87.8649 - val_loss: 221.4056\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 87.9303 - val_loss: 224.5615\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 88.9356 - val_loss: 221.0956\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 88.2253 - val_loss: 218.8372\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 87.0687 - val_loss: 218.3142\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 86.4019 - val_loss: 221.0413\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 86.4580 - val_loss: 217.7582\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 87.5583 - val_loss: 215.1220\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.9168 - val_loss: 226.4843\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 86.8055 - val_loss: 220.8602\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 87.5501 - val_loss: 221.6867\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 88.8799 - val_loss: 221.7649\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 86.4067 - val_loss: 215.9379\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.0927 - val_loss: 216.7184\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.3118 - val_loss: 218.3977\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.8732 - val_loss: 236.9035\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 84.8260 - val_loss: 239.6167\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 86.4057 - val_loss: 218.9468\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 84.3800 - val_loss: 217.0897\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 83.2496 - val_loss: 226.9405\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 84.5152 - val_loss: 231.3707\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.5300 - val_loss: 224.8559\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 84.1774 - val_loss: 214.6833\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 83.2621 - val_loss: 213.8545\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.3493 - val_loss: 213.9466\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 84.2505 - val_loss: 215.7162\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 83.4436 - val_loss: 229.3182\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.7019 - val_loss: 219.0546\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.2408 - val_loss: 214.9239\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.2895 - val_loss: 223.9684\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.6092 - val_loss: 220.3071\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 83.2549 - val_loss: 223.9910\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.3170 - val_loss: 220.4766\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 81.7453 - val_loss: 229.7038\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 81.9381 - val_loss: 227.2611\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 81.6336 - val_loss: 231.3822\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 85.2003 - val_loss: 225.4516\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 85.3410 - val_loss: 225.2158\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 90.2693 - val_loss: 231.3055\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 89.3482 - val_loss: 222.0855\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.6506 - val_loss: 237.2878\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 84.1671 - val_loss: 226.0701\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 80.9483 - val_loss: 228.4550\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 80.9877 - val_loss: 229.7600\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.9399 - val_loss: 240.8041\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.1895 - val_loss: 232.5459\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 79.7949 - val_loss: 226.7725\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 80.3143 - val_loss: 221.1267\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 80.4250 - val_loss: 227.3762\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 79.2517 - val_loss: 219.3912\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 79.7386 - val_loss: 231.2831\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 81.0917 - val_loss: 227.0517\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 78.6035 - val_loss: 235.1405\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 79.1386 - val_loss: 223.9739\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 78.6712 - val_loss: 220.7116\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 78.5632 - val_loss: 218.2095\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 83.0489 - val_loss: 223.9431\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 79.2277 - val_loss: 226.0425\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 78.1681 - val_loss: 220.7195\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 80.1967 - val_loss: 227.1069\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 78.4839 - val_loss: 232.0276\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 81.7864 - val_loss: 219.6515\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.9372 - val_loss: 242.6668\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.7337 - val_loss: 221.6710\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 80.9510 - val_loss: 225.6908\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 77.9651 - val_loss: 237.9742\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 78.5665 - val_loss: 233.2783\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 77.1111 - val_loss: 220.5562\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.1301 - val_loss: 219.3843\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.7457 - val_loss: 223.5790\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.4058 - val_loss: 228.4375\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.0718 - val_loss: 232.1381\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 77.6202 - val_loss: 227.3469\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 77.5726 - val_loss: 230.1976\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 75.6547 - val_loss: 223.5811\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 77.5527 - val_loss: 225.6918\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.7374 - val_loss: 231.2369\n",
            "108/108 [==============================] - 4s 40ms/step - loss: 76.5056 - val_loss: 230.3439\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.1459 - val_loss: 225.5879\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 75.9354 - val_loss: 225.2580\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 77.7580 - val_loss: 234.8954\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.6789 - val_loss: 228.0339\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 74.6948 - val_loss: 225.0652\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.0355 - val_loss: 223.4356\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 78.1635 - val_loss: 236.5854\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 79.3096 - val_loss: 232.1282\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 78.1691 - val_loss: 232.9306\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 78.2050 - val_loss: 231.6463\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 77.1245 - val_loss: 238.1295\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 78.8994 - val_loss: 245.4609\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 85.2843 - val_loss: 235.7234\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 83.7407 - val_loss: 235.2833\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 79.0685 - val_loss: 231.0286\n",
            "108/108 [==============================] - 5s 44ms/step - loss: 77.7097 - val_loss: 225.9484\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 83.2000 - val_loss: 225.1203\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 82.1743 - val_loss: 230.0642\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 80.0291 - val_loss: 229.9119\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 74.1268 - val_loss: 234.3968\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 75.9121 - val_loss: 246.9127\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 76.5000 - val_loss: 224.1380\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 77.2214 - val_loss: 226.7728\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 73.4406 - val_loss: 224.7531\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 74.6036 - val_loss: 229.7686\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 75.0848 - val_loss: 222.0286\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 74.3951 - val_loss: 236.8598\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 73.5338 - val_loss: 225.2468\n",
            "108/108 [==============================] - 4s 42ms/step - loss: 72.0305 - val_loss: 228.4422\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.6429 - val_loss: 232.8781\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 73.4070 - val_loss: 232.5508\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 71.9979 - val_loss: 232.9733\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 71.6844 - val_loss: 225.9635\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 71.8277 - val_loss: 227.9465\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.3110 - val_loss: 230.7852\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 71.2003 - val_loss: 232.5340\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.7379 - val_loss: 238.8426\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.0843 - val_loss: 237.3546\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.8090 - val_loss: 237.3076\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 74.1058 - val_loss: 222.7022\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 74.9367 - val_loss: 238.9830\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 73.8133 - val_loss: 234.3848\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 74.1149 - val_loss: 236.7288\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 75.1151 - val_loss: 225.2096\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 74.8008 - val_loss: 224.3714\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 71.9058 - val_loss: 223.6240\n",
            "108/108 [==============================] - 5s 43ms/step - loss: 73.6260 - val_loss: 233.6277\n",
            "108/108 [==============================] - 5s 42ms/step - loss: 70.2890 - val_loss: 230.1038\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 70.3101 - val_loss: 232.5965\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.1621 - val_loss: 228.9610\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.4027 - val_loss: 236.6134\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.1601 - val_loss: 235.3513\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 72.0118 - val_loss: 234.0497\n",
            "108/108 [==============================] - 4s 41ms/step - loss: 74.6229 - val_loss: 241.5640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GSuo6wm3KCVF",
        "outputId": "72636131-737b-42e3-a659-5a4a03384701"
      },
      "source": [
        "# 모델 사용하기\r\n",
        "xhat = test_x[5]\r\n",
        "print(xhat)\r\n",
        "print(xhat.shape)\r\n",
        "weights = model.get_weights()\r\n",
        "single_item_model_1 = Sequential()\r\n",
        "for l in range(2):\r\n",
        "    single_item_model_1.add(LSTM(128, batch_input_shape=(1, 336, 8), stateful=True, return_sequences=True))\r\n",
        "    single_item_model_1.add(Dropout(0.3))\r\n",
        "single_item_model_1.add(LSTM(128, batch_input_shape=(1, 336, 8), stateful=True))\r\n",
        "single_item_model_1.add(Dropout(0.3))\r\n",
        "single_item_model_1.add(Dense(96))  \r\n",
        "single_item_model_1.set_weights(weights)\r\n",
        "single_item_model_1.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer='adam')\r\n",
        "\r\n",
        "prediction = single_item_model_1.predict(np.array([xhat]), batch_size=1)\r\n",
        "print(prediction)\r\n",
        "print(prediction.shape)\r\n",
        "print(max(prediction[0]))\r\n",
        "\r\n",
        "plt.plot(np.reshape(prediction,(96)))\r\n",
        "plt.plot(test_y[5])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.          0.          0.         ...  2.         -6.97958958\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  2.         -6.03649269\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  2.         -6.8010495\n",
            "   0.        ]\n",
            " ...\n",
            " [ 0.          0.          0.         ...  8.         -6.19348764\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  8.         -6.87535672\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  8.         -6.87535672\n",
            "   0.        ]]\n",
            "(336, 8)\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38c174bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[-8.66117072e-04  1.67450646e-03  1.27642415e-03 -4.68828715e-03\n",
            "   2.94265477e-03  1.16937989e-02  6.07891660e-03 -2.22043879e-03\n",
            "   1.03669483e-02  1.53546873e-02 -3.09743360e-02  1.54104471e-01\n",
            "   4.08835083e-01  2.84161925e+00  8.79239941e+00  1.70228920e+01\n",
            "   2.56011143e+01  3.40888710e+01  4.26213837e+01  4.79267159e+01\n",
            "   5.40237885e+01  5.72023697e+01  6.00307961e+01  5.93603706e+01\n",
            "   5.87468758e+01  5.46619148e+01  5.19045105e+01  4.85877380e+01\n",
            "   4.40286522e+01  3.92583466e+01  3.19975929e+01  2.59702873e+01\n",
            "   1.86927452e+01  1.14356918e+01  4.99748993e+00  1.00086164e+00\n",
            "  -1.91551268e-01 -1.81961060e-02 -1.79078802e-02  3.20117618e-03\n",
            "   9.98378266e-03 -1.25234695e-02 -6.68833731e-04  1.15610822e-03\n",
            "  -3.56640248e-03  3.84120271e-04  3.90007393e-03 -3.99073865e-03\n",
            "   6.00164407e-04 -2.90248106e-04  4.78392188e-03  2.86515278e-04\n",
            "   2.96149810e-04  1.54787358e-02 -5.78859309e-03  7.03566615e-03\n",
            "  -1.01110768e-02  5.20494115e-03 -7.58823007e-03  1.81758344e-01\n",
            "   4.12099302e-01  2.79409695e+00  8.87667179e+00  1.71902771e+01\n",
            "   2.57773800e+01  3.41039429e+01  4.29083023e+01  4.90065727e+01\n",
            "   5.54742165e+01  5.88352394e+01  6.16020355e+01  6.16898346e+01\n",
            "   6.13923225e+01  5.85066643e+01  5.57055740e+01  5.10521927e+01\n",
            "   4.70606499e+01  4.01291161e+01  3.25803375e+01  2.69230003e+01\n",
            "   1.98419037e+01  1.21344233e+01  5.83575821e+00  1.71270084e+00\n",
            "   1.82960063e-01  9.76533890e-02  3.06359679e-03 -7.29266182e-03\n",
            "   5.18823741e-04 -2.05996723e-04  1.30093619e-02 -7.13917892e-03\n",
            "   4.59656632e-03  4.52052150e-03  2.56848335e-03 -2.32488965e-03]]\n",
            "(1, 96)\n",
            "61.689835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f38bd56ac50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yJbZyEQky-a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}