{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kingsolarbeam.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rH3GAkJs36t"
      },
      "source": [
        "import numpy as np\r\n",
        "from numpy import concatenate\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "import math\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, LSTM, Dropout\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import RepeatVector\r\n",
        "from keras.layers import TimeDistributed\r\n",
        "from keras.layers import ConvLSTM2D\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtGQCg2stOeq"
      },
      "source": [
        "solar_data = pd.read_csv('train.csv',encoding='utf-8')\r\n",
        "solar_data.drop(['Day', 'Hour','Minute'], axis='columns', inplace=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khBNwoQOtXLG"
      },
      "source": [
        "class CustomHistory(keras.callbacks.Callback):\r\n",
        "    def init(self):\r\n",
        "        self.train_loss = []\r\n",
        "        self.val_loss = []\r\n",
        "        \r\n",
        "    def on_epoch_end(self, batch, logs={}):\r\n",
        "        self.train_loss.append(logs.get('loss'))\r\n",
        "        self.val_loss.append(logs.get('val_loss'))\r\n",
        "\r\n",
        "def create_dataset(solar_data, index):\r\n",
        "    dataX, dataY = [], []\r\n",
        "    for i in range(0,48*7):\r\n",
        "        dataX.append(list(np.array(solar_data.loc[index+i].tolist())))\r\n",
        "    for i in range(48*7,48*7+48*2):\r\n",
        "        dataY.append(solar_data.loc[index+i,'TARGET'])\r\n",
        "    return np.array(dataX), np.array(dataY)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGcvKLehtXgX"
      },
      "source": [
        "# 데이터셋 생성\r\n",
        "input_data, output_data = [], []\r\n",
        "last_index = 3*365*48-48*9\r\n",
        "list_index = list(range(0,last_index,48))\r\n",
        "for i in list_index:\r\n",
        "    X, Y = create_dataset(solar_data,i)\r\n",
        "    input_data.append(X)\r\n",
        "    output_data.append(Y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KBltPk21Pt5"
      },
      "source": [
        "input_array = np.array(input_data)\r\n",
        "output_array = np.reshape(np.array(output_data),(1086,96,1))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHCurx1B0zO0",
        "outputId": "f2a9433a-3d9e-4f71-9a90-04108b92854f"
      },
      "source": [
        "input_array.shape, output_array.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1086, 336, 6), (1086, 96, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UniartIetYjT"
      },
      "source": [
        "# 데이터셋 분배\r\n",
        "train_x, test_x, train_y, test_y = train_test_split(input_array, output_array, test_size = 0.1,shuffle = False)\r\n",
        "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1,shuffle=False)\r\n",
        "\r\n",
        "# x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\r\n",
        "# train_x = train_x.reshape((train_x.shape[0], 7, 1, 48, 6))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNgnOOHRexZ9",
        "outputId": "794d8a8b-c465-473f-f84d-78947f65b9cb"
      },
      "source": [
        "print(x_train.shape,y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(977, 336, 6) (977, 96, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQp90KRmJg3Q",
        "outputId": "ef9255b3-ab37-4d75-83ff-b2bccef1587b"
      },
      "source": [
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(LSTM(1048, activation='relu', input_shape=(336, 6)))\r\n",
        "\tmodel.add(Dense(256, activation='relu'))\r\n",
        "\tmodel.add(Dense(96))\r\n",
        "\tmodel.compile(loss='mse', optimizer='adam')\r\n",
        "\t# fit network\r\n",
        "\tmodel.fit(train_x, train_y, epochs=100, batch_size=8, verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/100\n",
            "123/123 [==============================] - 45s 356ms/step - loss: 20428.5544\n",
            "Epoch 2/100\n",
            "123/123 [==============================] - 44s 355ms/step - loss: 1076.7567\n",
            "Epoch 3/100\n",
            "123/123 [==============================] - 44s 355ms/step - loss: 1060.4737\n",
            "Epoch 4/100\n",
            "123/123 [==============================] - 44s 358ms/step - loss: 1021.5069\n",
            "Epoch 5/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 986.7109\n",
            "Epoch 6/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 1007.0980\n",
            "Epoch 7/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 1005.9595\n",
            "Epoch 8/100\n",
            "123/123 [==============================] - 44s 354ms/step - loss: 1022.9938\n",
            "Epoch 9/100\n",
            "123/123 [==============================] - 44s 355ms/step - loss: 1023.5692\n",
            "Epoch 10/100\n",
            "123/123 [==============================] - 44s 354ms/step - loss: 1023.1452\n",
            "Epoch 11/100\n",
            "123/123 [==============================] - 43s 352ms/step - loss: 997.9941\n",
            "Epoch 12/100\n",
            "123/123 [==============================] - 43s 349ms/step - loss: 1017.6080\n",
            "Epoch 13/100\n",
            "123/123 [==============================] - 43s 349ms/step - loss: 981.0183\n",
            "Epoch 14/100\n",
            "123/123 [==============================] - 43s 353ms/step - loss: 984.8768\n",
            "Epoch 15/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 1006.5445\n",
            "Epoch 16/100\n",
            "123/123 [==============================] - 46s 372ms/step - loss: 980.7976\n",
            "Epoch 17/100\n",
            "123/123 [==============================] - 44s 362ms/step - loss: 965.8406\n",
            "Epoch 18/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 955.2076\n",
            "Epoch 19/100\n",
            "123/123 [==============================] - 45s 362ms/step - loss: 937.8610\n",
            "Epoch 20/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 941.0137\n",
            "Epoch 21/100\n",
            "123/123 [==============================] - 45s 363ms/step - loss: 970.7426\n",
            "Epoch 22/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 952.3636\n",
            "Epoch 23/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 944.6616\n",
            "Epoch 24/100\n",
            "123/123 [==============================] - 44s 358ms/step - loss: 914.4888\n",
            "Epoch 25/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 966.8239\n",
            "Epoch 26/100\n",
            "123/123 [==============================] - 45s 369ms/step - loss: 963.3538\n",
            "Epoch 27/100\n",
            "123/123 [==============================] - 46s 370ms/step - loss: 934.0681\n",
            "Epoch 28/100\n",
            "123/123 [==============================] - 45s 362ms/step - loss: 917.1438\n",
            "Epoch 29/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 919.0056\n",
            "Epoch 30/100\n",
            "123/123 [==============================] - 44s 357ms/step - loss: 899.3925\n",
            "Epoch 31/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 909.6792\n",
            "Epoch 32/100\n",
            "123/123 [==============================] - 44s 358ms/step - loss: 898.1368\n",
            "Epoch 33/100\n",
            "123/123 [==============================] - 44s 361ms/step - loss: 897.6427\n",
            "Epoch 34/100\n",
            "123/123 [==============================] - 44s 361ms/step - loss: 903.3113\n",
            "Epoch 35/100\n",
            "123/123 [==============================] - 45s 367ms/step - loss: 910.8111\n",
            "Epoch 36/100\n",
            "123/123 [==============================] - 45s 363ms/step - loss: 929.5919\n",
            "Epoch 37/100\n",
            "123/123 [==============================] - 44s 362ms/step - loss: 862.3004\n",
            "Epoch 38/100\n",
            "123/123 [==============================] - 45s 363ms/step - loss: 858.8917\n",
            "Epoch 39/100\n",
            "123/123 [==============================] - 45s 363ms/step - loss: 902.0870\n",
            "Epoch 40/100\n",
            "123/123 [==============================] - 45s 364ms/step - loss: 898.7194\n",
            "Epoch 41/100\n",
            "123/123 [==============================] - 45s 362ms/step - loss: 908.2229\n",
            "Epoch 42/100\n",
            "123/123 [==============================] - 46s 370ms/step - loss: 874.0113\n",
            "Epoch 43/100\n",
            "123/123 [==============================] - 45s 365ms/step - loss: 862.2450\n",
            "Epoch 44/100\n",
            "123/123 [==============================] - 45s 366ms/step - loss: 864.9980\n",
            "Epoch 45/100\n",
            "123/123 [==============================] - 44s 359ms/step - loss: 876.8265\n",
            "Epoch 46/100\n",
            "123/123 [==============================] - 44s 359ms/step - loss: 836.9753\n",
            "Epoch 47/100\n",
            "123/123 [==============================] - 44s 359ms/step - loss: 847.6880\n",
            "Epoch 48/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 831.5308\n",
            "Epoch 49/100\n",
            "123/123 [==============================] - 45s 363ms/step - loss: 869.7110\n",
            "Epoch 50/100\n",
            "123/123 [==============================] - 44s 358ms/step - loss: 829.0769\n",
            "Epoch 51/100\n",
            "123/123 [==============================] - 44s 357ms/step - loss: 856.5490\n",
            "Epoch 52/100\n",
            "123/123 [==============================] - 44s 355ms/step - loss: 835.1749\n",
            "Epoch 53/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 830.2871\n",
            "Epoch 54/100\n",
            "123/123 [==============================] - 44s 357ms/step - loss: 815.3328\n",
            "Epoch 55/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 817.7878\n",
            "Epoch 56/100\n",
            "123/123 [==============================] - 44s 362ms/step - loss: 839.0308\n",
            "Epoch 57/100\n",
            "123/123 [==============================] - 44s 354ms/step - loss: 801.1145\n",
            "Epoch 58/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 821.7221\n",
            "Epoch 59/100\n",
            "123/123 [==============================] - 43s 352ms/step - loss: 804.9015\n",
            "Epoch 60/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 810.9863\n",
            "Epoch 61/100\n",
            "123/123 [==============================] - 43s 353ms/step - loss: 784.8839\n",
            "Epoch 62/100\n",
            "123/123 [==============================] - 43s 353ms/step - loss: 814.1702\n",
            "Epoch 63/100\n",
            "123/123 [==============================] - 44s 358ms/step - loss: 802.4779\n",
            "Epoch 64/100\n",
            "123/123 [==============================] - 44s 354ms/step - loss: 787.7292\n",
            "Epoch 65/100\n",
            "123/123 [==============================] - 44s 354ms/step - loss: 823.7759\n",
            "Epoch 66/100\n",
            "123/123 [==============================] - 44s 356ms/step - loss: 798.8691\n",
            "Epoch 67/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 767.1933\n",
            "Epoch 68/100\n",
            "123/123 [==============================] - 44s 359ms/step - loss: 782.2181\n",
            "Epoch 69/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 810.3763\n",
            "Epoch 70/100\n",
            "123/123 [==============================] - 45s 363ms/step - loss: 774.3955\n",
            "Epoch 71/100\n",
            "123/123 [==============================] - 44s 360ms/step - loss: 740.3584\n",
            "Epoch 72/100\n",
            "123/123 [==============================] - 44s 359ms/step - loss: 786.2084\n",
            "Epoch 73/100\n",
            "123/123 [==============================] - 44s 359ms/step - loss: 741.3083\n",
            "Epoch 74/100\n",
            "123/123 [==============================] - 45s 362ms/step - loss: 758.6678\n",
            "Epoch 75/100\n",
            "123/123 [==============================] - 44s 361ms/step - loss: 785.6702\n",
            "Epoch 76/100\n",
            "123/123 [==============================] - 44s 361ms/step - loss: 745.2211\n",
            "Epoch 77/100\n",
            "123/123 [==============================] - 45s 365ms/step - loss: 759.1188\n",
            "Epoch 78/100\n",
            "123/123 [==============================] - 44s 362ms/step - loss: 744.0652\n",
            "Epoch 79/100\n",
            "123/123 [==============================] - 45s 363ms/step - loss: 714.0915\n",
            "Epoch 80/100\n",
            "123/123 [==============================] - 45s 365ms/step - loss: 775.5202\n",
            "Epoch 81/100\n",
            "123/123 [==============================] - 45s 364ms/step - loss: 750.8385\n",
            "Epoch 82/100\n",
            "123/123 [==============================] - 45s 364ms/step - loss: 742.1879\n",
            "Epoch 83/100\n",
            "123/123 [==============================] - 45s 365ms/step - loss: 763.5164\n",
            "Epoch 84/100\n",
            "123/123 [==============================] - 45s 369ms/step - loss: 706.9415\n",
            "Epoch 85/100\n",
            "123/123 [==============================] - 45s 366ms/step - loss: 735.2547\n",
            "Epoch 86/100\n",
            "123/123 [==============================] - 45s 366ms/step - loss: 711.6992\n",
            "Epoch 87/100\n",
            "123/123 [==============================] - 45s 368ms/step - loss: 714.7333\n",
            "Epoch 88/100\n",
            "123/123 [==============================] - 45s 364ms/step - loss: 706.0620\n",
            "Epoch 89/100\n",
            "123/123 [==============================] - 45s 364ms/step - loss: 708.6927\n",
            "Epoch 90/100\n",
            "123/123 [==============================] - 45s 364ms/step - loss: 680.8307\n",
            "Epoch 91/100\n",
            "123/123 [==============================] - 45s 368ms/step - loss: 706.6017\n",
            "Epoch 92/100\n",
            "123/123 [==============================] - 45s 364ms/step - loss: 687.1187\n",
            "Epoch 93/100\n",
            "123/123 [==============================] - 45s 366ms/step - loss: 694.9124\n",
            "Epoch 94/100\n",
            "123/123 [==============================] - 45s 367ms/step - loss: 669.9590\n",
            "Epoch 95/100\n",
            "123/123 [==============================] - 45s 365ms/step - loss: 699.3694\n",
            "Epoch 96/100\n",
            "123/123 [==============================] - 45s 366ms/step - loss: 700.5503\n",
            "Epoch 97/100\n",
            "123/123 [==============================] - 45s 365ms/step - loss: 692.3833\n",
            "Epoch 98/100\n",
            "123/123 [==============================] - 46s 373ms/step - loss: 690.0101\n",
            "Epoch 99/100\n",
            "123/123 [==============================] - 45s 366ms/step - loss: 701.1386\n",
            "Epoch 100/100\n",
            "123/123 [==============================] - 45s 366ms/step - loss: 640.6843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbeb8b10710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KULcXoPauc5F",
        "outputId": "194f3570-a701-42c5-85f0-ca0f615270ca"
      },
      "source": [
        "# 모델 사용하기\r\n",
        "xhat = test_x[2]\r\n",
        "print(xhat)\r\n",
        "print(xhat.shape)\r\n",
        "prediction = model.predict(np.array([xhat]), batch_size=1)\r\n",
        "print(prediction)\r\n",
        "print(prediction.shape)\r\n",
        "print(max(prediction[0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.    0.    2.4  62.11 13.    0.  ]\n",
            " [ 0.    0.    2.3  62.09 13.    0.  ]\n",
            " [ 0.    0.    2.2  59.43 13.    0.  ]\n",
            " ...\n",
            " [ 0.    0.    1.2  58.26 14.    0.  ]\n",
            " [ 0.    0.    1.3  57.19 14.    0.  ]\n",
            " [ 0.    0.    1.3  57.19 13.    0.  ]]\n",
            "(336, 6)\n",
            "[[ 1.78972241e-34 -2.05262964e-34 -2.11686581e-34 -1.97395417e-34\n",
            "   1.93304742e-34  2.02863509e-34 -2.03251927e-34  2.05966401e-34\n",
            "  -2.02997244e-34  1.38574116e-34  4.93683964e-01  2.28831315e+00\n",
            "   5.31586504e+00  8.02067757e+00  1.01801491e+01  1.09812775e+01\n",
            "   1.14030733e+01  1.16627073e+01  1.16493378e+01  1.18145208e+01\n",
            "   1.19134378e+01  1.17452240e+01  1.18858719e+01  1.19335699e+01\n",
            "   1.18877764e+01  1.17725630e+01  1.18658466e+01  1.18201456e+01\n",
            "   1.17904921e+01  1.15942297e+01  1.15990591e+01  1.14418840e+01\n",
            "   1.11444235e+01  1.04738026e+01  8.64311886e+00  6.67635536e+00\n",
            "   4.07305813e+00  1.78613305e+00  4.58744407e-01 -2.12024398e-34\n",
            "   1.11422469e-34  1.99756002e-34  1.99852751e-34 -1.98158616e-34\n",
            "  -2.16735444e-34 -2.01577697e-34 -2.12166445e-34 -2.01760083e-34\n",
            "  -2.03568209e-34 -2.04579524e-34  1.97574955e-34 -2.05027199e-34\n",
            "  -1.93088562e-34 -1.98605740e-34 -1.66449621e-34 -3.87917921e-29\n",
            "   1.75387993e-34  2.00549760e-34  4.93273765e-01  2.29633880e+00\n",
            "   5.16682196e+00  8.25700283e+00  9.77453709e+00  1.04639664e+01\n",
            "   1.13497324e+01  1.16812859e+01  1.17985802e+01  1.17445526e+01\n",
            "   1.19044485e+01  1.19366579e+01  1.19139090e+01  1.18232927e+01\n",
            "   1.19260168e+01  1.17794504e+01  1.17790880e+01  1.17673264e+01\n",
            "   1.17063036e+01  1.16154070e+01  1.13896561e+01  1.11920834e+01\n",
            "   1.09232512e+01  1.03097830e+01  9.07684517e+00  6.65132189e+00\n",
            "   4.06769228e+00  1.78983784e+00  4.59864676e-01 -1.78652622e-22\n",
            "  -4.09606781e-29 -7.23457453e-30  2.73534183e-34 -4.00879675e-23\n",
            "  -1.74988256e-34 -2.01011371e-34 -2.10462001e-34 -2.02331919e-34]]\n",
            "(1, 96)\n",
            "11.936658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_WOg2lYpmmp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}